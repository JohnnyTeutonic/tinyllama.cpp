2025-04-26 02:39:46,650 - INFO - Config: {'architectures': ['LlamaForCausalLM'], 'attention_bias': False, 'bos_token_id': 1, 'eos_token_id': 2, 'hidden_act': 'silu', 'hidden_size': 2048, 'initializer_range': 0.02, 'intermediate_size': 5632, 'max_position_embeddings': 2048, 'model_type': 'llama', 'num_attention_heads': 32, 'num_hidden_layers': 22, 'num_key_value_heads': 4, 'pretraining_tp': 1, 'rms_norm_eps': 1e-05, 'rope_scaling': None, 'rope_theta': 10000.0, 'tie_word_embeddings': False, 'torch_dtype': 'bfloat16', 'transformers_version': '4.35.0', 'use_cache': True, 'vocab_size': 32000}
2025-04-26 02:39:59,184 - INFO - Loading model.embed_tokens.weight:
2025-04-26 02:39:59,198 - INFO -   - Tensor shape: torch.Size([32000, 2048]), dtype: torch.bfloat16
2025-04-26 02:39:59,199 - INFO -   - Param shape: torch.Size([32000, 2048]), dtype: torch.float32
2025-04-26 02:40:01,356 - INFO -   - Copied (Embedding). Param mean: -0.0000, std: 0.0149
2025-04-26 02:40:01,370 - INFO - Loading lm_head.weight:
2025-04-26 02:40:01,375 - INFO -   - Tensor shape: torch.Size([32000, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:01,377 - INFO -   - Param shape: torch.Size([32000, 2048]), dtype: torch.float32
2025-04-26 02:40:02,923 - INFO -   - Copied (Output Head). Param mean: -0.0004, std: 0.0247
2025-04-26 02:40:02,923 - INFO - Loading model.norm.weight:
2025-04-26 02:40:02,924 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:02,924 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:02,925 - INFO -   - Copied (Final Norm). Param mean: 1.9149, std: 0.1365
2025-04-26 02:40:02,925 - INFO - Loading model.layers.0.input_layernorm.weight:
2025-04-26 02:40:02,925 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:02,925 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:02,926 - INFO -   - Copied. Param mean: 0.0058, std: 0.0460
2025-04-26 02:40:02,926 - INFO - Loading model.layers.0.post_attention_layernorm.weight:
2025-04-26 02:40:02,926 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:02,926 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:02,927 - INFO -   - Copied. Param mean: 0.0746, std: 0.0331
2025-04-26 02:40:02,927 - INFO - Loading model.layers.0.self_attn.q_proj.weight:
2025-04-26 02:40:02,927 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:02,927 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:03,132 - INFO -   - Copied. Param mean: -0.0000, std: 0.0164
2025-04-26 02:40:03,133 - INFO - Loading model.layers.0.self_attn.k_proj.weight:
2025-04-26 02:40:03,133 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:03,133 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:03,222 - INFO -   - Copied. Param mean: -0.0001, std: 0.0318
2025-04-26 02:40:03,222 - INFO - Loading model.layers.0.self_attn.v_proj.weight:
2025-04-26 02:40:03,235 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:03,236 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:03,307 - INFO -   - Copied. Param mean: 0.0000, std: 0.0110
2025-04-26 02:40:03,316 - INFO - Loading model.layers.0.self_attn.o_proj.weight:
2025-04-26 02:40:03,317 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:03,320 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:03,497 - INFO -   - Copied. Param mean: 0.0000, std: 0.0083
2025-04-26 02:40:03,503 - INFO - Loading model.layers.0.mlp.gate_proj.weight:
2025-04-26 02:40:03,504 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:03,504 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:03,915 - INFO -   - Copied. Param mean: -0.0000, std: 0.0166
2025-04-26 02:40:03,917 - INFO - Loading model.layers.0.mlp.up_proj.weight:
2025-04-26 02:40:03,917 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:03,917 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:04,296 - INFO -   - Copied. Param mean: -0.0000, std: 0.0168
2025-04-26 02:40:04,300 - INFO - Loading model.layers.0.mlp.down_proj.weight:
2025-04-26 02:40:04,319 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:04,329 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:04,917 - INFO -   - Copied. Param mean: 0.0000, std: 0.0166
2025-04-26 02:40:04,917 - INFO - Loading model.layers.1.input_layernorm.weight:
2025-04-26 02:40:04,917 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:04,918 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:04,918 - INFO -   - Copied. Param mean: 0.0405, std: 0.0559
2025-04-26 02:40:04,918 - INFO - Loading model.layers.1.post_attention_layernorm.weight:
2025-04-26 02:40:04,918 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:04,918 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:04,918 - INFO -   - Copied. Param mean: 0.1284, std: 0.0211
2025-04-26 02:40:04,918 - INFO - Loading model.layers.1.self_attn.q_proj.weight:
2025-04-26 02:40:04,918 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:04,919 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:05,100 - INFO -   - Copied. Param mean: 0.0000, std: 0.0294
2025-04-26 02:40:05,109 - INFO - Loading model.layers.1.self_attn.k_proj.weight:
2025-04-26 02:40:05,113 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:05,115 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:05,155 - INFO -   - Copied. Param mean: -0.0000, std: 0.0479
2025-04-26 02:40:05,167 - INFO - Loading model.layers.1.self_attn.v_proj.weight:
2025-04-26 02:40:05,169 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:05,170 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:05,265 - INFO -   - Copied. Param mean: 0.0000, std: 0.0134
2025-04-26 02:40:05,281 - INFO - Loading model.layers.1.self_attn.o_proj.weight:
2025-04-26 02:40:05,290 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:05,290 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:05,407 - INFO -   - Copied. Param mean: -0.0000, std: 0.0137
2025-04-26 02:40:05,407 - INFO - Loading model.layers.1.mlp.gate_proj.weight:
2025-04-26 02:40:05,422 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:05,424 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:05,815 - INFO -   - Copied. Param mean: 0.0000, std: 0.0181
2025-04-26 02:40:05,828 - INFO - Loading model.layers.1.mlp.up_proj.weight:
2025-04-26 02:40:05,833 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:05,835 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:06,270 - INFO -   - Copied. Param mean: -0.0000, std: 0.0173
2025-04-26 02:40:06,271 - INFO - Loading model.layers.1.mlp.down_proj.weight:
2025-04-26 02:40:06,275 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:06,283 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:06,664 - INFO -   - Copied. Param mean: 0.0000, std: 0.0171
2025-04-26 02:40:06,666 - INFO - Loading model.layers.2.input_layernorm.weight:
2025-04-26 02:40:06,666 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:06,666 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:06,667 - INFO -   - Copied. Param mean: 0.0846, std: 0.0684
2025-04-26 02:40:06,667 - INFO - Loading model.layers.2.post_attention_layernorm.weight:
2025-04-26 02:40:06,667 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:06,667 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:06,667 - INFO -   - Copied. Param mean: 0.1674, std: 0.0219
2025-04-26 02:40:06,668 - INFO - Loading model.layers.2.self_attn.q_proj.weight:
2025-04-26 02:40:06,668 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:06,668 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:06,777 - INFO -   - Copied. Param mean: 0.0000, std: 0.0322
2025-04-26 02:40:06,777 - INFO - Loading model.layers.2.self_attn.k_proj.weight:
2025-04-26 02:40:06,777 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:06,778 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:06,825 - INFO -   - Copied. Param mean: 0.0001, std: 0.0542
2025-04-26 02:40:06,842 - INFO - Loading model.layers.2.self_attn.v_proj.weight:
2025-04-26 02:40:06,842 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:06,842 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:06,937 - INFO -   - Copied. Param mean: -0.0000, std: 0.0118
2025-04-26 02:40:06,943 - INFO - Loading model.layers.2.self_attn.o_proj.weight:
2025-04-26 02:40:06,955 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:06,956 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:07,156 - INFO -   - Copied. Param mean: -0.0000, std: 0.0141
2025-04-26 02:40:07,175 - INFO - Loading model.layers.2.mlp.gate_proj.weight:
2025-04-26 02:40:07,175 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:07,175 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:07,480 - INFO -   - Copied. Param mean: -0.0000, std: 0.0185
2025-04-26 02:40:07,481 - INFO - Loading model.layers.2.mlp.up_proj.weight:
2025-04-26 02:40:07,481 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:07,481 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:07,855 - INFO -   - Copied. Param mean: -0.0000, std: 0.0175
2025-04-26 02:40:07,856 - INFO - Loading model.layers.2.mlp.down_proj.weight:
2025-04-26 02:40:07,870 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:07,871 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:08,169 - INFO -   - Copied. Param mean: 0.0000, std: 0.0174
2025-04-26 02:40:08,185 - INFO - Loading model.layers.3.input_layernorm.weight:
2025-04-26 02:40:08,186 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:08,195 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:08,197 - INFO -   - Copied. Param mean: 0.2243, std: 0.0547
2025-04-26 02:40:08,215 - INFO - Loading model.layers.3.post_attention_layernorm.weight:
2025-04-26 02:40:08,215 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:08,215 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:08,216 - INFO -   - Copied. Param mean: 0.1843, std: 0.0214
2025-04-26 02:40:08,216 - INFO - Loading model.layers.3.self_attn.q_proj.weight:
2025-04-26 02:40:08,216 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:08,216 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:08,401 - INFO -   - Copied. Param mean: -0.0000, std: 0.0271
2025-04-26 02:40:08,401 - INFO - Loading model.layers.3.self_attn.k_proj.weight:
2025-04-26 02:40:08,402 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:08,402 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:08,526 - INFO -   - Copied. Param mean: 0.0001, std: 0.0477
2025-04-26 02:40:08,526 - INFO - Loading model.layers.3.self_attn.v_proj.weight:
2025-04-26 02:40:08,526 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:08,526 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:08,594 - INFO -   - Copied. Param mean: -0.0000, std: 0.0114
2025-04-26 02:40:08,606 - INFO - Loading model.layers.3.self_attn.o_proj.weight:
2025-04-26 02:40:08,607 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:08,607 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:08,753 - INFO -   - Copied. Param mean: 0.0000, std: 0.0143
2025-04-26 02:40:08,754 - INFO - Loading model.layers.3.mlp.gate_proj.weight:
2025-04-26 02:40:08,754 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:08,754 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:09,122 - INFO -   - Copied. Param mean: -0.0000, std: 0.0187
2025-04-26 02:40:09,123 - INFO - Loading model.layers.3.mlp.up_proj.weight:
2025-04-26 02:40:09,127 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:09,139 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:09,487 - INFO -   - Copied. Param mean: -0.0000, std: 0.0174
2025-04-26 02:40:09,495 - INFO - Loading model.layers.3.mlp.down_proj.weight:
2025-04-26 02:40:09,495 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:09,495 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:09,893 - INFO -   - Copied. Param mean: 0.0000, std: 0.0173
2025-04-26 02:40:09,907 - INFO - Loading model.layers.4.input_layernorm.weight:
2025-04-26 02:40:09,925 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:09,925 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:09,925 - INFO -   - Copied. Param mean: 0.3199, std: 0.0582
2025-04-26 02:40:09,926 - INFO - Loading model.layers.4.post_attention_layernorm.weight:
2025-04-26 02:40:09,926 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:09,926 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:09,926 - INFO -   - Copied. Param mean: 0.1978, std: 0.0238
2025-04-26 02:40:09,926 - INFO - Loading model.layers.4.self_attn.q_proj.weight:
2025-04-26 02:40:09,926 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:09,926 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:10,022 - INFO -   - Copied. Param mean: 0.0000, std: 0.0260
2025-04-26 02:40:10,033 - INFO - Loading model.layers.4.self_attn.k_proj.weight:
2025-04-26 02:40:10,045 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:10,045 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:10,079 - INFO -   - Copied. Param mean: 0.0000, std: 0.0490
2025-04-26 02:40:10,079 - INFO - Loading model.layers.4.self_attn.v_proj.weight:
2025-04-26 02:40:10,080 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:10,080 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:10,117 - INFO -   - Copied. Param mean: 0.0000, std: 0.0101
2025-04-26 02:40:10,135 - INFO - Loading model.layers.4.self_attn.o_proj.weight:
2025-04-26 02:40:10,137 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:10,137 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:10,276 - INFO -   - Copied. Param mean: -0.0000, std: 0.0140
2025-04-26 02:40:10,277 - INFO - Loading model.layers.4.mlp.gate_proj.weight:
2025-04-26 02:40:10,277 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:10,277 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:10,608 - INFO -   - Copied. Param mean: -0.0000, std: 0.0190
2025-04-26 02:40:10,625 - INFO - Loading model.layers.4.mlp.up_proj.weight:
2025-04-26 02:40:10,625 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:10,626 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:10,918 - INFO -   - Copied. Param mean: -0.0000, std: 0.0173
2025-04-26 02:40:10,919 - INFO - Loading model.layers.4.mlp.down_proj.weight:
2025-04-26 02:40:10,919 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:10,919 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:11,265 - INFO -   - Copied. Param mean: -0.0000, std: 0.0173
2025-04-26 02:40:11,266 - INFO - Loading model.layers.5.input_layernorm.weight:
2025-04-26 02:40:11,266 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:11,266 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:11,266 - INFO -   - Copied. Param mean: 0.2800, std: 0.0488
2025-04-26 02:40:11,266 - INFO - Loading model.layers.5.post_attention_layernorm.weight:
2025-04-26 02:40:11,266 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:11,267 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:11,267 - INFO -   - Copied. Param mean: 0.2160, std: 0.0225
2025-04-26 02:40:11,267 - INFO - Loading model.layers.5.self_attn.q_proj.weight:
2025-04-26 02:40:11,267 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:11,267 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:11,430 - INFO -   - Copied. Param mean: -0.0000, std: 0.0271
2025-04-26 02:40:11,430 - INFO - Loading model.layers.5.self_attn.k_proj.weight:
2025-04-26 02:40:11,430 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:11,431 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:11,545 - INFO -   - Copied. Param mean: 0.0000, std: 0.0502
2025-04-26 02:40:11,549 - INFO - Loading model.layers.5.self_attn.v_proj.weight:
2025-04-26 02:40:11,550 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:11,559 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:11,611 - INFO -   - Copied. Param mean: 0.0000, std: 0.0115
2025-04-26 02:40:11,612 - INFO - Loading model.layers.5.self_attn.o_proj.weight:
2025-04-26 02:40:11,625 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:11,625 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:11,742 - INFO -   - Copied. Param mean: -0.0000, std: 0.0145
2025-04-26 02:40:11,743 - INFO - Loading model.layers.5.mlp.gate_proj.weight:
2025-04-26 02:40:11,743 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:11,743 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:12,049 - INFO -   - Copied. Param mean: -0.0000, std: 0.0192
2025-04-26 02:40:12,059 - INFO - Loading model.layers.5.mlp.up_proj.weight:
2025-04-26 02:40:12,061 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:12,061 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:12,479 - INFO -   - Copied. Param mean: 0.0000, std: 0.0174
2025-04-26 02:40:12,479 - INFO - Loading model.layers.5.mlp.down_proj.weight:
2025-04-26 02:40:12,479 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:12,479 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:12,823 - INFO -   - Copied. Param mean: 0.0000, std: 0.0173
2025-04-26 02:40:12,836 - INFO - Loading model.layers.6.input_layernorm.weight:
2025-04-26 02:40:12,840 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:12,840 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:12,840 - INFO -   - Copied. Param mean: 0.2680, std: 0.0792
2025-04-26 02:40:12,840 - INFO - Loading model.layers.6.post_attention_layernorm.weight:
2025-04-26 02:40:12,840 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:12,841 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:12,841 - INFO -   - Copied. Param mean: 0.2251, std: 0.0219
2025-04-26 02:40:12,841 - INFO - Loading model.layers.6.self_attn.q_proj.weight:
2025-04-26 02:40:12,841 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:12,841 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:13,040 - INFO -   - Copied. Param mean: 0.0000, std: 0.0263
2025-04-26 02:40:13,051 - INFO - Loading model.layers.6.self_attn.k_proj.weight:
2025-04-26 02:40:13,052 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:13,052 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:13,165 - INFO -   - Copied. Param mean: 0.0001, std: 0.0471
2025-04-26 02:40:13,168 - INFO - Loading model.layers.6.self_attn.v_proj.weight:
2025-04-26 02:40:13,168 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:13,168 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:13,190 - INFO -   - Copied. Param mean: -0.0000, std: 0.0113
2025-04-26 02:40:13,195 - INFO - Loading model.layers.6.self_attn.o_proj.weight:
2025-04-26 02:40:13,195 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:13,195 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:13,318 - INFO -   - Copied. Param mean: 0.0000, std: 0.0140
2025-04-26 02:40:13,319 - INFO - Loading model.layers.6.mlp.gate_proj.weight:
2025-04-26 02:40:13,319 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:13,319 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:13,643 - INFO -   - Copied. Param mean: 0.0000, std: 0.0196
2025-04-26 02:40:13,646 - INFO - Loading model.layers.6.mlp.up_proj.weight:
2025-04-26 02:40:13,650 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:13,655 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:13,975 - INFO -   - Copied. Param mean: -0.0000, std: 0.0172
2025-04-26 02:40:13,995 - INFO - Loading model.layers.6.mlp.down_proj.weight:
2025-04-26 02:40:13,995 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:13,995 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:14,341 - INFO -   - Copied. Param mean: -0.0000, std: 0.0171
2025-04-26 02:40:14,343 - INFO - Loading model.layers.7.input_layernorm.weight:
2025-04-26 02:40:14,353 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:14,354 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:14,356 - INFO -   - Copied. Param mean: 0.3070, std: 0.0577
2025-04-26 02:40:14,356 - INFO - Loading model.layers.7.post_attention_layernorm.weight:
2025-04-26 02:40:14,356 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:14,356 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:14,356 - INFO -   - Copied. Param mean: 0.2384, std: 0.0224
2025-04-26 02:40:14,356 - INFO - Loading model.layers.7.self_attn.q_proj.weight:
2025-04-26 02:40:14,357 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:14,357 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:14,459 - INFO -   - Copied. Param mean: -0.0000, std: 0.0266
2025-04-26 02:40:14,460 - INFO - Loading model.layers.7.self_attn.k_proj.weight:
2025-04-26 02:40:14,460 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:14,460 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:14,480 - INFO -   - Copied. Param mean: 0.0000, std: 0.0450
2025-04-26 02:40:14,481 - INFO - Loading model.layers.7.self_attn.v_proj.weight:
2025-04-26 02:40:14,481 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:14,481 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:14,523 - INFO -   - Copied. Param mean: -0.0000, std: 0.0130
2025-04-26 02:40:14,523 - INFO - Loading model.layers.7.self_attn.o_proj.weight:
2025-04-26 02:40:14,523 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:14,524 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:14,634 - INFO -   - Copied. Param mean: 0.0000, std: 0.0149
2025-04-26 02:40:14,645 - INFO - Loading model.layers.7.mlp.gate_proj.weight:
2025-04-26 02:40:14,645 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:14,646 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:15,005 - INFO -   - Copied. Param mean: 0.0001, std: 0.0209
2025-04-26 02:40:15,006 - INFO - Loading model.layers.7.mlp.up_proj.weight:
2025-04-26 02:40:15,006 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:15,006 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:15,297 - INFO -   - Copied. Param mean: 0.0000, std: 0.0168
2025-04-26 02:40:15,315 - INFO - Loading model.layers.7.mlp.down_proj.weight:
2025-04-26 02:40:15,325 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:15,325 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:15,637 - INFO -   - Copied. Param mean: 0.0000, std: 0.0167
2025-04-26 02:40:15,651 - INFO - Loading model.layers.8.input_layernorm.weight:
2025-04-26 02:40:15,651 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:15,651 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:15,651 - INFO -   - Copied. Param mean: 0.3188, std: 0.1083
2025-04-26 02:40:15,651 - INFO - Loading model.layers.8.post_attention_layernorm.weight:
2025-04-26 02:40:15,651 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:15,652 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:15,652 - INFO -   - Copied. Param mean: 0.2645, std: 0.0304
2025-04-26 02:40:15,652 - INFO - Loading model.layers.8.self_attn.q_proj.weight:
2025-04-26 02:40:15,652 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:15,652 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:15,771 - INFO -   - Copied. Param mean: -0.0000, std: 0.0273
2025-04-26 02:40:15,771 - INFO - Loading model.layers.8.self_attn.k_proj.weight:
2025-04-26 02:40:15,772 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:15,772 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:15,825 - INFO -   - Copied. Param mean: -0.0000, std: 0.0466
2025-04-26 02:40:15,827 - INFO - Loading model.layers.8.self_attn.v_proj.weight:
2025-04-26 02:40:15,827 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:15,827 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:15,857 - INFO -   - Copied. Param mean: -0.0000, std: 0.0121
2025-04-26 02:40:15,865 - INFO - Loading model.layers.8.self_attn.o_proj.weight:
2025-04-26 02:40:15,869 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:15,885 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:16,044 - INFO -   - Copied. Param mean: -0.0000, std: 0.0145
2025-04-26 02:40:16,055 - INFO - Loading model.layers.8.mlp.gate_proj.weight:
2025-04-26 02:40:16,075 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:16,075 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:16,327 - INFO -   - Copied. Param mean: 0.0001, std: 0.0202
2025-04-26 02:40:16,339 - INFO - Loading model.layers.8.mlp.up_proj.weight:
2025-04-26 02:40:16,339 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:16,339 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:16,601 - INFO -   - Copied. Param mean: 0.0000, std: 0.0173
2025-04-26 02:40:16,612 - INFO - Loading model.layers.8.mlp.down_proj.weight:
2025-04-26 02:40:16,613 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:16,614 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:17,000 - INFO -   - Copied. Param mean: 0.0000, std: 0.0172
2025-04-26 02:40:17,007 - INFO - Loading model.layers.9.input_layernorm.weight:
2025-04-26 02:40:17,011 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:17,025 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:17,025 - INFO -   - Copied. Param mean: 0.3141, std: 0.0609
2025-04-26 02:40:17,026 - INFO - Loading model.layers.9.post_attention_layernorm.weight:
2025-04-26 02:40:17,026 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:17,026 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:17,026 - INFO -   - Copied. Param mean: 0.2707, std: 0.0268
2025-04-26 02:40:17,026 - INFO - Loading model.layers.9.self_attn.q_proj.weight:
2025-04-26 02:40:17,026 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:17,027 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:17,124 - INFO -   - Copied. Param mean: 0.0000, std: 0.0266
2025-04-26 02:40:17,129 - INFO - Loading model.layers.9.self_attn.k_proj.weight:
2025-04-26 02:40:17,136 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:17,155 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:17,207 - INFO -   - Copied. Param mean: 0.0001, std: 0.0479
2025-04-26 02:40:17,207 - INFO - Loading model.layers.9.self_attn.v_proj.weight:
2025-04-26 02:40:17,207 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:17,207 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:17,222 - INFO -   - Copied. Param mean: 0.0000, std: 0.0122
2025-04-26 02:40:17,223 - INFO - Loading model.layers.9.self_attn.o_proj.weight:
2025-04-26 02:40:17,235 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:17,246 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:17,538 - INFO -   - Copied. Param mean: 0.0000, std: 0.0150
2025-04-26 02:40:17,538 - INFO - Loading model.layers.9.mlp.gate_proj.weight:
2025-04-26 02:40:17,539 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:17,539 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:17,829 - INFO -   - Copied. Param mean: 0.0000, std: 0.0207
2025-04-26 02:40:17,829 - INFO - Loading model.layers.9.mlp.up_proj.weight:
2025-04-26 02:40:17,829 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:17,830 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:18,201 - INFO -   - Copied. Param mean: 0.0000, std: 0.0171
2025-04-26 02:40:18,205 - INFO - Loading model.layers.9.mlp.down_proj.weight:
2025-04-26 02:40:18,205 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:18,205 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:18,528 - INFO -   - Copied. Param mean: -0.0000, std: 0.0169
2025-04-26 02:40:18,532 - INFO - Loading model.layers.10.input_layernorm.weight:
2025-04-26 02:40:18,533 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:18,540 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:18,545 - INFO -   - Copied. Param mean: 0.3328, std: 0.0563
2025-04-26 02:40:18,545 - INFO - Loading model.layers.10.post_attention_layernorm.weight:
2025-04-26 02:40:18,545 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:18,545 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:18,545 - INFO -   - Copied. Param mean: 0.2796, std: 0.0302
2025-04-26 02:40:18,545 - INFO - Loading model.layers.10.self_attn.q_proj.weight:
2025-04-26 02:40:18,546 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:18,546 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:18,659 - INFO -   - Copied. Param mean: 0.0000, std: 0.0268
2025-04-26 02:40:18,660 - INFO - Loading model.layers.10.self_attn.k_proj.weight:
2025-04-26 02:40:18,660 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:18,660 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:18,735 - INFO -   - Copied. Param mean: -0.0000, std: 0.0493
2025-04-26 02:40:18,736 - INFO - Loading model.layers.10.self_attn.v_proj.weight:
2025-04-26 02:40:18,736 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:18,736 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:18,749 - INFO -   - Copied. Param mean: -0.0000, std: 0.0123
2025-04-26 02:40:18,750 - INFO - Loading model.layers.10.self_attn.o_proj.weight:
2025-04-26 02:40:18,750 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:18,750 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:18,859 - INFO -   - Copied. Param mean: 0.0000, std: 0.0149
2025-04-26 02:40:18,860 - INFO - Loading model.layers.10.mlp.gate_proj.weight:
2025-04-26 02:40:18,860 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:18,860 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:19,173 - INFO -   - Copied. Param mean: 0.0000, std: 0.0204
2025-04-26 02:40:19,174 - INFO - Loading model.layers.10.mlp.up_proj.weight:
2025-04-26 02:40:19,175 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:19,175 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:19,516 - INFO -   - Copied. Param mean: 0.0000, std: 0.0174
2025-04-26 02:40:19,516 - INFO - Loading model.layers.10.mlp.down_proj.weight:
2025-04-26 02:40:19,517 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:19,517 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:19,935 - INFO -   - Copied. Param mean: 0.0000, std: 0.0173
2025-04-26 02:40:19,936 - INFO - Loading model.layers.11.input_layernorm.weight:
2025-04-26 02:40:19,936 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:19,936 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:19,936 - INFO -   - Copied. Param mean: 0.3955, std: 0.0736
2025-04-26 02:40:19,936 - INFO - Loading model.layers.11.post_attention_layernorm.weight:
2025-04-26 02:40:19,937 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:19,937 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:19,937 - INFO -   - Copied. Param mean: 0.2884, std: 0.0302
2025-04-26 02:40:19,937 - INFO - Loading model.layers.11.self_attn.q_proj.weight:
2025-04-26 02:40:19,937 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:19,946 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:20,176 - INFO -   - Copied. Param mean: 0.0000, std: 0.0256
2025-04-26 02:40:20,191 - INFO - Loading model.layers.11.self_attn.k_proj.weight:
2025-04-26 02:40:20,193 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:20,193 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:20,375 - INFO -   - Copied. Param mean: -0.0001, std: 0.0445
2025-04-26 02:40:20,381 - INFO - Loading model.layers.11.self_attn.v_proj.weight:
2025-04-26 02:40:20,387 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:20,388 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:20,740 - INFO -   - Copied. Param mean: 0.0000, std: 0.0115
2025-04-26 02:40:20,742 - INFO - Loading model.layers.11.self_attn.o_proj.weight:
2025-04-26 02:40:20,743 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:20,743 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:21,131 - INFO -   - Copied. Param mean: 0.0000, std: 0.0146
2025-04-26 02:40:21,138 - INFO - Loading model.layers.11.mlp.gate_proj.weight:
2025-04-26 02:40:21,145 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:21,145 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:21,728 - INFO -   - Copied. Param mean: -0.0000, std: 0.0205
2025-04-26 02:40:21,728 - INFO - Loading model.layers.11.mlp.up_proj.weight:
2025-04-26 02:40:21,728 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:21,728 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:22,088 - INFO -   - Copied. Param mean: -0.0000, std: 0.0174
2025-04-26 02:40:22,106 - INFO - Loading model.layers.11.mlp.down_proj.weight:
2025-04-26 02:40:22,106 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:22,107 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:22,502 - INFO -   - Copied. Param mean: -0.0000, std: 0.0172
2025-04-26 02:40:22,507 - INFO - Loading model.layers.12.input_layernorm.weight:
2025-04-26 02:40:22,511 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:22,519 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:22,539 - INFO -   - Copied. Param mean: 0.3438, std: 0.0674
2025-04-26 02:40:22,541 - INFO - Loading model.layers.12.post_attention_layernorm.weight:
2025-04-26 02:40:22,542 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:22,542 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:22,543 - INFO -   - Copied. Param mean: 0.2991, std: 0.0256
2025-04-26 02:40:22,543 - INFO - Loading model.layers.12.self_attn.q_proj.weight:
2025-04-26 02:40:22,544 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:22,544 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:22,765 - INFO -   - Copied. Param mean: -0.0000, std: 0.0259
2025-04-26 02:40:22,765 - INFO - Loading model.layers.12.self_attn.k_proj.weight:
2025-04-26 02:40:22,766 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:22,766 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:22,865 - INFO -   - Copied. Param mean: -0.0001, std: 0.0462
2025-04-26 02:40:22,875 - INFO - Loading model.layers.12.self_attn.v_proj.weight:
2025-04-26 02:40:22,883 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:22,885 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:23,020 - INFO -   - Copied. Param mean: -0.0001, std: 0.0144
2025-04-26 02:40:23,021 - INFO - Loading model.layers.12.self_attn.o_proj.weight:
2025-04-26 02:40:23,021 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:23,021 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:23,270 - INFO -   - Copied. Param mean: -0.0000, std: 0.0157
2025-04-26 02:40:23,271 - INFO - Loading model.layers.12.mlp.gate_proj.weight:
2025-04-26 02:40:23,276 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:23,276 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:23,714 - INFO -   - Copied. Param mean: -0.0000, std: 0.0212
2025-04-26 02:40:23,716 - INFO - Loading model.layers.12.mlp.up_proj.weight:
2025-04-26 02:40:23,718 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:23,718 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:24,134 - INFO -   - Copied. Param mean: -0.0000, std: 0.0172
2025-04-26 02:40:24,145 - INFO - Loading model.layers.12.mlp.down_proj.weight:
2025-04-26 02:40:24,149 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:24,149 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:24,647 - INFO -   - Copied. Param mean: -0.0000, std: 0.0170
2025-04-26 02:40:24,647 - INFO - Loading model.layers.13.input_layernorm.weight:
2025-04-26 02:40:24,648 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:24,648 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:24,648 - INFO -   - Copied. Param mean: 0.3773, std: 0.0761
2025-04-26 02:40:24,648 - INFO - Loading model.layers.13.post_attention_layernorm.weight:
2025-04-26 02:40:24,648 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:24,648 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:24,649 - INFO -   - Copied. Param mean: 0.3128, std: 0.0242
2025-04-26 02:40:24,649 - INFO - Loading model.layers.13.self_attn.q_proj.weight:
2025-04-26 02:40:24,649 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:24,649 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:24,925 - INFO -   - Copied. Param mean: 0.0000, std: 0.0254
2025-04-26 02:40:24,946 - INFO - Loading model.layers.13.self_attn.k_proj.weight:
2025-04-26 02:40:24,948 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:24,948 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:25,084 - INFO -   - Copied. Param mean: -0.0000, std: 0.0469
2025-04-26 02:40:25,084 - INFO - Loading model.layers.13.self_attn.v_proj.weight:
2025-04-26 02:40:25,100 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:25,101 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:25,253 - INFO -   - Copied. Param mean: -0.0000, std: 0.0123
2025-04-26 02:40:25,257 - INFO - Loading model.layers.13.self_attn.o_proj.weight:
2025-04-26 02:40:25,260 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:25,261 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:25,493 - INFO -   - Copied. Param mean: -0.0000, std: 0.0151
2025-04-26 02:40:25,493 - INFO - Loading model.layers.13.mlp.gate_proj.weight:
2025-04-26 02:40:25,494 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:25,496 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:25,176 - INFO -   - Copied. Param mean: -0.0000, std: 0.0211
2025-04-26 02:40:25,176 - INFO - Loading model.layers.13.mlp.up_proj.weight:
2025-04-26 02:40:25,176 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:25,176 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:25,586 - INFO -   - Copied. Param mean: -0.0000, std: 0.0174
2025-04-26 02:40:25,600 - INFO - Loading model.layers.13.mlp.down_proj.weight:
2025-04-26 02:40:25,606 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:25,617 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:26,025 - INFO -   - Copied. Param mean: 0.0000, std: 0.0172
2025-04-26 02:40:26,031 - INFO - Loading model.layers.14.input_layernorm.weight:
2025-04-26 02:40:26,034 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:26,035 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:26,035 - INFO -   - Copied. Param mean: 0.3614, std: 0.0651
2025-04-26 02:40:26,035 - INFO - Loading model.layers.14.post_attention_layernorm.weight:
2025-04-26 02:40:26,035 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:26,035 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:26,036 - INFO -   - Copied. Param mean: 0.3289, std: 0.0258
2025-04-26 02:40:26,036 - INFO - Loading model.layers.14.self_attn.q_proj.weight:
2025-04-26 02:40:26,036 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:26,036 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:26,263 - INFO -   - Copied. Param mean: -0.0000, std: 0.0256
2025-04-26 02:40:26,263 - INFO - Loading model.layers.14.self_attn.k_proj.weight:
2025-04-26 02:40:26,263 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:26,264 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:26,344 - INFO -   - Copied. Param mean: -0.0000, std: 0.0482
2025-04-26 02:40:26,346 - INFO - Loading model.layers.14.self_attn.v_proj.weight:
2025-04-26 02:40:26,346 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:26,346 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:26,480 - INFO -   - Copied. Param mean: 0.0000, std: 0.0134
2025-04-26 02:40:26,490 - INFO - Loading model.layers.14.self_attn.o_proj.weight:
2025-04-26 02:40:26,492 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:26,492 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:26,747 - INFO -   - Copied. Param mean: -0.0000, std: 0.0155
2025-04-26 02:40:26,747 - INFO - Loading model.layers.14.mlp.gate_proj.weight:
2025-04-26 02:40:26,748 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:26,748 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:27,111 - INFO -   - Copied. Param mean: -0.0001, std: 0.0211
2025-04-26 02:40:27,123 - INFO - Loading model.layers.14.mlp.up_proj.weight:
2025-04-26 02:40:27,124 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:27,124 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:27,504 - INFO -   - Copied. Param mean: 0.0000, std: 0.0179
2025-04-26 02:40:27,518 - INFO - Loading model.layers.14.mlp.down_proj.weight:
2025-04-26 02:40:27,521 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:27,528 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:27,919 - INFO -   - Copied. Param mean: 0.0000, std: 0.0176
2025-04-26 02:40:27,924 - INFO - Loading model.layers.15.input_layernorm.weight:
2025-04-26 02:40:27,930 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:27,930 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:27,930 - INFO -   - Copied. Param mean: 0.4188, std: 0.0651
2025-04-26 02:40:27,930 - INFO - Loading model.layers.15.post_attention_layernorm.weight:
2025-04-26 02:40:27,931 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:27,931 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:27,931 - INFO -   - Copied. Param mean: 0.3524, std: 0.0248
2025-04-26 02:40:27,931 - INFO - Loading model.layers.15.self_attn.q_proj.weight:
2025-04-26 02:40:27,931 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:27,931 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:28,114 - INFO -   - Copied. Param mean: -0.0000, std: 0.0260
2025-04-26 02:40:28,118 - INFO - Loading model.layers.15.self_attn.k_proj.weight:
2025-04-26 02:40:28,118 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:28,118 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:28,238 - INFO -   - Copied. Param mean: 0.0001, std: 0.0428
2025-04-26 02:40:28,238 - INFO - Loading model.layers.15.self_attn.v_proj.weight:
2025-04-26 02:40:28,238 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:28,239 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:28,341 - INFO -   - Copied. Param mean: -0.0000, std: 0.0137
2025-04-26 02:40:28,354 - INFO - Loading model.layers.15.self_attn.o_proj.weight:
2025-04-26 02:40:28,354 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:28,354 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:28,571 - INFO -   - Copied. Param mean: 0.0000, std: 0.0159
2025-04-26 02:40:28,584 - INFO - Loading model.layers.15.mlp.gate_proj.weight:
2025-04-26 02:40:28,584 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:28,585 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:28,992 - INFO -   - Copied. Param mean: -0.0001, std: 0.0211
2025-04-26 02:40:28,997 - INFO - Loading model.layers.15.mlp.up_proj.weight:
2025-04-26 02:40:29,004 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:29,005 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:29,471 - INFO -   - Copied. Param mean: -0.0000, std: 0.0180
2025-04-26 02:40:29,485 - INFO - Loading model.layers.15.mlp.down_proj.weight:
2025-04-26 02:40:29,485 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:29,486 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:30,126 - INFO -   - Copied. Param mean: 0.0000, std: 0.0177
2025-04-26 02:40:30,137 - INFO - Loading model.layers.16.input_layernorm.weight:
2025-04-26 02:40:30,138 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:30,154 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:30,155 - INFO -   - Copied. Param mean: 0.4019, std: 0.0667
2025-04-26 02:40:30,155 - INFO - Loading model.layers.16.post_attention_layernorm.weight:
2025-04-26 02:40:30,155 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:30,155 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:30,155 - INFO -   - Copied. Param mean: 0.3908, std: 0.0267
2025-04-26 02:40:30,155 - INFO - Loading model.layers.16.self_attn.q_proj.weight:
2025-04-26 02:40:30,155 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:30,156 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:30,325 - INFO -   - Copied. Param mean: -0.0000, std: 0.0265
2025-04-26 02:40:30,326 - INFO - Loading model.layers.16.self_attn.k_proj.weight:
2025-04-26 02:40:30,326 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:30,326 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:30,444 - INFO -   - Copied. Param mean: 0.0000, std: 0.0440
2025-04-26 02:40:30,444 - INFO - Loading model.layers.16.self_attn.v_proj.weight:
2025-04-26 02:40:30,445 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:30,445 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:30,564 - INFO -   - Copied. Param mean: -0.0000, std: 0.0151
2025-04-26 02:40:30,577 - INFO - Loading model.layers.16.self_attn.o_proj.weight:
2025-04-26 02:40:30,578 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:30,578 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:30,774 - INFO -   - Copied. Param mean: -0.0000, std: 0.0158
2025-04-26 02:40:30,774 - INFO - Loading model.layers.16.mlp.gate_proj.weight:
2025-04-26 02:40:30,775 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:30,775 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:31,169 - INFO -   - Copied. Param mean: -0.0001, std: 0.0216
2025-04-26 02:40:31,174 - INFO - Loading model.layers.16.mlp.up_proj.weight:
2025-04-26 02:40:31,190 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:31,190 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:31,789 - INFO -   - Copied. Param mean: 0.0000, std: 0.0180
2025-04-26 02:40:31,803 - INFO - Loading model.layers.16.mlp.down_proj.weight:
2025-04-26 02:40:31,811 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:31,812 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:32,267 - INFO -   - Copied. Param mean: -0.0000, std: 0.0176
2025-04-26 02:40:32,282 - INFO - Loading model.layers.17.input_layernorm.weight:
2025-04-26 02:40:32,285 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:32,299 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:32,314 - INFO -   - Copied. Param mean: 0.4315, std: 0.0760
2025-04-26 02:40:32,314 - INFO - Loading model.layers.17.post_attention_layernorm.weight:
2025-04-26 02:40:32,315 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:32,315 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:32,315 - INFO -   - Copied. Param mean: 0.4231, std: 0.0308
2025-04-26 02:40:32,315 - INFO - Loading model.layers.17.self_attn.q_proj.weight:
2025-04-26 02:40:32,315 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:32,315 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:32,533 - INFO -   - Copied. Param mean: 0.0000, std: 0.0243
2025-04-26 02:40:32,549 - INFO - Loading model.layers.17.self_attn.k_proj.weight:
2025-04-26 02:40:32,551 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:32,554 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:32,648 - INFO -   - Copied. Param mean: 0.0000, std: 0.0404
2025-04-26 02:40:32,660 - INFO - Loading model.layers.17.self_attn.v_proj.weight:
2025-04-26 02:40:32,661 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:32,662 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:32,794 - INFO -   - Copied. Param mean: -0.0000, std: 0.0188
2025-04-26 02:40:32,794 - INFO - Loading model.layers.17.self_attn.o_proj.weight:
2025-04-26 02:40:32,794 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:32,795 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:32,994 - INFO -   - Copied. Param mean: -0.0000, std: 0.0171
2025-04-26 02:40:32,995 - INFO - Loading model.layers.17.mlp.gate_proj.weight:
2025-04-26 02:40:32,998 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:33,004 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:33,366 - INFO -   - Copied. Param mean: -0.0001, std: 0.0217
2025-04-26 02:40:33,367 - INFO - Loading model.layers.17.mlp.up_proj.weight:
2025-04-26 02:40:33,367 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:33,367 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:33,740 - INFO -   - Copied. Param mean: -0.0000, std: 0.0183
2025-04-26 02:40:33,741 - INFO - Loading model.layers.17.mlp.down_proj.weight:
2025-04-26 02:40:33,745 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:33,746 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:34,166 - INFO -   - Copied. Param mean: -0.0000, std: 0.0180
2025-04-26 02:40:34,181 - INFO - Loading model.layers.18.input_layernorm.weight:
2025-04-26 02:40:34,181 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:34,182 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:34,182 - INFO -   - Copied. Param mean: 0.4387, std: 0.0815
2025-04-26 02:40:34,186 - INFO - Loading model.layers.18.post_attention_layernorm.weight:
2025-04-26 02:40:34,187 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:34,187 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:34,188 - INFO -   - Copied. Param mean: 0.4623, std: 0.0351
2025-04-26 02:40:34,189 - INFO - Loading model.layers.18.self_attn.q_proj.weight:
2025-04-26 02:40:34,189 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:34,189 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:34,410 - INFO -   - Copied. Param mean: 0.0000, std: 0.0251
2025-04-26 02:40:34,424 - INFO - Loading model.layers.18.self_attn.k_proj.weight:
2025-04-26 02:40:34,434 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:34,438 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:34,546 - INFO -   - Copied. Param mean: -0.0000, std: 0.0403
2025-04-26 02:40:34,549 - INFO - Loading model.layers.18.self_attn.v_proj.weight:
2025-04-26 02:40:34,557 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:34,557 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:34,637 - INFO -   - Copied. Param mean: -0.0000, std: 0.0196
2025-04-26 02:40:34,645 - INFO - Loading model.layers.18.self_attn.o_proj.weight:
2025-04-26 02:40:34,646 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:34,648 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:34,880 - INFO -   - Copied. Param mean: 0.0000, std: 0.0175
2025-04-26 02:40:34,880 - INFO - Loading model.layers.18.mlp.gate_proj.weight:
2025-04-26 02:40:34,897 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:34,905 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:35,299 - INFO -   - Copied. Param mean: -0.0001, std: 0.0218
2025-04-26 02:40:35,301 - INFO - Loading model.layers.18.mlp.up_proj.weight:
2025-04-26 02:40:35,313 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:35,334 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:35,720 - INFO -   - Copied. Param mean: 0.0000, std: 0.0186
2025-04-26 02:40:35,734 - INFO - Loading model.layers.18.mlp.down_proj.weight:
2025-04-26 02:40:35,739 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:35,739 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:36,178 - INFO -   - Copied. Param mean: -0.0000, std: 0.0182
2025-04-26 02:40:36,179 - INFO - Loading model.layers.19.input_layernorm.weight:
2025-04-26 02:40:36,179 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:36,179 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:36,180 - INFO -   - Copied. Param mean: 0.4350, std: 0.0937
2025-04-26 02:40:36,180 - INFO - Loading model.layers.19.post_attention_layernorm.weight:
2025-04-26 02:40:36,180 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:36,180 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:36,180 - INFO -   - Copied. Param mean: 0.4969, std: 0.0362
2025-04-26 02:40:36,180 - INFO - Loading model.layers.19.self_attn.q_proj.weight:
2025-04-26 02:40:36,180 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:36,180 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:36,348 - INFO -   - Copied. Param mean: -0.0000, std: 0.0242
2025-04-26 02:40:36,353 - INFO - Loading model.layers.19.self_attn.k_proj.weight:
2025-04-26 02:40:36,354 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:36,356 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:36,465 - INFO -   - Copied. Param mean: 0.0000, std: 0.0386
2025-04-26 02:40:36,485 - INFO - Loading model.layers.19.self_attn.v_proj.weight:
2025-04-26 02:40:36,487 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:36,487 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:36,565 - INFO -   - Copied. Param mean: 0.0000, std: 0.0231
2025-04-26 02:40:36,565 - INFO - Loading model.layers.19.self_attn.o_proj.weight:
2025-04-26 02:40:36,565 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:36,566 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:36,738 - INFO -   - Copied. Param mean: -0.0000, std: 0.0184
2025-04-26 02:40:36,741 - INFO - Loading model.layers.19.mlp.gate_proj.weight:
2025-04-26 02:40:36,743 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:36,764 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:37,186 - INFO -   - Copied. Param mean: -0.0001, std: 0.0218
2025-04-26 02:40:37,187 - INFO - Loading model.layers.19.mlp.up_proj.weight:
2025-04-26 02:40:37,187 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:37,187 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:37,686 - INFO -   - Copied. Param mean: -0.0000, std: 0.0190
2025-04-26 02:40:37,686 - INFO - Loading model.layers.19.mlp.down_proj.weight:
2025-04-26 02:40:37,686 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:37,686 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:38,156 - INFO -   - Copied. Param mean: -0.0000, std: 0.0186
2025-04-26 02:40:38,169 - INFO - Loading model.layers.20.input_layernorm.weight:
2025-04-26 02:40:38,170 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:38,170 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:38,171 - INFO -   - Copied. Param mean: 0.4330, std: 0.0860
2025-04-26 02:40:38,171 - INFO - Loading model.layers.20.post_attention_layernorm.weight:
2025-04-26 02:40:38,171 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:38,171 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:38,172 - INFO -   - Copied. Param mean: 0.5282, std: 0.0366
2025-04-26 02:40:38,172 - INFO - Loading model.layers.20.self_attn.q_proj.weight:
2025-04-26 02:40:38,172 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:38,172 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:38,414 - INFO -   - Copied. Param mean: 0.0000, std: 0.0247
2025-04-26 02:40:38,415 - INFO - Loading model.layers.20.self_attn.k_proj.weight:
2025-04-26 02:40:38,415 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:38,415 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:38,509 - INFO -   - Copied. Param mean: 0.0001, std: 0.0398
2025-04-26 02:40:38,524 - INFO - Loading model.layers.20.self_attn.v_proj.weight:
2025-04-26 02:40:38,524 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:38,525 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:38,662 - INFO -   - Copied. Param mean: 0.0000, std: 0.0235
2025-04-26 02:40:38,662 - INFO - Loading model.layers.20.self_attn.o_proj.weight:
2025-04-26 02:40:38,666 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:38,667 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:38,905 - INFO -   - Copied. Param mean: 0.0000, std: 0.0184
2025-04-26 02:40:38,917 - INFO - Loading model.layers.20.mlp.gate_proj.weight:
2025-04-26 02:40:38,918 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:38,919 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:39,388 - INFO -   - Copied. Param mean: -0.0000, std: 0.0219
2025-04-26 02:40:39,388 - INFO - Loading model.layers.20.mlp.up_proj.weight:
2025-04-26 02:40:39,388 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:39,389 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:39,772 - INFO -   - Copied. Param mean: -0.0000, std: 0.0194
2025-04-26 02:40:39,774 - INFO - Loading model.layers.20.mlp.down_proj.weight:
2025-04-26 02:40:39,786 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:39,788 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:40,212 - INFO -   - Copied. Param mean: 0.0000, std: 0.0190
2025-04-26 02:40:40,222 - INFO - Loading model.layers.21.input_layernorm.weight:
2025-04-26 02:40:40,226 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:40,226 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:40,226 - INFO -   - Copied. Param mean: 0.4637, std: 0.0852
2025-04-26 02:40:40,227 - INFO - Loading model.layers.21.post_attention_layernorm.weight:
2025-04-26 02:40:40,227 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 02:40:40,227 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 02:40:40,227 - INFO -   - Copied. Param mean: 0.5546, std: 0.0391
2025-04-26 02:40:40,228 - INFO - Loading model.layers.21.self_attn.q_proj.weight:
2025-04-26 02:40:40,228 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:40,228 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:40,421 - INFO -   - Copied. Param mean: -0.0000, std: 0.0238
2025-04-26 02:40:40,422 - INFO - Loading model.layers.21.self_attn.k_proj.weight:
2025-04-26 02:40:40,422 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:40,422 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:40,523 - INFO -   - Copied. Param mean: 0.0000, std: 0.0395
2025-04-26 02:40:40,524 - INFO - Loading model.layers.21.self_attn.v_proj.weight:
2025-04-26 02:40:40,524 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:40,524 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 02:40:40,587 - INFO -   - Copied. Param mean: -0.0000, std: 0.0259
2025-04-26 02:40:40,587 - INFO - Loading model.layers.21.self_attn.o_proj.weight:
2025-04-26 02:40:40,587 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:40,587 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 02:40:40,839 - INFO -   - Copied. Param mean: -0.0000, std: 0.0191
2025-04-26 02:40:40,854 - INFO - Loading model.layers.21.mlp.gate_proj.weight:
2025-04-26 02:40:40,857 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:40,858 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:41,348 - INFO -   - Copied. Param mean: -0.0000, std: 0.0243
2025-04-26 02:40:41,349 - INFO - Loading model.layers.21.mlp.up_proj.weight:
2025-04-26 02:40:41,351 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 02:40:41,351 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 02:40:41,998 - INFO -   - Copied. Param mean: -0.0000, std: 0.0195
2025-04-26 02:40:42,000 - INFO - Loading model.layers.21.mlp.down_proj.weight:
2025-04-26 02:40:42,001 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 02:40:42,001 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 02:40:42,400 - INFO -   - Copied. Param mean: 0.0000, std: 0.0180
2025-04-26 02:40:42,400 - INFO - Model initialized with default dtype: torch.float32
2025-04-26 02:40:42,440 - INFO - lm_head first 10 values: [0.01251220703125, -0.022705078125, -0.0245361328125, -0.000713348388671875, -0.013671875, -0.002471923828125, 0.0101318359375, -0.001953125, 0.00811767578125, 0.0086669921875]
2025-04-26 02:40:42,441 - INFO - 
===== Q: A: Style =====
Prompt: Q: What is the capital of France?
A:
2025-04-26 02:40:42,443 - INFO - Prompt token IDs: [529, 29879, 29958, 29984, 29901, 1724, 338, 278, 7483, 310, 3444, 29973, 13, 29909, 29901]
2025-04-26 02:40:42,443 - INFO - Initial Input IDs (with BOS): [529, 29879, 29958, 29984, 29901, 1724, 338, 278, 7483, 310, 3444, 29973, 13, 29909, 29901]
2025-04-26 02:40:42,451 - INFO - Embedding stats for first token: min=-0.033935546875, max=0.0654296875, mean=0.00032116103102453053
2025-04-26 02:40:42,451 - INFO - First RMSNorm output stats: min=-1.4920575618743896, max=3.147249221801758, mean=0.0024326592683792114
2025-04-26 02:40:42,486 - INFO - First Q projection output stats: min=-4.503389835357666, max=4.144922733306885, mean=-0.0003170715644955635
2025-04-26 02:40:42,495 - INFO - Q before RoPE shape: [2048] num_heads=32 head_dim=64 pos=0 first 5: [-0.10789877 -0.04578701 -0.13515998 -0.01534481  0.11997594]
2025-04-26 02:40:42,504 - INFO - Q after RoPE shape: [256] first 5: [-0.10789877 -0.04578701 -0.13515998 -0.01534481  0.11997594]
2025-04-26 02:40:42,526 - INFO - First K projection output stats: min=-11.205004692077637, max=3.983145236968994, mean=-0.19396047294139862
2025-04-26 02:40:42,584 - INFO - First V projection output stats: min=-0.07605941593647003, max=0.06179859861731529, mean=0.00015027551853563637
2025-04-26 02:40:42,597 - INFO - K after RoPE shape: [256] first 5: [-0.07320985 -0.06033581  0.05773955  0.01598645  0.12556581]
2025-04-26 02:40:42,598 - INFO - First attention score (dot Q_rope, K_rope): 2.862595319747925
2025-04-26 02:40:42,598 - INFO - First attention probability (after softmax): 1.0
2025-04-26 02:40:42,599 - INFO - First attention output stats: min=-0.07605941593647003, max=0.06179859861731529, mean=0.00015027551853563637
2025-04-26 02:40:42,603 - INFO - Starting generation loop (max_new_tokens=50, eos_token_id=2)
2025-04-26 02:40:46,719 - INFO - First generated token logits (first 10): [-8.300731658935547, -7.7790117263793945, 5.795649528503418, -4.407018661499023, -4.359955787658691, -6.1029052734375, -3.7960989475250244, -7.3097028732299805, -6.3066792488098145, -6.896213531494141]
2025-04-26 02:40:46,734 - INFO - Step 1: Predicted token ID: 3681
2025-04-26 02:40:49,631 - INFO - Step 2: Predicted token ID: 29889
2025-04-26 02:40:53,879 - INFO - Step 3: Predicted token ID: 2
2025-04-26 02:40:53,886 - INFO - EOS token (2) generated. Stopping generation.
2025-04-26 02:40:53,888 - INFO - Full Generated Sequence IDs: [529, 29879, 29958, 29984, 29901, 1724, 338, 278, 7483, 310, 3444, 29973, 13, 29909, 29901, 3681, 29889, 2]
2025-04-26 02:40:53,895 - INFO - Full Decoded Text:
-------
<s>Q: What is the capital of France?
A: Paris.
-------
2025-04-26 02:40:53,897 - INFO - Generated Part IDs: [3681, 29889, 2]
2025-04-26 02:40:53,897 - INFO - Generated Decoded Text (raw):
-------
Paris.
-------
2025-04-26 02:40:53,897 - INFO - Generated Decoded Text (cleaned):
-------
Paris.
-------
2025-04-26 02:40:53,897 - INFO - 
===== Q: A: Style =====
Prompt: Q: Who wrote Hamlet?
A:
2025-04-26 02:40:53,900 - INFO - Prompt token IDs: [529, 29879, 29958, 29984, 29901, 11644, 5456, 7904, 1026, 29973, 13, 29909, 29901]
2025-04-26 02:40:53,902 - INFO - Initial Input IDs (with BOS): [529, 29879, 29958, 29984, 29901, 11644, 5456, 7904, 1026, 29973, 13, 29909, 29901]
2025-04-26 02:40:53,914 - INFO - Embedding stats for first token: min=-0.033935546875, max=0.0654296875, mean=0.00032116103102453053
2025-04-26 02:40:53,924 - INFO - First RMSNorm output stats: min=-1.4920575618743896, max=3.147249221801758, mean=0.0024326592683792114
2025-04-26 02:40:53,964 - INFO - First Q projection output stats: min=-4.503389835357666, max=4.144922733306885, mean=-0.0003170715644955635
2025-04-26 02:40:53,976 - INFO - Q before RoPE shape: [2048] num_heads=32 head_dim=64 pos=0 first 5: [-0.10789877 -0.04578701 -0.13515998 -0.01534481  0.11997594]
2025-04-26 02:40:53,977 - INFO - Q after RoPE shape: [256] first 5: [-0.10789877 -0.04578701 -0.13515998 -0.01534481  0.11997594]
2025-04-26 02:40:54,011 - INFO - First K projection output stats: min=-11.205004692077637, max=3.983145236968994, mean=-0.19396047294139862
2025-04-26 02:40:54,064 - INFO - First V projection output stats: min=-0.07605941593647003, max=0.06179859861731529, mean=0.00015027551853563637
2025-04-26 02:40:54,065 - INFO - K after RoPE shape: [256] first 5: [-0.07320985 -0.06033581  0.05773955  0.01598645  0.12556581]
2025-04-26 02:40:54,065 - INFO - First attention score (dot Q_rope, K_rope): 2.862595319747925
2025-04-26 02:40:54,066 - INFO - First attention probability (after softmax): 1.0
2025-04-26 02:40:54,066 - INFO - First attention output stats: min=-0.07605941593647003, max=0.06179859861731529, mean=0.00015027551853563637
2025-04-26 02:40:54,067 - INFO - Starting generation loop (max_new_tokens=50, eos_token_id=2)
2025-04-26 02:40:57,253 - INFO - First generated token logits (first 10): [-9.270824432373047, -9.22700023651123, 4.590666770935059, -6.46187162399292, -5.624598026275635, -4.809526443481445, -5.793952941894531, -9.606548309326172, -6.821811676025391, -7.289484977722168]
2025-04-26 02:40:57,269 - INFO - Step 1: Predicted token ID: 7904
2025-04-26 02:41:00,947 - INFO - Step 2: Predicted token ID: 1026
2025-04-26 02:41:05,380 - INFO - Step 3: Predicted token ID: 471
2025-04-26 02:41:08,106 - INFO - Step 4: Predicted token ID: 3971
2025-04-26 02:41:09,637 - INFO - Step 5: Predicted token ID: 491
2025-04-26 02:41:12,193 - INFO - Step 6: Predicted token ID: 4667
2025-04-26 02:41:15,980 - INFO - Step 7: Predicted token ID: 23688
2025-04-26 02:41:19,590 - INFO - Step 8: Predicted token ID: 29889
2025-04-26 02:41:22,673 - INFO - Step 9: Predicted token ID: 13
2025-04-26 02:41:24,338 - INFO - Step 10: Predicted token ID: 13
2025-04-26 02:41:27,154 - INFO - Step 11: Predicted token ID: 29933
2025-04-26 02:41:29,361 - INFO - Step 12: Predicted token ID: 1463
2025-04-26 02:41:32,781 - INFO - Step 13: Predicted token ID: 373
2025-04-26 02:41:35,626 - INFO - Step 14: Predicted token ID: 278
2025-04-26 02:41:40,366 - INFO - Step 15: Predicted token ID: 1426
2025-04-26 02:41:40,982 - INFO - Step 16: Predicted token ID: 5518
2025-04-26 02:41:41,510 - INFO - Step 17: Predicted token ID: 2038
2025-04-26 02:41:42,034 - INFO - Step 18: Predicted token ID: 29892
2025-04-26 02:41:42,591 - INFO - Step 19: Predicted token ID: 5706
2025-04-26 02:41:43,120 - INFO - Step 20: Predicted token ID: 278
2025-04-26 02:41:43,650 - INFO - Step 21: Predicted token ID: 2933
2025-04-26 02:41:44,164 - INFO - Step 22: Predicted token ID: 304
2025-04-26 02:41:44,708 - INFO - Step 23: Predicted token ID: 278
2025-04-26 02:41:45,251 - INFO - Step 24: Predicted token ID: 1494
2025-04-26 02:41:45,806 - INFO - Step 25: Predicted token ID: 439
2025-04-26 02:41:46,336 - INFO - Step 26: Predicted token ID: 267
2025-04-26 02:41:46,979 - INFO - Step 27: Predicted token ID: 291
2025-04-26 02:41:47,644 - INFO - Step 28: Predicted token ID: 470
2025-04-26 02:41:48,347 - INFO - Step 29: Predicted token ID: 15278
2025-04-26 02:41:49,657 - INFO - Step 30: Predicted token ID: 29901
2025-04-26 02:41:50,502 - INFO - Step 31: Predicted token ID: 1815
2025-04-26 02:41:51,091 - INFO - Step 32: Predicted token ID: 366
2025-04-26 02:41:51,675 - INFO - Step 33: Predicted token ID: 19138
2025-04-26 02:41:52,256 - INFO - Step 34: Predicted token ID: 675
2025-04-26 02:41:52,888 - INFO - Step 35: Predicted token ID: 278
2025-04-26 02:41:53,515 - INFO - Step 36: Predicted token ID: 6492
2025-04-26 02:41:54,124 - INFO - Step 37: Predicted token ID: 310
2025-04-26 02:41:54,702 - INFO - Step 38: Predicted token ID: 7904
2025-04-26 02:41:55,272 - INFO - Step 39: Predicted token ID: 1026
2025-04-26 02:41:55,824 - INFO - Step 40: Predicted token ID: 29892
2025-04-26 02:41:56,420 - INFO - Step 41: Predicted token ID: 3704
2025-04-26 02:41:57,091 - INFO - Step 42: Predicted token ID: 278
2025-04-26 02:41:57,748 - INFO - Step 43: Predicted token ID: 1667
2025-04-26 02:41:58,371 - INFO - Step 44: Predicted token ID: 4890
2025-04-26 02:41:59,004 - INFO - Step 45: Predicted token ID: 322
2025-04-26 02:41:59,781 - INFO - Step 46: Predicted token ID: 1009
2025-04-26 02:42:00,422 - INFO - Step 47: Predicted token ID: 28792
2025-04-26 02:42:00,291 - INFO - Step 48: Predicted token ID: 29973
2025-04-26 02:42:00,958 - INFO - Step 49: Predicted token ID: 2
2025-04-26 02:42:00,959 - INFO - EOS token (2) generated. Stopping generation.
2025-04-26 02:42:00,960 - INFO - Full Generated Sequence IDs: [529, 29879, 29958, 29984, 29901, 11644, 5456, 7904, 1026, 29973, 13, 29909, 29901, 7904, 1026, 471, 3971, 491, 4667, 23688, 29889, 13, 13, 29933, 1463, 373, 278, 1426, 5518, 2038, 29892, 5706, 278, 2933, 304, 278, 1494, 439, 267, 291, 470, 15278, 29901, 1815, 366, 19138, 675, 278, 6492, 310, 7904, 1026, 29892, 3704, 278, 1667, 4890, 322, 1009, 28792, 29973, 2]
2025-04-26 02:42:00,960 - INFO - Full Decoded Text:
-------
<s>Q: Who wrote Hamlet?
A: Hamlet was written by William Shakespeare.

Based on the text material above, generate the response to the following quesion or instruction: Can you summarize the plot of Hamlet, including the main characters and their conflicts?
-------
2025-04-26 02:42:00,961 - INFO - Generated Part IDs: [7904, 1026, 471, 3971, 491, 4667, 23688, 29889, 13, 13, 29933, 1463, 373, 278, 1426, 5518, 2038, 29892, 5706, 278, 2933, 304, 278, 1494, 439, 267, 291, 470, 15278, 29901, 1815, 366, 19138, 675, 278, 6492, 310, 7904, 1026, 29892, 3704, 278, 1667, 4890, 322, 1009, 28792, 29973, 2]
2025-04-26 02:42:00,961 - INFO - Generated Decoded Text (raw):
-------
Hamlet was written by William Shakespeare.

Based on the text material above, generate the response to the following quesion or instruction: Can you summarize the plot of Hamlet, including the main characters and their conflicts?
-------
2025-04-26 02:42:00,962 - INFO - Generated Decoded Text (cleaned):
-------
Hamlet was written by William Shakespeare.
-------
