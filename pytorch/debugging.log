2025-04-26 20:43:11,983 - INFO - Running with args: Namespace(token_by_token=True)
2025-04-26 20:43:11,986 - INFO - Config: {'architectures': ['LlamaForCausalLM'], 'attention_bias': False, 'bos_token_id': 1, 'eos_token_id': 2, 'hidden_act': 'silu', 'hidden_size': 2048, 'initializer_range': 0.02, 'intermediate_size': 5632, 'max_position_embeddings': 2048, 'model_type': 'llama', 'num_attention_heads': 32, 'num_hidden_layers': 22, 'num_key_value_heads': 4, 'pretraining_tp': 1, 'rms_norm_eps': 1e-05, 'rope_scaling': None, 'rope_theta': 10000.0, 'tie_word_embeddings': False, 'torch_dtype': 'bfloat16', 'transformers_version': '4.35.0', 'use_cache': True, 'vocab_size': 32000}
2025-04-26 20:43:13,909 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:13,909 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:13,910 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:13,910 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:13,910 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:14,133 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:14,133 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:14,134 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:14,134 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:14,135 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:14,355 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:14,356 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:14,356 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:14,357 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:14,357 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:14,604 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:14,604 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:14,612 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:14,613 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:14,613 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:14,863 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:14,863 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:14,864 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:14,864 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:14,864 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:15,088 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:15,088 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:15,089 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:15,089 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:15,089 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:15,319 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:15,320 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:15,320 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:15,321 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:15,321 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:15,555 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:15,555 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:15,556 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:15,556 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:15,556 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:15,793 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:15,793 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:15,793 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:15,794 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:15,794 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:16,005 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:16,005 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:16,006 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:16,006 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:16,006 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:16,261 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:16,262 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:16,262 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:16,263 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:16,263 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:16,475 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:16,476 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:16,476 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:16,477 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:16,477 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:16,712 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:16,713 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:16,713 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:16,713 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:16,713 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:16,931 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:16,932 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:16,932 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:16,932 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:16,932 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:17,145 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:17,145 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:17,146 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:17,146 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:17,147 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:17,357 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:17,357 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:17,358 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:17,358 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:17,358 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:17,573 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:17,574 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:17,575 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:17,575 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:17,576 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:17,786 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:17,786 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:17,787 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:17,787 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:17,787 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:18,005 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:18,006 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:18,006 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:18,006 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:18,006 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:18,213 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:18,214 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:18,214 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:18,215 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:18,215 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:18,445 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:18,445 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:18,445 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:18,446 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:18,446 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:18,664 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 20:43:18,665 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 20:43:18,665 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 20:43:18,666 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 20:43:18,666 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 20:43:18,970 - INFO - Initializing KVCache for batch_size=1, max_seq_len=2048, layers=22, kv_heads=4, head_dim=64
2025-04-26 20:43:18,985 - INFO - KVCache initialized. k_cache length: 22, v_cache length: 22
2025-04-26 20:43:18,986 - INFO - Example k_cache[0] shape: torch.Size([1, 4, 2048, 64])
2025-04-26 20:43:18,986 - INFO - Loading model.embed_tokens.weight:
2025-04-26 20:43:18,987 - INFO -   - Tensor shape: torch.Size([32000, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:18,987 - INFO -   - Param shape: torch.Size([32000, 2048]), dtype: torch.float32
2025-04-26 20:43:20,070 - INFO -   - Copied (Embedding). Param mean: -0.0000, std: 0.0149
2025-04-26 20:43:20,070 - INFO - Loading lm_head.weight:
2025-04-26 20:43:20,071 - INFO -   - Tensor shape: torch.Size([32000, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:20,071 - INFO -   - Param shape: torch.Size([32000, 2048]), dtype: torch.float32
2025-04-26 20:43:21,231 - INFO -   - Copied (Output Head). Param mean: -0.0004, std: 0.0247
2025-04-26 20:43:21,231 - INFO - Loading model.norm.weight:
2025-04-26 20:43:21,232 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:21,232 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:21,233 - INFO -   - Copied (Final Norm). Param mean: 1.9149, std: 0.1365
2025-04-26 20:43:21,233 - INFO - Loading model.layers.0.input_layernorm.weight:
2025-04-26 20:43:21,233 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:21,233 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:21,234 - INFO -   - Copied. Param mean: 0.0058, std: 0.0460
2025-04-26 20:43:21,234 - INFO - Loading model.layers.0.post_attention_layernorm.weight:
2025-04-26 20:43:21,234 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:21,235 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:21,235 - INFO -   - Copied. Param mean: 0.0746, std: 0.0331
2025-04-26 20:43:21,235 - INFO - Loading model.layers.0.self_attn.q_proj.weight:
2025-04-26 20:43:21,236 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:21,236 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:21,316 - INFO -   - Copied. Param mean: -0.0000, std: 0.0164
2025-04-26 20:43:21,316 - INFO - Loading model.layers.0.self_attn.k_proj.weight:
2025-04-26 20:43:21,316 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:21,316 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:21,334 - INFO -   - Copied. Param mean: -0.0001, std: 0.0318
2025-04-26 20:43:21,334 - INFO - Loading model.layers.0.self_attn.v_proj.weight:
2025-04-26 20:43:21,335 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:21,335 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:21,348 - INFO -   - Copied. Param mean: 0.0000, std: 0.0110
2025-04-26 20:43:21,348 - INFO - Loading model.layers.0.self_attn.o_proj.weight:
2025-04-26 20:43:21,349 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:21,349 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:21,438 - INFO -   - Copied. Param mean: 0.0000, std: 0.0083
2025-04-26 20:43:21,439 - INFO - Loading model.layers.0.mlp.gate_proj.weight:
2025-04-26 20:43:21,439 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:21,439 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:21,655 - INFO -   - Copied. Param mean: -0.0000, std: 0.0166
2025-04-26 20:43:21,655 - INFO - Loading model.layers.0.mlp.up_proj.weight:
2025-04-26 20:43:21,656 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:21,656 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:21,863 - INFO -   - Copied. Param mean: -0.0000, std: 0.0168
2025-04-26 20:43:21,864 - INFO - Loading model.layers.0.mlp.down_proj.weight:
2025-04-26 20:43:21,864 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:21,864 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:22,058 - INFO -   - Copied. Param mean: 0.0000, std: 0.0166
2025-04-26 20:43:22,058 - INFO - Loading model.layers.1.input_layernorm.weight:
2025-04-26 20:43:22,058 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:22,059 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:22,059 - INFO -   - Copied. Param mean: 0.0405, std: 0.0559
2025-04-26 20:43:22,059 - INFO - Loading model.layers.1.post_attention_layernorm.weight:
2025-04-26 20:43:22,059 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:22,059 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:22,060 - INFO -   - Copied. Param mean: 0.1284, std: 0.0211
2025-04-26 20:43:22,060 - INFO - Loading model.layers.1.self_attn.q_proj.weight:
2025-04-26 20:43:22,060 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:22,060 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:22,128 - INFO -   - Copied. Param mean: 0.0000, std: 0.0294
2025-04-26 20:43:22,129 - INFO - Loading model.layers.1.self_attn.k_proj.weight:
2025-04-26 20:43:22,129 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:22,129 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:22,147 - INFO -   - Copied. Param mean: -0.0000, std: 0.0479
2025-04-26 20:43:22,147 - INFO - Loading model.layers.1.self_attn.v_proj.weight:
2025-04-26 20:43:22,148 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:22,148 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:22,159 - INFO -   - Copied. Param mean: 0.0000, std: 0.0134
2025-04-26 20:43:22,160 - INFO - Loading model.layers.1.self_attn.o_proj.weight:
2025-04-26 20:43:22,160 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:22,161 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:22,236 - INFO -   - Copied. Param mean: -0.0000, std: 0.0137
2025-04-26 20:43:22,237 - INFO - Loading model.layers.1.mlp.gate_proj.weight:
2025-04-26 20:43:22,237 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:22,238 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:22,446 - INFO -   - Copied. Param mean: 0.0000, std: 0.0181
2025-04-26 20:43:22,446 - INFO - Loading model.layers.1.mlp.up_proj.weight:
2025-04-26 20:43:22,447 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:22,447 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:22,684 - INFO -   - Copied. Param mean: -0.0000, std: 0.0173
2025-04-26 20:43:22,685 - INFO - Loading model.layers.1.mlp.down_proj.weight:
2025-04-26 20:43:22,685 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:22,686 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:22,898 - INFO -   - Copied. Param mean: 0.0000, std: 0.0171
2025-04-26 20:43:22,899 - INFO - Loading model.layers.2.input_layernorm.weight:
2025-04-26 20:43:22,899 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:22,899 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:22,900 - INFO -   - Copied. Param mean: 0.0846, std: 0.0684
2025-04-26 20:43:22,900 - INFO - Loading model.layers.2.post_attention_layernorm.weight:
2025-04-26 20:43:22,900 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:22,900 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:22,900 - INFO -   - Copied. Param mean: 0.1674, std: 0.0219
2025-04-26 20:43:22,901 - INFO - Loading model.layers.2.self_attn.q_proj.weight:
2025-04-26 20:43:22,901 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:22,901 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:22,972 - INFO -   - Copied. Param mean: 0.0000, std: 0.0322
2025-04-26 20:43:22,972 - INFO - Loading model.layers.2.self_attn.k_proj.weight:
2025-04-26 20:43:22,972 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:22,973 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:22,993 - INFO -   - Copied. Param mean: 0.0001, std: 0.0542
2025-04-26 20:43:22,993 - INFO - Loading model.layers.2.self_attn.v_proj.weight:
2025-04-26 20:43:22,994 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:22,994 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:23,015 - INFO -   - Copied. Param mean: -0.0000, std: 0.0118
2025-04-26 20:43:23,015 - INFO - Loading model.layers.2.self_attn.o_proj.weight:
2025-04-26 20:43:23,016 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:23,016 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:23,088 - INFO -   - Copied. Param mean: -0.0000, std: 0.0141
2025-04-26 20:43:23,088 - INFO - Loading model.layers.2.mlp.gate_proj.weight:
2025-04-26 20:43:23,089 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:23,089 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:23,300 - INFO -   - Copied. Param mean: -0.0000, std: 0.0185
2025-04-26 20:43:23,301 - INFO - Loading model.layers.2.mlp.up_proj.weight:
2025-04-26 20:43:23,301 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:23,302 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:23,550 - INFO -   - Copied. Param mean: -0.0000, std: 0.0175
2025-04-26 20:43:23,551 - INFO - Loading model.layers.2.mlp.down_proj.weight:
2025-04-26 20:43:23,552 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:23,552 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:23,779 - INFO -   - Copied. Param mean: 0.0000, std: 0.0174
2025-04-26 20:43:23,780 - INFO - Loading model.layers.3.input_layernorm.weight:
2025-04-26 20:43:23,780 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:23,780 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:23,781 - INFO -   - Copied. Param mean: 0.2243, std: 0.0547
2025-04-26 20:43:23,781 - INFO - Loading model.layers.3.post_attention_layernorm.weight:
2025-04-26 20:43:23,781 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:23,782 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:23,782 - INFO -   - Copied. Param mean: 0.1843, std: 0.0214
2025-04-26 20:43:23,782 - INFO - Loading model.layers.3.self_attn.q_proj.weight:
2025-04-26 20:43:23,782 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:23,783 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:23,865 - INFO -   - Copied. Param mean: -0.0000, std: 0.0271
2025-04-26 20:43:23,865 - INFO - Loading model.layers.3.self_attn.k_proj.weight:
2025-04-26 20:43:23,865 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:23,865 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:23,877 - INFO -   - Copied. Param mean: 0.0001, std: 0.0477
2025-04-26 20:43:23,878 - INFO - Loading model.layers.3.self_attn.v_proj.weight:
2025-04-26 20:43:23,878 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:23,878 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:23,897 - INFO -   - Copied. Param mean: -0.0000, std: 0.0114
2025-04-26 20:43:23,897 - INFO - Loading model.layers.3.self_attn.o_proj.weight:
2025-04-26 20:43:23,897 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:23,898 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:23,983 - INFO -   - Copied. Param mean: 0.0000, std: 0.0143
2025-04-26 20:43:23,984 - INFO - Loading model.layers.3.mlp.gate_proj.weight:
2025-04-26 20:43:23,984 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:23,984 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:24,186 - INFO -   - Copied. Param mean: -0.0000, std: 0.0187
2025-04-26 20:43:24,186 - INFO - Loading model.layers.3.mlp.up_proj.weight:
2025-04-26 20:43:24,186 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:24,186 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:24,406 - INFO -   - Copied. Param mean: -0.0000, std: 0.0174
2025-04-26 20:43:24,407 - INFO - Loading model.layers.3.mlp.down_proj.weight:
2025-04-26 20:43:24,407 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:24,407 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:24,617 - INFO -   - Copied. Param mean: 0.0000, std: 0.0173
2025-04-26 20:43:24,618 - INFO - Loading model.layers.4.input_layernorm.weight:
2025-04-26 20:43:24,618 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:24,618 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:24,619 - INFO -   - Copied. Param mean: 0.3199, std: 0.0582
2025-04-26 20:43:24,619 - INFO - Loading model.layers.4.post_attention_layernorm.weight:
2025-04-26 20:43:24,619 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:24,619 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:24,620 - INFO -   - Copied. Param mean: 0.1978, std: 0.0238
2025-04-26 20:43:24,620 - INFO - Loading model.layers.4.self_attn.q_proj.weight:
2025-04-26 20:43:24,620 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:24,621 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:24,701 - INFO -   - Copied. Param mean: 0.0000, std: 0.0260
2025-04-26 20:43:24,701 - INFO - Loading model.layers.4.self_attn.k_proj.weight:
2025-04-26 20:43:24,702 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:24,702 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:24,726 - INFO -   - Copied. Param mean: 0.0000, std: 0.0490
2025-04-26 20:43:24,726 - INFO - Loading model.layers.4.self_attn.v_proj.weight:
2025-04-26 20:43:24,727 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:24,727 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:24,750 - INFO -   - Copied. Param mean: 0.0000, std: 0.0101
2025-04-26 20:43:24,750 - INFO - Loading model.layers.4.self_attn.o_proj.weight:
2025-04-26 20:43:24,750 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:24,750 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:24,820 - INFO -   - Copied. Param mean: -0.0000, std: 0.0140
2025-04-26 20:43:24,820 - INFO - Loading model.layers.4.mlp.gate_proj.weight:
2025-04-26 20:43:24,820 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:24,821 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:25,041 - INFO -   - Copied. Param mean: -0.0000, std: 0.0190
2025-04-26 20:43:25,042 - INFO - Loading model.layers.4.mlp.up_proj.weight:
2025-04-26 20:43:25,042 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:25,042 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:25,248 - INFO -   - Copied. Param mean: -0.0000, std: 0.0173
2025-04-26 20:43:25,248 - INFO - Loading model.layers.4.mlp.down_proj.weight:
2025-04-26 20:43:25,248 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:25,248 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:25,447 - INFO -   - Copied. Param mean: -0.0000, std: 0.0173
2025-04-26 20:43:25,447 - INFO - Loading model.layers.5.input_layernorm.weight:
2025-04-26 20:43:25,447 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:25,447 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:25,448 - INFO -   - Copied. Param mean: 0.2800, std: 0.0488
2025-04-26 20:43:25,448 - INFO - Loading model.layers.5.post_attention_layernorm.weight:
2025-04-26 20:43:25,448 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:25,448 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:25,449 - INFO -   - Copied. Param mean: 0.2160, std: 0.0225
2025-04-26 20:43:25,449 - INFO - Loading model.layers.5.self_attn.q_proj.weight:
2025-04-26 20:43:25,449 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:25,449 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:25,528 - INFO -   - Copied. Param mean: -0.0000, std: 0.0271
2025-04-26 20:43:25,528 - INFO - Loading model.layers.5.self_attn.k_proj.weight:
2025-04-26 20:43:25,529 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:25,529 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:25,547 - INFO -   - Copied. Param mean: 0.0000, std: 0.0502
2025-04-26 20:43:25,547 - INFO - Loading model.layers.5.self_attn.v_proj.weight:
2025-04-26 20:43:25,548 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:25,548 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:25,566 - INFO -   - Copied. Param mean: 0.0000, std: 0.0115
2025-04-26 20:43:25,566 - INFO - Loading model.layers.5.self_attn.o_proj.weight:
2025-04-26 20:43:25,566 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:25,566 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:25,645 - INFO -   - Copied. Param mean: -0.0000, std: 0.0145
2025-04-26 20:43:25,645 - INFO - Loading model.layers.5.mlp.gate_proj.weight:
2025-04-26 20:43:25,646 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:25,646 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:25,868 - INFO -   - Copied. Param mean: -0.0000, std: 0.0192
2025-04-26 20:43:25,869 - INFO - Loading model.layers.5.mlp.up_proj.weight:
2025-04-26 20:43:25,869 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:25,869 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:26,071 - INFO -   - Copied. Param mean: 0.0000, std: 0.0174
2025-04-26 20:43:26,072 - INFO - Loading model.layers.5.mlp.down_proj.weight:
2025-04-26 20:43:26,072 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:26,072 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:26,256 - INFO -   - Copied. Param mean: 0.0000, std: 0.0173
2025-04-26 20:43:26,256 - INFO - Loading model.layers.6.input_layernorm.weight:
2025-04-26 20:43:26,257 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:26,257 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:26,257 - INFO -   - Copied. Param mean: 0.2680, std: 0.0792
2025-04-26 20:43:26,258 - INFO - Loading model.layers.6.post_attention_layernorm.weight:
2025-04-26 20:43:26,258 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:26,258 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:26,258 - INFO -   - Copied. Param mean: 0.2251, std: 0.0219
2025-04-26 20:43:26,259 - INFO - Loading model.layers.6.self_attn.q_proj.weight:
2025-04-26 20:43:26,259 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:26,259 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:26,324 - INFO -   - Copied. Param mean: 0.0000, std: 0.0263
2025-04-26 20:43:26,324 - INFO - Loading model.layers.6.self_attn.k_proj.weight:
2025-04-26 20:43:26,325 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:26,325 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:26,341 - INFO -   - Copied. Param mean: 0.0001, std: 0.0471
2025-04-26 20:43:26,341 - INFO - Loading model.layers.6.self_attn.v_proj.weight:
2025-04-26 20:43:26,341 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:26,342 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:26,357 - INFO -   - Copied. Param mean: -0.0000, std: 0.0113
2025-04-26 20:43:26,358 - INFO - Loading model.layers.6.self_attn.o_proj.weight:
2025-04-26 20:43:26,358 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:26,358 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:26,418 - INFO -   - Copied. Param mean: 0.0000, std: 0.0140
2025-04-26 20:43:26,419 - INFO - Loading model.layers.6.mlp.gate_proj.weight:
2025-04-26 20:43:26,419 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:26,419 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:26,607 - INFO -   - Copied. Param mean: 0.0000, std: 0.0196
2025-04-26 20:43:26,607 - INFO - Loading model.layers.6.mlp.up_proj.weight:
2025-04-26 20:43:26,607 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:26,608 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:26,817 - INFO -   - Copied. Param mean: -0.0000, std: 0.0172
2025-04-26 20:43:26,818 - INFO - Loading model.layers.6.mlp.down_proj.weight:
2025-04-26 20:43:26,818 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:26,818 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:27,010 - INFO -   - Copied. Param mean: -0.0000, std: 0.0171
2025-04-26 20:43:27,010 - INFO - Loading model.layers.7.input_layernorm.weight:
2025-04-26 20:43:27,011 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:27,011 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:27,011 - INFO -   - Copied. Param mean: 0.3070, std: 0.0577
2025-04-26 20:43:27,012 - INFO - Loading model.layers.7.post_attention_layernorm.weight:
2025-04-26 20:43:27,012 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:27,012 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:27,012 - INFO -   - Copied. Param mean: 0.2384, std: 0.0224
2025-04-26 20:43:27,013 - INFO - Loading model.layers.7.self_attn.q_proj.weight:
2025-04-26 20:43:27,013 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:27,013 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:27,081 - INFO -   - Copied. Param mean: -0.0000, std: 0.0266
2025-04-26 20:43:27,081 - INFO - Loading model.layers.7.self_attn.k_proj.weight:
2025-04-26 20:43:27,081 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:27,082 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:27,098 - INFO -   - Copied. Param mean: 0.0000, std: 0.0450
2025-04-26 20:43:27,098 - INFO - Loading model.layers.7.self_attn.v_proj.weight:
2025-04-26 20:43:27,099 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:27,099 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:27,115 - INFO -   - Copied. Param mean: -0.0000, std: 0.0130
2025-04-26 20:43:27,115 - INFO - Loading model.layers.7.self_attn.o_proj.weight:
2025-04-26 20:43:27,116 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:27,116 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:27,189 - INFO -   - Copied. Param mean: 0.0000, std: 0.0149
2025-04-26 20:43:27,189 - INFO - Loading model.layers.7.mlp.gate_proj.weight:
2025-04-26 20:43:27,189 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:27,190 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:27,377 - INFO -   - Copied. Param mean: 0.0001, std: 0.0209
2025-04-26 20:43:27,377 - INFO - Loading model.layers.7.mlp.up_proj.weight:
2025-04-26 20:43:27,377 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:27,378 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:27,551 - INFO -   - Copied. Param mean: 0.0000, std: 0.0168
2025-04-26 20:43:27,552 - INFO - Loading model.layers.7.mlp.down_proj.weight:
2025-04-26 20:43:27,552 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:27,552 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:27,756 - INFO -   - Copied. Param mean: 0.0000, std: 0.0167
2025-04-26 20:43:27,756 - INFO - Loading model.layers.8.input_layernorm.weight:
2025-04-26 20:43:27,757 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:27,757 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:27,757 - INFO -   - Copied. Param mean: 0.3188, std: 0.1083
2025-04-26 20:43:27,758 - INFO - Loading model.layers.8.post_attention_layernorm.weight:
2025-04-26 20:43:27,758 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:27,759 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:27,759 - INFO -   - Copied. Param mean: 0.2645, std: 0.0304
2025-04-26 20:43:27,759 - INFO - Loading model.layers.8.self_attn.q_proj.weight:
2025-04-26 20:43:27,759 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:27,760 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:27,845 - INFO -   - Copied. Param mean: -0.0000, std: 0.0273
2025-04-26 20:43:27,846 - INFO - Loading model.layers.8.self_attn.k_proj.weight:
2025-04-26 20:43:27,846 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:27,846 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:27,865 - INFO -   - Copied. Param mean: -0.0000, std: 0.0466
2025-04-26 20:43:27,866 - INFO - Loading model.layers.8.self_attn.v_proj.weight:
2025-04-26 20:43:27,866 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:27,866 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:27,886 - INFO -   - Copied. Param mean: -0.0000, std: 0.0121
2025-04-26 20:43:27,886 - INFO - Loading model.layers.8.self_attn.o_proj.weight:
2025-04-26 20:43:27,886 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:27,886 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:27,953 - INFO -   - Copied. Param mean: -0.0000, std: 0.0145
2025-04-26 20:43:27,953 - INFO - Loading model.layers.8.mlp.gate_proj.weight:
2025-04-26 20:43:27,954 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:27,954 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:28,142 - INFO -   - Copied. Param mean: 0.0001, std: 0.0202
2025-04-26 20:43:28,142 - INFO - Loading model.layers.8.mlp.up_proj.weight:
2025-04-26 20:43:28,142 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:28,142 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:28,320 - INFO -   - Copied. Param mean: 0.0000, std: 0.0173
2025-04-26 20:43:28,320 - INFO - Loading model.layers.8.mlp.down_proj.weight:
2025-04-26 20:43:28,320 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:28,320 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:28,507 - INFO -   - Copied. Param mean: 0.0000, std: 0.0172
2025-04-26 20:43:28,508 - INFO - Loading model.layers.9.input_layernorm.weight:
2025-04-26 20:43:28,508 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:28,508 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:28,508 - INFO -   - Copied. Param mean: 0.3141, std: 0.0609
2025-04-26 20:43:28,509 - INFO - Loading model.layers.9.post_attention_layernorm.weight:
2025-04-26 20:43:28,509 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:28,509 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:28,509 - INFO -   - Copied. Param mean: 0.2707, std: 0.0268
2025-04-26 20:43:28,509 - INFO - Loading model.layers.9.self_attn.q_proj.weight:
2025-04-26 20:43:28,510 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:28,510 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:28,576 - INFO -   - Copied. Param mean: 0.0000, std: 0.0266
2025-04-26 20:43:28,577 - INFO - Loading model.layers.9.self_attn.k_proj.weight:
2025-04-26 20:43:28,577 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:28,577 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:28,586 - INFO -   - Copied. Param mean: 0.0001, std: 0.0479
2025-04-26 20:43:28,586 - INFO - Loading model.layers.9.self_attn.v_proj.weight:
2025-04-26 20:43:28,586 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:28,586 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:28,595 - INFO -   - Copied. Param mean: 0.0000, std: 0.0122
2025-04-26 20:43:28,595 - INFO - Loading model.layers.9.self_attn.o_proj.weight:
2025-04-26 20:43:28,595 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:28,595 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:28,656 - INFO -   - Copied. Param mean: 0.0000, std: 0.0150
2025-04-26 20:43:28,657 - INFO - Loading model.layers.9.mlp.gate_proj.weight:
2025-04-26 20:43:28,657 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:28,657 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:28,843 - INFO -   - Copied. Param mean: 0.0000, std: 0.0207
2025-04-26 20:43:28,843 - INFO - Loading model.layers.9.mlp.up_proj.weight:
2025-04-26 20:43:28,843 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:28,843 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:29,034 - INFO -   - Copied. Param mean: 0.0000, std: 0.0171
2025-04-26 20:43:29,034 - INFO - Loading model.layers.9.mlp.down_proj.weight:
2025-04-26 20:43:29,035 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:29,035 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:29,314 - INFO -   - Copied. Param mean: -0.0000, std: 0.0169
2025-04-26 20:43:29,314 - INFO - Loading model.layers.10.input_layernorm.weight:
2025-04-26 20:43:29,315 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:29,315 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:29,315 - INFO -   - Copied. Param mean: 0.3328, std: 0.0563
2025-04-26 20:43:29,316 - INFO - Loading model.layers.10.post_attention_layernorm.weight:
2025-04-26 20:43:29,316 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:29,316 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:29,317 - INFO -   - Copied. Param mean: 0.2796, std: 0.0302
2025-04-26 20:43:29,317 - INFO - Loading model.layers.10.self_attn.q_proj.weight:
2025-04-26 20:43:29,317 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:29,317 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:29,398 - INFO -   - Copied. Param mean: 0.0000, std: 0.0268
2025-04-26 20:43:29,399 - INFO - Loading model.layers.10.self_attn.k_proj.weight:
2025-04-26 20:43:29,399 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:29,399 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:29,422 - INFO -   - Copied. Param mean: -0.0000, std: 0.0493
2025-04-26 20:43:29,423 - INFO - Loading model.layers.10.self_attn.v_proj.weight:
2025-04-26 20:43:29,423 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:29,423 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:29,440 - INFO -   - Copied. Param mean: -0.0000, std: 0.0123
2025-04-26 20:43:29,441 - INFO - Loading model.layers.10.self_attn.o_proj.weight:
2025-04-26 20:43:29,441 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:29,441 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:29,521 - INFO -   - Copied. Param mean: 0.0000, std: 0.0149
2025-04-26 20:43:29,521 - INFO - Loading model.layers.10.mlp.gate_proj.weight:
2025-04-26 20:43:29,521 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:29,522 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:29,766 - INFO -   - Copied. Param mean: 0.0000, std: 0.0204
2025-04-26 20:43:29,766 - INFO - Loading model.layers.10.mlp.up_proj.weight:
2025-04-26 20:43:29,766 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:29,767 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:30,055 - INFO -   - Copied. Param mean: 0.0000, std: 0.0174
2025-04-26 20:43:30,055 - INFO - Loading model.layers.10.mlp.down_proj.weight:
2025-04-26 20:43:30,056 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:30,056 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:30,327 - INFO -   - Copied. Param mean: 0.0000, std: 0.0173
2025-04-26 20:43:30,328 - INFO - Loading model.layers.11.input_layernorm.weight:
2025-04-26 20:43:30,328 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:30,328 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:30,329 - INFO -   - Copied. Param mean: 0.3955, std: 0.0736
2025-04-26 20:43:30,329 - INFO - Loading model.layers.11.post_attention_layernorm.weight:
2025-04-26 20:43:30,329 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:30,329 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:30,330 - INFO -   - Copied. Param mean: 0.2884, std: 0.0302
2025-04-26 20:43:30,330 - INFO - Loading model.layers.11.self_attn.q_proj.weight:
2025-04-26 20:43:30,330 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:30,331 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:30,431 - INFO -   - Copied. Param mean: 0.0000, std: 0.0256
2025-04-26 20:43:30,431 - INFO - Loading model.layers.11.self_attn.k_proj.weight:
2025-04-26 20:43:30,432 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:30,432 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:30,441 - INFO -   - Copied. Param mean: -0.0001, std: 0.0445
2025-04-26 20:43:30,441 - INFO - Loading model.layers.11.self_attn.v_proj.weight:
2025-04-26 20:43:30,441 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:30,442 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:30,458 - INFO -   - Copied. Param mean: 0.0000, std: 0.0115
2025-04-26 20:43:30,459 - INFO - Loading model.layers.11.self_attn.o_proj.weight:
2025-04-26 20:43:30,459 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:30,459 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:30,531 - INFO -   - Copied. Param mean: 0.0000, std: 0.0146
2025-04-26 20:43:30,531 - INFO - Loading model.layers.11.mlp.gate_proj.weight:
2025-04-26 20:43:30,531 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:30,532 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:30,786 - INFO -   - Copied. Param mean: -0.0000, std: 0.0205
2025-04-26 20:43:30,787 - INFO - Loading model.layers.11.mlp.up_proj.weight:
2025-04-26 20:43:30,788 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:30,788 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:31,011 - INFO -   - Copied. Param mean: -0.0000, std: 0.0174
2025-04-26 20:43:31,011 - INFO - Loading model.layers.11.mlp.down_proj.weight:
2025-04-26 20:43:31,012 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:31,012 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:31,240 - INFO -   - Copied. Param mean: -0.0000, std: 0.0172
2025-04-26 20:43:31,241 - INFO - Loading model.layers.12.input_layernorm.weight:
2025-04-26 20:43:31,241 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:31,241 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:31,242 - INFO -   - Copied. Param mean: 0.3438, std: 0.0674
2025-04-26 20:43:31,242 - INFO - Loading model.layers.12.post_attention_layernorm.weight:
2025-04-26 20:43:31,242 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:31,243 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:31,243 - INFO -   - Copied. Param mean: 0.2991, std: 0.0256
2025-04-26 20:43:31,243 - INFO - Loading model.layers.12.self_attn.q_proj.weight:
2025-04-26 20:43:31,244 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:31,244 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:31,343 - INFO -   - Copied. Param mean: -0.0000, std: 0.0259
2025-04-26 20:43:31,343 - INFO - Loading model.layers.12.self_attn.k_proj.weight:
2025-04-26 20:43:31,343 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:31,344 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:31,371 - INFO -   - Copied. Param mean: -0.0001, std: 0.0462
2025-04-26 20:43:31,372 - INFO - Loading model.layers.12.self_attn.v_proj.weight:
2025-04-26 20:43:31,372 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:31,373 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:31,388 - INFO -   - Copied. Param mean: -0.0001, std: 0.0144
2025-04-26 20:43:31,388 - INFO - Loading model.layers.12.self_attn.o_proj.weight:
2025-04-26 20:43:31,388 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:31,388 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:31,478 - INFO -   - Copied. Param mean: -0.0000, std: 0.0157
2025-04-26 20:43:31,478 - INFO - Loading model.layers.12.mlp.gate_proj.weight:
2025-04-26 20:43:31,478 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:31,478 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:31,724 - INFO -   - Copied. Param mean: -0.0000, std: 0.0212
2025-04-26 20:43:31,724 - INFO - Loading model.layers.12.mlp.up_proj.weight:
2025-04-26 20:43:31,725 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:31,725 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:31,941 - INFO -   - Copied. Param mean: -0.0000, std: 0.0172
2025-04-26 20:43:31,941 - INFO - Loading model.layers.12.mlp.down_proj.weight:
2025-04-26 20:43:31,941 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:31,942 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:32,143 - INFO -   - Copied. Param mean: -0.0000, std: 0.0170
2025-04-26 20:43:32,143 - INFO - Loading model.layers.13.input_layernorm.weight:
2025-04-26 20:43:32,143 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:32,144 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:32,144 - INFO -   - Copied. Param mean: 0.3773, std: 0.0761
2025-04-26 20:43:32,144 - INFO - Loading model.layers.13.post_attention_layernorm.weight:
2025-04-26 20:43:32,144 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:32,144 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:32,145 - INFO -   - Copied. Param mean: 0.3128, std: 0.0242
2025-04-26 20:43:32,145 - INFO - Loading model.layers.13.self_attn.q_proj.weight:
2025-04-26 20:43:32,145 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:32,145 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:32,231 - INFO -   - Copied. Param mean: 0.0000, std: 0.0254
2025-04-26 20:43:32,232 - INFO - Loading model.layers.13.self_attn.k_proj.weight:
2025-04-26 20:43:32,232 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:32,232 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:32,252 - INFO -   - Copied. Param mean: -0.0000, std: 0.0469
2025-04-26 20:43:32,252 - INFO - Loading model.layers.13.self_attn.v_proj.weight:
2025-04-26 20:43:32,252 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:32,253 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:32,272 - INFO -   - Copied. Param mean: -0.0000, std: 0.0123
2025-04-26 20:43:32,272 - INFO - Loading model.layers.13.self_attn.o_proj.weight:
2025-04-26 20:43:32,273 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:32,273 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:32,339 - INFO -   - Copied. Param mean: -0.0000, std: 0.0151
2025-04-26 20:43:32,340 - INFO - Loading model.layers.13.mlp.gate_proj.weight:
2025-04-26 20:43:32,340 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:32,340 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:32,600 - INFO -   - Copied. Param mean: -0.0000, std: 0.0211
2025-04-26 20:43:32,601 - INFO - Loading model.layers.13.mlp.up_proj.weight:
2025-04-26 20:43:32,601 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:32,601 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:32,854 - INFO -   - Copied. Param mean: -0.0000, std: 0.0174
2025-04-26 20:43:32,855 - INFO - Loading model.layers.13.mlp.down_proj.weight:
2025-04-26 20:43:32,855 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:32,855 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:33,103 - INFO -   - Copied. Param mean: 0.0000, std: 0.0172
2025-04-26 20:43:33,103 - INFO - Loading model.layers.14.input_layernorm.weight:
2025-04-26 20:43:33,104 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:33,104 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:33,104 - INFO -   - Copied. Param mean: 0.3614, std: 0.0651
2025-04-26 20:43:33,105 - INFO - Loading model.layers.14.post_attention_layernorm.weight:
2025-04-26 20:43:33,105 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:33,105 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:33,105 - INFO -   - Copied. Param mean: 0.3289, std: 0.0258
2025-04-26 20:43:33,106 - INFO - Loading model.layers.14.self_attn.q_proj.weight:
2025-04-26 20:43:33,106 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:33,106 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:33,188 - INFO -   - Copied. Param mean: -0.0000, std: 0.0256
2025-04-26 20:43:33,189 - INFO - Loading model.layers.14.self_attn.k_proj.weight:
2025-04-26 20:43:33,189 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:33,189 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:33,200 - INFO -   - Copied. Param mean: -0.0000, std: 0.0482
2025-04-26 20:43:33,201 - INFO - Loading model.layers.14.self_attn.v_proj.weight:
2025-04-26 20:43:33,201 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:33,201 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:33,226 - INFO -   - Copied. Param mean: 0.0000, std: 0.0134
2025-04-26 20:43:33,226 - INFO - Loading model.layers.14.self_attn.o_proj.weight:
2025-04-26 20:43:33,227 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:33,227 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:33,330 - INFO -   - Copied. Param mean: -0.0000, std: 0.0155
2025-04-26 20:43:33,330 - INFO - Loading model.layers.14.mlp.gate_proj.weight:
2025-04-26 20:43:33,331 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:33,331 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:33,698 - INFO -   - Copied. Param mean: -0.0001, std: 0.0211
2025-04-26 20:43:33,699 - INFO - Loading model.layers.14.mlp.up_proj.weight:
2025-04-26 20:43:33,699 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:33,700 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:33,977 - INFO -   - Copied. Param mean: 0.0000, std: 0.0179
2025-04-26 20:43:33,977 - INFO - Loading model.layers.14.mlp.down_proj.weight:
2025-04-26 20:43:33,978 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:33,978 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:34,253 - INFO -   - Copied. Param mean: 0.0000, std: 0.0176
2025-04-26 20:43:34,253 - INFO - Loading model.layers.15.input_layernorm.weight:
2025-04-26 20:43:34,253 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:34,254 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:34,254 - INFO -   - Copied. Param mean: 0.4188, std: 0.0651
2025-04-26 20:43:34,255 - INFO - Loading model.layers.15.post_attention_layernorm.weight:
2025-04-26 20:43:34,255 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:34,255 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:34,256 - INFO -   - Copied. Param mean: 0.3524, std: 0.0248
2025-04-26 20:43:34,256 - INFO - Loading model.layers.15.self_attn.q_proj.weight:
2025-04-26 20:43:34,256 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:34,257 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:34,341 - INFO -   - Copied. Param mean: -0.0000, std: 0.0260
2025-04-26 20:43:34,342 - INFO - Loading model.layers.15.self_attn.k_proj.weight:
2025-04-26 20:43:34,342 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:34,342 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:34,354 - INFO -   - Copied. Param mean: 0.0001, std: 0.0428
2025-04-26 20:43:34,355 - INFO - Loading model.layers.15.self_attn.v_proj.weight:
2025-04-26 20:43:34,355 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:34,355 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:34,379 - INFO -   - Copied. Param mean: -0.0000, std: 0.0137
2025-04-26 20:43:34,380 - INFO - Loading model.layers.15.self_attn.o_proj.weight:
2025-04-26 20:43:34,380 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:34,381 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:34,493 - INFO -   - Copied. Param mean: 0.0000, std: 0.0159
2025-04-26 20:43:34,493 - INFO - Loading model.layers.15.mlp.gate_proj.weight:
2025-04-26 20:43:34,493 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:34,494 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:34,779 - INFO -   - Copied. Param mean: -0.0001, std: 0.0211
2025-04-26 20:43:34,780 - INFO - Loading model.layers.15.mlp.up_proj.weight:
2025-04-26 20:43:34,780 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:34,780 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:35,042 - INFO -   - Copied. Param mean: -0.0000, std: 0.0180
2025-04-26 20:43:35,042 - INFO - Loading model.layers.15.mlp.down_proj.weight:
2025-04-26 20:43:35,042 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:35,043 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:35,307 - INFO -   - Copied. Param mean: 0.0000, std: 0.0177
2025-04-26 20:43:35,307 - INFO - Loading model.layers.16.input_layernorm.weight:
2025-04-26 20:43:35,307 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:35,308 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:35,308 - INFO -   - Copied. Param mean: 0.4019, std: 0.0667
2025-04-26 20:43:35,308 - INFO - Loading model.layers.16.post_attention_layernorm.weight:
2025-04-26 20:43:35,308 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:35,309 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:35,309 - INFO -   - Copied. Param mean: 0.3908, std: 0.0267
2025-04-26 20:43:35,309 - INFO - Loading model.layers.16.self_attn.q_proj.weight:
2025-04-26 20:43:35,309 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:35,309 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:35,382 - INFO -   - Copied. Param mean: -0.0000, std: 0.0265
2025-04-26 20:43:35,382 - INFO - Loading model.layers.16.self_attn.k_proj.weight:
2025-04-26 20:43:35,383 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:35,383 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:35,396 - INFO -   - Copied. Param mean: 0.0000, std: 0.0440
2025-04-26 20:43:35,397 - INFO - Loading model.layers.16.self_attn.v_proj.weight:
2025-04-26 20:43:35,397 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:35,397 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:35,432 - INFO -   - Copied. Param mean: -0.0000, std: 0.0151
2025-04-26 20:43:35,433 - INFO - Loading model.layers.16.self_attn.o_proj.weight:
2025-04-26 20:43:35,433 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:35,433 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:35,518 - INFO -   - Copied. Param mean: -0.0000, std: 0.0158
2025-04-26 20:43:35,519 - INFO - Loading model.layers.16.mlp.gate_proj.weight:
2025-04-26 20:43:35,519 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:35,519 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:35,791 - INFO -   - Copied. Param mean: -0.0001, std: 0.0216
2025-04-26 20:43:35,791 - INFO - Loading model.layers.16.mlp.up_proj.weight:
2025-04-26 20:43:35,792 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:35,792 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:36,039 - INFO -   - Copied. Param mean: 0.0000, std: 0.0180
2025-04-26 20:43:36,039 - INFO - Loading model.layers.16.mlp.down_proj.weight:
2025-04-26 20:43:36,040 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:36,040 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:36,346 - INFO -   - Copied. Param mean: -0.0000, std: 0.0176
2025-04-26 20:43:36,347 - INFO - Loading model.layers.17.input_layernorm.weight:
2025-04-26 20:43:36,347 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:36,347 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:36,348 - INFO -   - Copied. Param mean: 0.4315, std: 0.0760
2025-04-26 20:43:36,349 - INFO - Loading model.layers.17.post_attention_layernorm.weight:
2025-04-26 20:43:36,349 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:36,349 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:36,349 - INFO -   - Copied. Param mean: 0.4231, std: 0.0308
2025-04-26 20:43:36,350 - INFO - Loading model.layers.17.self_attn.q_proj.weight:
2025-04-26 20:43:36,350 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:36,350 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:36,447 - INFO -   - Copied. Param mean: 0.0000, std: 0.0243
2025-04-26 20:43:36,448 - INFO - Loading model.layers.17.self_attn.k_proj.weight:
2025-04-26 20:43:36,448 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:36,448 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:36,478 - INFO -   - Copied. Param mean: 0.0000, std: 0.0404
2025-04-26 20:43:36,478 - INFO - Loading model.layers.17.self_attn.v_proj.weight:
2025-04-26 20:43:36,479 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:36,479 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:36,498 - INFO -   - Copied. Param mean: -0.0000, std: 0.0188
2025-04-26 20:43:36,499 - INFO - Loading model.layers.17.self_attn.o_proj.weight:
2025-04-26 20:43:36,499 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:36,499 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:36,586 - INFO -   - Copied. Param mean: -0.0000, std: 0.0171
2025-04-26 20:43:36,586 - INFO - Loading model.layers.17.mlp.gate_proj.weight:
2025-04-26 20:43:36,586 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:36,586 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:36,827 - INFO -   - Copied. Param mean: -0.0001, std: 0.0217
2025-04-26 20:43:36,828 - INFO - Loading model.layers.17.mlp.up_proj.weight:
2025-04-26 20:43:36,828 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:36,828 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:37,116 - INFO -   - Copied. Param mean: -0.0000, std: 0.0183
2025-04-26 20:43:37,117 - INFO - Loading model.layers.17.mlp.down_proj.weight:
2025-04-26 20:43:37,117 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:37,117 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:37,379 - INFO -   - Copied. Param mean: -0.0000, std: 0.0180
2025-04-26 20:43:37,379 - INFO - Loading model.layers.18.input_layernorm.weight:
2025-04-26 20:43:37,380 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:37,380 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:37,381 - INFO -   - Copied. Param mean: 0.4387, std: 0.0815
2025-04-26 20:43:37,381 - INFO - Loading model.layers.18.post_attention_layernorm.weight:
2025-04-26 20:43:37,381 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:37,382 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:37,382 - INFO -   - Copied. Param mean: 0.4623, std: 0.0351
2025-04-26 20:43:37,382 - INFO - Loading model.layers.18.self_attn.q_proj.weight:
2025-04-26 20:43:37,383 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:37,383 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:37,472 - INFO -   - Copied. Param mean: 0.0000, std: 0.0251
2025-04-26 20:43:37,473 - INFO - Loading model.layers.18.self_attn.k_proj.weight:
2025-04-26 20:43:37,473 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:37,473 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:37,488 - INFO -   - Copied. Param mean: -0.0000, std: 0.0403
2025-04-26 20:43:37,489 - INFO - Loading model.layers.18.self_attn.v_proj.weight:
2025-04-26 20:43:37,489 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:37,489 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:37,515 - INFO -   - Copied. Param mean: -0.0000, std: 0.0196
2025-04-26 20:43:37,515 - INFO - Loading model.layers.18.self_attn.o_proj.weight:
2025-04-26 20:43:37,516 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:37,516 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:37,613 - INFO -   - Copied. Param mean: 0.0000, std: 0.0175
2025-04-26 20:43:37,613 - INFO - Loading model.layers.18.mlp.gate_proj.weight:
2025-04-26 20:43:37,614 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:37,614 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:37,903 - INFO -   - Copied. Param mean: -0.0001, std: 0.0218
2025-04-26 20:43:37,904 - INFO - Loading model.layers.18.mlp.up_proj.weight:
2025-04-26 20:43:37,904 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:37,905 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:38,188 - INFO -   - Copied. Param mean: 0.0000, std: 0.0186
2025-04-26 20:43:38,189 - INFO - Loading model.layers.18.mlp.down_proj.weight:
2025-04-26 20:43:38,189 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:38,190 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:38,449 - INFO -   - Copied. Param mean: -0.0000, std: 0.0182
2025-04-26 20:43:38,450 - INFO - Loading model.layers.19.input_layernorm.weight:
2025-04-26 20:43:38,451 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:38,451 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:38,452 - INFO -   - Copied. Param mean: 0.4350, std: 0.0937
2025-04-26 20:43:38,452 - INFO - Loading model.layers.19.post_attention_layernorm.weight:
2025-04-26 20:43:38,452 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:38,453 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:38,453 - INFO -   - Copied. Param mean: 0.4969, std: 0.0362
2025-04-26 20:43:38,454 - INFO - Loading model.layers.19.self_attn.q_proj.weight:
2025-04-26 20:43:38,454 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:38,454 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:38,562 - INFO -   - Copied. Param mean: -0.0000, std: 0.0242
2025-04-26 20:43:38,562 - INFO - Loading model.layers.19.self_attn.k_proj.weight:
2025-04-26 20:43:38,563 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:38,563 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:38,588 - INFO -   - Copied. Param mean: 0.0000, std: 0.0386
2025-04-26 20:43:38,589 - INFO - Loading model.layers.19.self_attn.v_proj.weight:
2025-04-26 20:43:38,589 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:38,589 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:38,612 - INFO -   - Copied. Param mean: 0.0000, std: 0.0231
2025-04-26 20:43:38,613 - INFO - Loading model.layers.19.self_attn.o_proj.weight:
2025-04-26 20:43:38,613 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:38,614 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:38,703 - INFO -   - Copied. Param mean: -0.0000, std: 0.0184
2025-04-26 20:43:38,704 - INFO - Loading model.layers.19.mlp.gate_proj.weight:
2025-04-26 20:43:38,704 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:38,705 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:38,972 - INFO -   - Copied. Param mean: -0.0001, std: 0.0218
2025-04-26 20:43:38,973 - INFO - Loading model.layers.19.mlp.up_proj.weight:
2025-04-26 20:43:38,973 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:38,974 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:39,243 - INFO -   - Copied. Param mean: -0.0000, std: 0.0190
2025-04-26 20:43:39,243 - INFO - Loading model.layers.19.mlp.down_proj.weight:
2025-04-26 20:43:39,244 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:39,244 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:39,528 - INFO -   - Copied. Param mean: -0.0000, std: 0.0186
2025-04-26 20:43:39,529 - INFO - Loading model.layers.20.input_layernorm.weight:
2025-04-26 20:43:39,529 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:39,530 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:39,530 - INFO -   - Copied. Param mean: 0.4330, std: 0.0860
2025-04-26 20:43:39,531 - INFO - Loading model.layers.20.post_attention_layernorm.weight:
2025-04-26 20:43:39,531 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:39,531 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:39,532 - INFO -   - Copied. Param mean: 0.5282, std: 0.0366
2025-04-26 20:43:39,532 - INFO - Loading model.layers.20.self_attn.q_proj.weight:
2025-04-26 20:43:39,532 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:39,533 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:39,643 - INFO -   - Copied. Param mean: 0.0000, std: 0.0247
2025-04-26 20:43:39,643 - INFO - Loading model.layers.20.self_attn.k_proj.weight:
2025-04-26 20:43:39,643 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:39,644 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:39,663 - INFO -   - Copied. Param mean: 0.0001, std: 0.0398
2025-04-26 20:43:39,663 - INFO - Loading model.layers.20.self_attn.v_proj.weight:
2025-04-26 20:43:39,664 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:39,664 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:39,705 - INFO -   - Copied. Param mean: 0.0000, std: 0.0235
2025-04-26 20:43:39,706 - INFO - Loading model.layers.20.self_attn.o_proj.weight:
2025-04-26 20:43:39,706 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:39,707 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:39,796 - INFO -   - Copied. Param mean: 0.0000, std: 0.0184
2025-04-26 20:43:39,797 - INFO - Loading model.layers.20.mlp.gate_proj.weight:
2025-04-26 20:43:39,797 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:39,798 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:40,062 - INFO -   - Copied. Param mean: -0.0000, std: 0.0219
2025-04-26 20:43:40,062 - INFO - Loading model.layers.20.mlp.up_proj.weight:
2025-04-26 20:43:40,063 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:40,063 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:40,367 - INFO -   - Copied. Param mean: -0.0000, std: 0.0194
2025-04-26 20:43:40,368 - INFO - Loading model.layers.20.mlp.down_proj.weight:
2025-04-26 20:43:40,368 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:40,368 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:40,623 - INFO -   - Copied. Param mean: 0.0000, std: 0.0190
2025-04-26 20:43:40,624 - INFO - Loading model.layers.21.input_layernorm.weight:
2025-04-26 20:43:40,624 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:40,625 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:40,625 - INFO -   - Copied. Param mean: 0.4637, std: 0.0852
2025-04-26 20:43:40,626 - INFO - Loading model.layers.21.post_attention_layernorm.weight:
2025-04-26 20:43:40,626 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 20:43:40,626 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 20:43:40,627 - INFO -   - Copied. Param mean: 0.5546, std: 0.0391
2025-04-26 20:43:40,627 - INFO - Loading model.layers.21.self_attn.q_proj.weight:
2025-04-26 20:43:40,627 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:40,628 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:40,733 - INFO -   - Copied. Param mean: -0.0000, std: 0.0238
2025-04-26 20:43:40,733 - INFO - Loading model.layers.21.self_attn.k_proj.weight:
2025-04-26 20:43:40,733 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:40,734 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:40,757 - INFO -   - Copied. Param mean: 0.0000, std: 0.0395
2025-04-26 20:43:40,758 - INFO - Loading model.layers.21.self_attn.v_proj.weight:
2025-04-26 20:43:40,758 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:40,758 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 20:43:40,767 - INFO -   - Copied. Param mean: -0.0000, std: 0.0259
2025-04-26 20:43:40,768 - INFO - Loading model.layers.21.self_attn.o_proj.weight:
2025-04-26 20:43:40,768 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:40,768 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 20:43:40,861 - INFO -   - Copied. Param mean: -0.0000, std: 0.0191
2025-04-26 20:43:40,861 - INFO - Loading model.layers.21.mlp.gate_proj.weight:
2025-04-26 20:43:40,861 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:40,862 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:41,119 - INFO -   - Copied. Param mean: -0.0000, std: 0.0243
2025-04-26 20:43:41,120 - INFO - Loading model.layers.21.mlp.up_proj.weight:
2025-04-26 20:43:41,120 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 20:43:41,120 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 20:43:41,372 - INFO -   - Copied. Param mean: -0.0000, std: 0.0195
2025-04-26 20:43:41,373 - INFO - Loading model.layers.21.mlp.down_proj.weight:
2025-04-26 20:43:41,373 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 20:43:41,373 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 20:43:41,642 - INFO -   - Copied. Param mean: 0.0000, std: 0.0180
2025-04-26 20:43:41,643 - INFO - Model initialized with default dtype: torch.float32
2025-04-26 20:43:41,689 - INFO - lm_head first 10 values: [0.01251220703125, -0.022705078125, -0.0245361328125, -0.000713348388671875, -0.013671875, -0.002471923828125, 0.0101318359375, -0.001953125, 0.00811767578125, 0.0086669921875]
2025-04-26 20:43:41,690 - INFO - 
===== Q: A: Style =====
Prompt: Q: What is the capital of France?
A:
2025-04-26 20:43:41,690 - INFO - Prompt token IDs: [529, 29879, 29958, 29984, 29901, 1724, 338, 278, 7483, 310, 3444, 29973, 13, 29909, 29901]
2025-04-26 20:43:41,691 - INFO - Initial Input IDs (with BOS): [529, 29879, 29958, 29984, 29901, 1724, 338, 278, 7483, 310, 3444, 29973, 13, 29909, 29901]
2025-04-26 20:43:41,699 - INFO - Embedding stats for first token: min=-0.033935546875, max=0.0654296875, mean=0.00032116103102453053
2025-04-26 20:43:41,700 - INFO - First RMSNorm output stats: min=-1.4920575618743896, max=3.147249221801758, mean=0.0024326592683792114
2025-04-26 20:43:41,702 - INFO - Saved reference RMSNorm output (Layer 0, Token 0) to rmsnorm_out_ref.bin
2025-04-26 20:43:41,721 - INFO - First Q projection output stats: min=-4.503389835357666, max=4.144922733306885, mean=-0.0003170715644955635
2025-04-26 20:43:41,731 - INFO - Q before RoPE shape: [2048] num_heads=32 head_dim=64 pos=0 first 5: [-0.10789877 -0.04578701 -0.13515998 -0.01534481  0.11997594]
2025-04-26 20:43:41,732 - INFO - Q after RoPE shape: [256] first 5: [-0.10789877 -0.04578701 -0.13515998 -0.01534481  0.11997594]
2025-04-26 20:43:41,761 - INFO - First K projection output stats: min=-11.205004692077637, max=3.983145236968994, mean=-0.19396047294139862
2025-04-26 20:43:41,774 - INFO - First V projection output stats: min=-0.07605941593647003, max=0.06179859861731529, mean=0.00015027551853563637
2025-04-26 20:43:41,775 - INFO - K after RoPE shape: [256] first 5: [-0.07320985 -0.06033581  0.05773955  0.01598645  0.12556581]
2025-04-26 20:43:41,776 - INFO - First attention score (dot Q_rope, K_rope): 2.862595319747925
2025-04-26 20:43:41,777 - INFO - First attention probability (after softmax): 1.0
2025-04-26 20:43:41,777 - INFO - First attention output stats: min=-0.07605941593647003, max=0.06179859861731529, mean=0.00015027551853563637
2025-04-26 20:43:41,781 - INFO - Starting generation loop (max_new_tokens=50, eos_token_id=2)
2025-04-26 20:43:41,784 - INFO - Using token-by-token generation mode with KVCache.
2025-04-26 20:43:41,796 - INFO - Initializing KVCache for batch_size=1, max_seq_len=2048, layers=22, kv_heads=4, head_dim=64
2025-04-26 20:43:41,923 - INFO - KVCache initialized. k_cache length: 22, v_cache length: 22
2025-04-26 20:43:41,923 - INFO - Example k_cache[0] shape: torch.Size([1, 4, 2048, 64])
2025-04-26 20:43:41,923 - INFO - Priming KVCache with prompt tokens (excluding last): [529, 29879, 29958, 29984, 29901, 1724, 338, 278, 7483, 310, 3444, 29973, 13, 29909]
2025-04-26 20:43:41,924 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=0
2025-04-26 20:43:42,214 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:42,214 - INFO - Primed KVCache for pos=0
2025-04-26 20:43:42,215 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=1
2025-04-26 20:43:42,224 - INFO - [PyTorch] PyT L0 Output (pos=1): shape=[1, 1, 2048], min=-0.282582, max=0.266320, mean=-0.000196, all_finite=True
2025-04-26 20:43:42,224 - INFO - [PyTorch] PyT L0 Output (pos=1) first 5: -0.008421 -0.010634 -0.010333 -0.005166 0.003777
2025-04-26 20:43:42,224 - INFO - --- PyTorch Layer 0 End for pos=1 ---
2025-04-26 20:43:42,516 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:42,517 - INFO - Primed KVCache for pos=1
2025-04-26 20:43:42,517 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=2
2025-04-26 20:43:42,686 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:42,687 - INFO - Primed KVCache for pos=2
2025-04-26 20:43:42,688 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=3
2025-04-26 20:43:42,851 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:42,852 - INFO - Primed KVCache for pos=3
2025-04-26 20:43:42,852 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=4
2025-04-26 20:43:43,014 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:43,014 - INFO - Primed KVCache for pos=4
2025-04-26 20:43:43,014 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=5
2025-04-26 20:43:43,162 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:43,163 - INFO - Primed KVCache for pos=5
2025-04-26 20:43:43,163 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=6
2025-04-26 20:43:43,326 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:43,327 - INFO - Primed KVCache for pos=6
2025-04-26 20:43:43,327 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=7
2025-04-26 20:43:43,483 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:43,484 - INFO - Primed KVCache for pos=7
2025-04-26 20:43:43,484 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=8
2025-04-26 20:43:43,642 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:43,642 - INFO - Primed KVCache for pos=8
2025-04-26 20:43:43,642 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=9
2025-04-26 20:43:43,796 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:43,796 - INFO - Primed KVCache for pos=9
2025-04-26 20:43:43,797 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=10
2025-04-26 20:43:43,951 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:43,952 - INFO - Primed KVCache for pos=10
2025-04-26 20:43:43,952 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=11
2025-04-26 20:43:44,104 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:44,105 - INFO - Primed KVCache for pos=11
2025-04-26 20:43:44,105 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=12
2025-04-26 20:43:44,258 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:44,259 - INFO - Primed KVCache for pos=12
2025-04-26 20:43:44,259 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=13
2025-04-26 20:43:44,424 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:44,424 - INFO - Primed KVCache for pos=13
2025-04-26 20:43:44,425 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=14
2025-04-26 20:43:44,623 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:44,624 - INFO - First generated token logits (token-by-token, first 10): [-8.300727844238281, -7.779010772705078, 5.79564905166626, -4.407018661499023, -4.3599534034729, -6.102900981903076, -3.7960963249206543, -7.309700012207031, -6.306676387786865, -6.896210193634033]
2025-04-26 20:43:44,624 - INFO - Step 1 (Token-by-Token, pos=14): Predicted token ID: 3681
2025-04-26 20:43:44,625 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=15
2025-04-26 20:43:44,785 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:44,786 - INFO - Step 2 (Token-by-Token, pos=15): Predicted token ID: 29889
2025-04-26 20:43:44,786 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=16
2025-04-26 20:43:44,962 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:44,963 - INFO - Step 3 (Token-by-Token, pos=16): Predicted token ID: 2
2025-04-26 20:43:44,963 - INFO - EOS token (2) generated. Stopping generation.
2025-04-26 20:43:44,964 - INFO - Full Generated Sequence IDs: [529, 29879, 29958, 29984, 29901, 1724, 338, 278, 7483, 310, 3444, 29973, 13, 29909, 29901, 3681, 29889, 2]
2025-04-26 20:43:44,965 - INFO - Full Decoded Text:
-------
<s>Q: What is the capital of France?
A: Paris.
-------
2025-04-26 20:43:44,966 - INFO - Generated Part IDs: [3681, 29889, 2]
2025-04-26 20:43:44,966 - INFO - Generated Decoded Text (raw):
-------
Paris.
-------
2025-04-26 20:43:44,966 - INFO - Generated Decoded Text (cleaned):
-------
Paris.
-------
2025-04-26 20:43:44,967 - INFO - 
===== Q: A: Style =====
Prompt: Q: Who wrote Hamlet?
A:
2025-04-26 20:43:44,967 - INFO - Prompt token IDs: [529, 29879, 29958, 29984, 29901, 11644, 5456, 7904, 1026, 29973, 13, 29909, 29901]
2025-04-26 20:43:44,968 - INFO - Initial Input IDs (with BOS): [529, 29879, 29958, 29984, 29901, 11644, 5456, 7904, 1026, 29973, 13, 29909, 29901]
2025-04-26 20:43:44,969 - INFO - Embedding stats for first token: min=-0.033935546875, max=0.0654296875, mean=0.00032116103102453053
2025-04-26 20:43:44,969 - INFO - First RMSNorm output stats: min=-1.4920575618743896, max=3.147249221801758, mean=0.0024326592683792114
2025-04-26 20:43:44,973 - INFO - Saved reference RMSNorm output (Layer 0, Token 0) to rmsnorm_out_ref.bin
2025-04-26 20:43:44,992 - INFO - First Q projection output stats: min=-4.503389835357666, max=4.144922733306885, mean=-0.0003170715644955635
2025-04-26 20:43:44,994 - INFO - Q before RoPE shape: [2048] num_heads=32 head_dim=64 pos=0 first 5: [-0.10789877 -0.04578701 -0.13515998 -0.01534481  0.11997594]
2025-04-26 20:43:44,997 - INFO - Q after RoPE shape: [256] first 5: [-0.10789877 -0.04578701 -0.13515998 -0.01534481  0.11997594]
2025-04-26 20:43:45,009 - INFO - First K projection output stats: min=-11.205004692077637, max=3.983145236968994, mean=-0.19396047294139862
2025-04-26 20:43:45,051 - INFO - First V projection output stats: min=-0.07605941593647003, max=0.06179859861731529, mean=0.00015027551853563637
2025-04-26 20:43:45,055 - INFO - K after RoPE shape: [256] first 5: [-0.07320985 -0.06033581  0.05773955  0.01598645  0.12556581]
2025-04-26 20:43:45,055 - INFO - First attention score (dot Q_rope, K_rope): 2.862595319747925
2025-04-26 20:43:45,058 - INFO - First attention probability (after softmax): 1.0
2025-04-26 20:43:45,078 - INFO - First attention output stats: min=-0.07605941593647003, max=0.06179859861731529, mean=0.00015027551853563637
2025-04-26 20:43:45,094 - INFO - Starting generation loop (max_new_tokens=50, eos_token_id=2)
2025-04-26 20:43:45,106 - INFO - Using token-by-token generation mode with KVCache.
2025-04-26 20:43:45,107 - INFO - Initializing KVCache for batch_size=1, max_seq_len=2048, layers=22, kv_heads=4, head_dim=64
2025-04-26 20:43:45,168 - INFO - KVCache initialized. k_cache length: 22, v_cache length: 22
2025-04-26 20:43:45,168 - INFO - Example k_cache[0] shape: torch.Size([1, 4, 2048, 64])
2025-04-26 20:43:45,169 - INFO - Priming KVCache with prompt tokens (excluding last): [529, 29879, 29958, 29984, 29901, 11644, 5456, 7904, 1026, 29973, 13, 29909]
2025-04-26 20:43:45,169 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=0
2025-04-26 20:43:45,336 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:45,337 - INFO - Primed KVCache for pos=0
2025-04-26 20:43:45,337 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=1
2025-04-26 20:43:45,346 - INFO - [PyTorch] PyT L0 Output (pos=1): shape=[1, 1, 2048], min=-0.282582, max=0.266320, mean=-0.000196, all_finite=True
2025-04-26 20:43:45,346 - INFO - [PyTorch] PyT L0 Output (pos=1) first 5: -0.008421 -0.010634 -0.010333 -0.005166 0.003777
2025-04-26 20:43:45,347 - INFO - --- PyTorch Layer 0 End for pos=1 ---
2025-04-26 20:43:45,515 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:45,515 - INFO - Primed KVCache for pos=1
2025-04-26 20:43:45,516 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=2
2025-04-26 20:43:45,687 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:45,687 - INFO - Primed KVCache for pos=2
2025-04-26 20:43:45,688 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=3
2025-04-26 20:43:45,863 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:45,864 - INFO - Primed KVCache for pos=3
2025-04-26 20:43:45,864 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=4
2025-04-26 20:43:46,024 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:46,025 - INFO - Primed KVCache for pos=4
2025-04-26 20:43:46,025 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=5
2025-04-26 20:43:46,180 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:46,181 - INFO - Primed KVCache for pos=5
2025-04-26 20:43:46,181 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=6
2025-04-26 20:43:46,414 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:46,414 - INFO - Primed KVCache for pos=6
2025-04-26 20:43:46,414 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=7
2025-04-26 20:43:46,587 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:46,588 - INFO - Primed KVCache for pos=7
2025-04-26 20:43:46,588 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=8
2025-04-26 20:43:46,762 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:46,762 - INFO - Primed KVCache for pos=8
2025-04-26 20:43:46,763 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=9
2025-04-26 20:43:46,903 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:46,903 - INFO - Primed KVCache for pos=9
2025-04-26 20:43:46,904 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=10
2025-04-26 20:43:47,045 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:47,045 - INFO - Primed KVCache for pos=10
2025-04-26 20:43:47,045 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=11
2025-04-26 20:43:47,187 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:47,187 - INFO - Primed KVCache for pos=11
2025-04-26 20:43:47,187 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=12
2025-04-26 20:43:47,328 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:47,329 - INFO - First generated token logits (token-by-token, first 10): [-9.270827293395996, -9.22700309753418, 4.590665817260742, -6.461870193481445, -5.624599456787109, -4.809528827667236, -5.793954849243164, -9.606549263000488, -6.8218159675598145, -7.289485454559326]
2025-04-26 20:43:47,329 - INFO - Step 1 (Token-by-Token, pos=12): Predicted token ID: 7904
2025-04-26 20:43:47,330 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=13
2025-04-26 20:43:47,466 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:47,467 - INFO - Step 2 (Token-by-Token, pos=13): Predicted token ID: 1026
2025-04-26 20:43:47,467 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=14
2025-04-26 20:43:47,606 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:47,607 - INFO - Step 3 (Token-by-Token, pos=14): Predicted token ID: 471
2025-04-26 20:43:47,607 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=15
2025-04-26 20:43:47,782 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:47,783 - INFO - Step 4 (Token-by-Token, pos=15): Predicted token ID: 3971
2025-04-26 20:43:47,783 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=16
2025-04-26 20:43:47,923 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:47,923 - INFO - Step 5 (Token-by-Token, pos=16): Predicted token ID: 491
2025-04-26 20:43:47,924 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=17
2025-04-26 20:43:48,096 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:48,096 - INFO - Step 6 (Token-by-Token, pos=17): Predicted token ID: 4667
2025-04-26 20:43:48,097 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=18
2025-04-26 20:43:48,265 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:48,266 - INFO - Step 7 (Token-by-Token, pos=18): Predicted token ID: 23688
2025-04-26 20:43:48,267 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=19
2025-04-26 20:43:48,415 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:48,416 - INFO - Step 8 (Token-by-Token, pos=19): Predicted token ID: 29889
2025-04-26 20:43:48,416 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=20
2025-04-26 20:43:48,578 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:48,578 - INFO - Step 9 (Token-by-Token, pos=20): Predicted token ID: 13
2025-04-26 20:43:48,579 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=21
2025-04-26 20:43:48,718 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:48,718 - INFO - Step 10 (Token-by-Token, pos=21): Predicted token ID: 13
2025-04-26 20:43:48,719 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=22
2025-04-26 20:43:48,877 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:48,877 - INFO - Step 11 (Token-by-Token, pos=22): Predicted token ID: 29933
2025-04-26 20:43:48,878 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=23
2025-04-26 20:43:49,039 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:49,040 - INFO - Step 12 (Token-by-Token, pos=23): Predicted token ID: 1463
2025-04-26 20:43:49,041 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=24
2025-04-26 20:43:49,185 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:49,186 - INFO - Step 13 (Token-by-Token, pos=24): Predicted token ID: 373
2025-04-26 20:43:49,187 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=25
2025-04-26 20:43:49,339 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:49,340 - INFO - Step 14 (Token-by-Token, pos=25): Predicted token ID: 278
2025-04-26 20:43:49,340 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=26
2025-04-26 20:43:49,496 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:49,497 - INFO - Step 15 (Token-by-Token, pos=26): Predicted token ID: 1426
2025-04-26 20:43:49,498 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=27
2025-04-26 20:43:49,647 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:49,647 - INFO - Step 16 (Token-by-Token, pos=27): Predicted token ID: 5518
2025-04-26 20:43:49,647 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=28
2025-04-26 20:43:49,788 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:49,789 - INFO - Step 17 (Token-by-Token, pos=28): Predicted token ID: 2038
2025-04-26 20:43:49,789 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=29
2025-04-26 20:43:49,926 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:49,927 - INFO - Step 18 (Token-by-Token, pos=29): Predicted token ID: 29892
2025-04-26 20:43:49,927 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=30
2025-04-26 20:43:50,074 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:50,075 - INFO - Step 19 (Token-by-Token, pos=30): Predicted token ID: 5706
2025-04-26 20:43:50,076 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=31
2025-04-26 20:43:50,227 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:50,228 - INFO - Step 20 (Token-by-Token, pos=31): Predicted token ID: 278
2025-04-26 20:43:50,228 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=32
2025-04-26 20:43:50,373 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:50,373 - INFO - Step 21 (Token-by-Token, pos=32): Predicted token ID: 2933
2025-04-26 20:43:50,374 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=33
2025-04-26 20:43:50,609 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:50,609 - INFO - Step 22 (Token-by-Token, pos=33): Predicted token ID: 304
2025-04-26 20:43:50,610 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=34
2025-04-26 20:43:50,760 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:50,761 - INFO - Step 23 (Token-by-Token, pos=34): Predicted token ID: 278
2025-04-26 20:43:50,761 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=35
2025-04-26 20:43:50,924 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:50,925 - INFO - Step 24 (Token-by-Token, pos=35): Predicted token ID: 1494
2025-04-26 20:43:50,925 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=36
2025-04-26 20:43:51,068 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:51,069 - INFO - Step 25 (Token-by-Token, pos=36): Predicted token ID: 439
2025-04-26 20:43:51,069 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=37
2025-04-26 20:43:51,214 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:51,215 - INFO - Step 26 (Token-by-Token, pos=37): Predicted token ID: 267
2025-04-26 20:43:51,215 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=38
2025-04-26 20:43:51,369 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:51,369 - INFO - Step 27 (Token-by-Token, pos=38): Predicted token ID: 291
2025-04-26 20:43:51,370 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=39
2025-04-26 20:43:51,518 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:51,518 - INFO - Step 28 (Token-by-Token, pos=39): Predicted token ID: 470
2025-04-26 20:43:51,519 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=40
2025-04-26 20:43:51,705 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:51,705 - INFO - Step 29 (Token-by-Token, pos=40): Predicted token ID: 15278
2025-04-26 20:43:51,706 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=41
2025-04-26 20:43:51,897 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:51,897 - INFO - Step 30 (Token-by-Token, pos=41): Predicted token ID: 29901
2025-04-26 20:43:51,898 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=42
2025-04-26 20:43:52,140 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:52,141 - INFO - Step 31 (Token-by-Token, pos=42): Predicted token ID: 1815
2025-04-26 20:43:52,142 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=43
2025-04-26 20:43:52,333 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:52,334 - INFO - Step 32 (Token-by-Token, pos=43): Predicted token ID: 366
2025-04-26 20:43:52,335 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=44
2025-04-26 20:43:52,499 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:52,500 - INFO - Step 33 (Token-by-Token, pos=44): Predicted token ID: 19138
2025-04-26 20:43:52,501 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=45
2025-04-26 20:43:52,704 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:52,705 - INFO - Step 34 (Token-by-Token, pos=45): Predicted token ID: 675
2025-04-26 20:43:52,706 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=46
2025-04-26 20:43:52,852 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:52,852 - INFO - Step 35 (Token-by-Token, pos=46): Predicted token ID: 278
2025-04-26 20:43:52,853 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=47
2025-04-26 20:43:53,008 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:53,009 - INFO - Step 36 (Token-by-Token, pos=47): Predicted token ID: 6492
2025-04-26 20:43:53,009 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=48
2025-04-26 20:43:53,175 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:53,176 - INFO - Step 37 (Token-by-Token, pos=48): Predicted token ID: 310
2025-04-26 20:43:53,176 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=49
2025-04-26 20:43:53,335 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:53,336 - INFO - Step 38 (Token-by-Token, pos=49): Predicted token ID: 7904
2025-04-26 20:43:53,336 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=50
2025-04-26 20:43:53,491 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:53,492 - INFO - Step 39 (Token-by-Token, pos=50): Predicted token ID: 1026
2025-04-26 20:43:53,493 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=51
2025-04-26 20:43:53,658 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:53,659 - INFO - Step 40 (Token-by-Token, pos=51): Predicted token ID: 29892
2025-04-26 20:43:53,659 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=52
2025-04-26 20:43:53,810 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:53,811 - INFO - Step 41 (Token-by-Token, pos=52): Predicted token ID: 3704
2025-04-26 20:43:53,811 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=53
2025-04-26 20:43:53,988 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:53,989 - INFO - Step 42 (Token-by-Token, pos=53): Predicted token ID: 278
2025-04-26 20:43:53,989 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=54
2025-04-26 20:43:54,151 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:54,152 - INFO - Step 43 (Token-by-Token, pos=54): Predicted token ID: 1667
2025-04-26 20:43:54,152 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=55
2025-04-26 20:43:54,317 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:54,318 - INFO - Step 44 (Token-by-Token, pos=55): Predicted token ID: 4890
2025-04-26 20:43:54,318 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=56
2025-04-26 20:43:54,483 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:54,483 - INFO - Step 45 (Token-by-Token, pos=56): Predicted token ID: 322
2025-04-26 20:43:54,484 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=57
2025-04-26 20:43:54,641 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:54,642 - INFO - Step 46 (Token-by-Token, pos=57): Predicted token ID: 1009
2025-04-26 20:43:54,642 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=58
2025-04-26 20:43:54,806 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:54,807 - INFO - Step 47 (Token-by-Token, pos=58): Predicted token ID: 28792
2025-04-26 20:43:54,807 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=59
2025-04-26 20:43:54,970 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:54,971 - INFO - Step 48 (Token-by-Token, pos=59): Predicted token ID: 29973
2025-04-26 20:43:54,971 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=60
2025-04-26 20:43:55,129 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 20:43:55,129 - INFO - Step 49 (Token-by-Token, pos=60): Predicted token ID: 2
2025-04-26 20:43:55,130 - INFO - EOS token (2) generated. Stopping generation.
2025-04-26 20:43:55,130 - INFO - Full Generated Sequence IDs: [529, 29879, 29958, 29984, 29901, 11644, 5456, 7904, 1026, 29973, 13, 29909, 29901, 7904, 1026, 471, 3971, 491, 4667, 23688, 29889, 13, 13, 29933, 1463, 373, 278, 1426, 5518, 2038, 29892, 5706, 278, 2933, 304, 278, 1494, 439, 267, 291, 470, 15278, 29901, 1815, 366, 19138, 675, 278, 6492, 310, 7904, 1026, 29892, 3704, 278, 1667, 4890, 322, 1009, 28792, 29973, 2]
2025-04-26 20:43:55,131 - INFO - Full Decoded Text:
-------
<s>Q: Who wrote Hamlet?
A: Hamlet was written by William Shakespeare.

Based on the text material above, generate the response to the following quesion or instruction: Can you summarize the plot of Hamlet, including the main characters and their conflicts?
-------
2025-04-26 20:43:55,131 - INFO - Generated Part IDs: [7904, 1026, 471, 3971, 491, 4667, 23688, 29889, 13, 13, 29933, 1463, 373, 278, 1426, 5518, 2038, 29892, 5706, 278, 2933, 304, 278, 1494, 439, 267, 291, 470, 15278, 29901, 1815, 366, 19138, 675, 278, 6492, 310, 7904, 1026, 29892, 3704, 278, 1667, 4890, 322, 1009, 28792, 29973, 2]
2025-04-26 20:43:55,131 - INFO - Generated Decoded Text (raw):
-------
Hamlet was written by William Shakespeare.

Based on the text material above, generate the response to the following quesion or instruction: Can you summarize the plot of Hamlet, including the main characters and their conflicts?
-------
2025-04-26 20:43:55,132 - INFO - Generated Decoded Text (cleaned):
-------
Hamlet was written by William Shakespeare.
-------
