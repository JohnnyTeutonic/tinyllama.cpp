2025-04-26 21:40:52,268 - INFO - Running with args: Namespace(token_by_token=True)
2025-04-26 21:40:52,271 - INFO - Config: {'architectures': ['LlamaForCausalLM'], 'attention_bias': False, 'bos_token_id': 1, 'eos_token_id': 2, 'hidden_act': 'silu', 'hidden_size': 2048, 'initializer_range': 0.02, 'intermediate_size': 5632, 'max_position_embeddings': 2048, 'model_type': 'llama', 'num_attention_heads': 32, 'num_hidden_layers': 22, 'num_key_value_heads': 4, 'pretraining_tp': 1, 'rms_norm_eps': 1e-05, 'rope_scaling': None, 'rope_theta': 10000.0, 'tie_word_embeddings': False, 'torch_dtype': 'bfloat16', 'transformers_version': '4.35.0', 'use_cache': True, 'vocab_size': 32000}
2025-04-26 21:40:54,285 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:54,285 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:54,285 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:54,285 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:54,286 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:54,495 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:54,496 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:54,496 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:54,496 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:54,496 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:54,708 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:54,709 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:54,709 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:54,709 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:54,709 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:54,918 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:54,919 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:54,919 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:54,919 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:54,920 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:55,130 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:55,130 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:55,131 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:55,131 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:55,131 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:55,341 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:55,342 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:55,342 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:55,343 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:55,343 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:55,552 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:55,552 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:55,552 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:55,553 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:55,553 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:55,764 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:55,765 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:55,765 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:55,765 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:55,765 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:55,977 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:55,977 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:55,978 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:55,978 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:55,979 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:56,188 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:56,188 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:56,189 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:56,189 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:56,190 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:56,433 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:56,433 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:56,434 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:56,434 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:56,434 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:56,651 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:56,652 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:56,652 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:56,653 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:56,653 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:56,861 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:56,862 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:56,862 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:56,862 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:56,862 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:57,069 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:57,070 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:57,070 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:57,071 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:57,071 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:57,282 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:57,283 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:57,283 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:57,283 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:57,284 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:57,491 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:57,492 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:57,492 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:57,493 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:57,493 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:57,706 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:57,707 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:57,707 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:57,707 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:57,708 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:57,940 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:57,940 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:57,941 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:57,941 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:57,942 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:58,190 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:58,191 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:58,191 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:58,192 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:58,192 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:58,487 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:58,488 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:58,488 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:58,489 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:58,489 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:58,778 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:58,778 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:58,779 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:58,779 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:58,779 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:58,999 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=0): cos=0.540302 sin=0.841471
2025-04-26 21:40:58,999 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=1): cos=0.731761 sin=0.681561
2025-04-26 21:40:59,000 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=2): cos=0.846009 sin=0.533168
2025-04-26 21:40:59,000 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=3): cos=0.912396 sin=0.409309
2025-04-26 21:40:59,000 - INFO - PyTorch RoPE Precompute (Pos=1, FreqDim=4): cos=0.950415 sin=0.310984
2025-04-26 21:40:59,349 - INFO - Initializing KVCache for batch_size=1, max_seq_len=2048, layers=22, kv_heads=4, head_dim=64
2025-04-26 21:40:59,368 - INFO - KVCache initialized. k_cache length: 22, v_cache length: 22
2025-04-26 21:40:59,368 - INFO - Example k_cache[0] shape: torch.Size([1, 4, 2048, 64])
2025-04-26 21:40:59,368 - INFO - Loading model.embed_tokens.weight:
2025-04-26 21:40:59,369 - INFO -   - Tensor shape: torch.Size([32000, 2048]), dtype: torch.bfloat16
2025-04-26 21:40:59,369 - INFO -   - Param shape: torch.Size([32000, 2048]), dtype: torch.float32
2025-04-26 21:41:00,461 - INFO -   - Copied (Embedding). Param mean: -0.0000, std: 0.0149
2025-04-26 21:41:00,462 - INFO - Loading lm_head.weight:
2025-04-26 21:41:00,462 - INFO -   - Tensor shape: torch.Size([32000, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:00,463 - INFO -   - Param shape: torch.Size([32000, 2048]), dtype: torch.float32
2025-04-26 21:41:01,454 - INFO -   - Copied (Output Head). Param mean: -0.0004, std: 0.0247
2025-04-26 21:41:01,454 - INFO - Loading model.norm.weight:
2025-04-26 21:41:01,455 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:01,455 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:01,456 - INFO -   - Copied (Final Norm). Param mean: 1.9149, std: 0.1365
2025-04-26 21:41:01,456 - INFO - Loading model.layers.0.input_layernorm.weight:
2025-04-26 21:41:01,456 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:01,456 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:01,457 - INFO -   - Copied. Param mean: 0.0058, std: 0.0460
2025-04-26 21:41:01,457 - INFO - Loading model.layers.0.post_attention_layernorm.weight:
2025-04-26 21:41:01,457 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:01,457 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:01,457 - INFO -   - Copied. Param mean: 0.0746, std: 0.0331
2025-04-26 21:41:01,457 - INFO - Loading model.layers.0.self_attn.q_proj.weight:
2025-04-26 21:41:01,458 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:01,458 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:01,528 - INFO -   - Copied. Param mean: -0.0000, std: 0.0164
2025-04-26 21:41:01,529 - INFO - Loading model.layers.0.self_attn.k_proj.weight:
2025-04-26 21:41:01,529 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:01,529 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:01,549 - INFO -   - Copied. Param mean: -0.0001, std: 0.0318
2025-04-26 21:41:01,549 - INFO - Loading model.layers.0.self_attn.v_proj.weight:
2025-04-26 21:41:01,550 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:01,550 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:01,563 - INFO -   - Copied. Param mean: 0.0000, std: 0.0110
2025-04-26 21:41:01,563 - INFO - Loading model.layers.0.self_attn.o_proj.weight:
2025-04-26 21:41:01,563 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:01,564 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:01,623 - INFO -   - Copied. Param mean: 0.0000, std: 0.0083
2025-04-26 21:41:01,624 - INFO - Loading model.layers.0.mlp.gate_proj.weight:
2025-04-26 21:41:01,624 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:01,624 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:01,792 - INFO -   - Copied. Param mean: -0.0000, std: 0.0166
2025-04-26 21:41:01,793 - INFO - Loading model.layers.0.mlp.up_proj.weight:
2025-04-26 21:41:01,793 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:01,794 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:01,968 - INFO -   - Copied. Param mean: -0.0000, std: 0.0168
2025-04-26 21:41:01,968 - INFO - Loading model.layers.0.mlp.down_proj.weight:
2025-04-26 21:41:01,969 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:01,969 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:02,163 - INFO -   - Copied. Param mean: 0.0000, std: 0.0166
2025-04-26 21:41:02,163 - INFO - Loading model.layers.1.input_layernorm.weight:
2025-04-26 21:41:02,163 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:02,163 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:02,164 - INFO -   - Copied. Param mean: 0.0405, std: 0.0559
2025-04-26 21:41:02,164 - INFO - Loading model.layers.1.post_attention_layernorm.weight:
2025-04-26 21:41:02,164 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:02,164 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:02,165 - INFO -   - Copied. Param mean: 0.1284, std: 0.0211
2025-04-26 21:41:02,165 - INFO - Loading model.layers.1.self_attn.q_proj.weight:
2025-04-26 21:41:02,165 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:02,166 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:02,224 - INFO -   - Copied. Param mean: 0.0000, std: 0.0294
2025-04-26 21:41:02,224 - INFO - Loading model.layers.1.self_attn.k_proj.weight:
2025-04-26 21:41:02,224 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:02,225 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:02,241 - INFO -   - Copied. Param mean: -0.0000, std: 0.0479
2025-04-26 21:41:02,242 - INFO - Loading model.layers.1.self_attn.v_proj.weight:
2025-04-26 21:41:02,242 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:02,242 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:02,251 - INFO -   - Copied. Param mean: 0.0000, std: 0.0134
2025-04-26 21:41:02,251 - INFO - Loading model.layers.1.self_attn.o_proj.weight:
2025-04-26 21:41:02,251 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:02,252 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:02,308 - INFO -   - Copied. Param mean: -0.0000, std: 0.0137
2025-04-26 21:41:02,308 - INFO - Loading model.layers.1.mlp.gate_proj.weight:
2025-04-26 21:41:02,309 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:02,309 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:02,477 - INFO -   - Copied. Param mean: 0.0000, std: 0.0181
2025-04-26 21:41:02,477 - INFO - Loading model.layers.1.mlp.up_proj.weight:
2025-04-26 21:41:02,477 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:02,477 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:02,670 - INFO -   - Copied. Param mean: -0.0000, std: 0.0173
2025-04-26 21:41:02,670 - INFO - Loading model.layers.1.mlp.down_proj.weight:
2025-04-26 21:41:02,670 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:02,671 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:02,831 - INFO -   - Copied. Param mean: 0.0000, std: 0.0171
2025-04-26 21:41:02,832 - INFO - Loading model.layers.2.input_layernorm.weight:
2025-04-26 21:41:02,832 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:02,832 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:02,833 - INFO -   - Copied. Param mean: 0.0846, std: 0.0684
2025-04-26 21:41:02,833 - INFO - Loading model.layers.2.post_attention_layernorm.weight:
2025-04-26 21:41:02,833 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:02,833 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:02,833 - INFO -   - Copied. Param mean: 0.1674, std: 0.0219
2025-04-26 21:41:02,833 - INFO - Loading model.layers.2.self_attn.q_proj.weight:
2025-04-26 21:41:02,834 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:02,834 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:02,891 - INFO -   - Copied. Param mean: 0.0000, std: 0.0322
2025-04-26 21:41:02,891 - INFO - Loading model.layers.2.self_attn.k_proj.weight:
2025-04-26 21:41:02,891 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:02,892 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:02,903 - INFO -   - Copied. Param mean: 0.0001, std: 0.0542
2025-04-26 21:41:02,903 - INFO - Loading model.layers.2.self_attn.v_proj.weight:
2025-04-26 21:41:02,904 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:02,904 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:02,920 - INFO -   - Copied. Param mean: -0.0000, std: 0.0118
2025-04-26 21:41:02,921 - INFO - Loading model.layers.2.self_attn.o_proj.weight:
2025-04-26 21:41:02,921 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:02,921 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:02,982 - INFO -   - Copied. Param mean: -0.0000, std: 0.0141
2025-04-26 21:41:02,982 - INFO - Loading model.layers.2.mlp.gate_proj.weight:
2025-04-26 21:41:02,983 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:02,983 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:03,163 - INFO -   - Copied. Param mean: -0.0000, std: 0.0185
2025-04-26 21:41:03,163 - INFO - Loading model.layers.2.mlp.up_proj.weight:
2025-04-26 21:41:03,163 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:03,164 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:03,330 - INFO -   - Copied. Param mean: -0.0000, std: 0.0175
2025-04-26 21:41:03,331 - INFO - Loading model.layers.2.mlp.down_proj.weight:
2025-04-26 21:41:03,331 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:03,331 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:03,495 - INFO -   - Copied. Param mean: 0.0000, std: 0.0174
2025-04-26 21:41:03,496 - INFO - Loading model.layers.3.input_layernorm.weight:
2025-04-26 21:41:03,496 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:03,496 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:03,496 - INFO -   - Copied. Param mean: 0.2243, std: 0.0547
2025-04-26 21:41:03,496 - INFO - Loading model.layers.3.post_attention_layernorm.weight:
2025-04-26 21:41:03,496 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:03,497 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:03,497 - INFO -   - Copied. Param mean: 0.1843, std: 0.0214
2025-04-26 21:41:03,497 - INFO - Loading model.layers.3.self_attn.q_proj.weight:
2025-04-26 21:41:03,497 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:03,497 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:03,560 - INFO -   - Copied. Param mean: -0.0000, std: 0.0271
2025-04-26 21:41:03,560 - INFO - Loading model.layers.3.self_attn.k_proj.weight:
2025-04-26 21:41:03,561 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:03,561 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:03,576 - INFO -   - Copied. Param mean: 0.0001, std: 0.0477
2025-04-26 21:41:03,576 - INFO - Loading model.layers.3.self_attn.v_proj.weight:
2025-04-26 21:41:03,576 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:03,576 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:03,588 - INFO -   - Copied. Param mean: -0.0000, std: 0.0114
2025-04-26 21:41:03,588 - INFO - Loading model.layers.3.self_attn.o_proj.weight:
2025-04-26 21:41:03,588 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:03,588 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:03,647 - INFO -   - Copied. Param mean: 0.0000, std: 0.0143
2025-04-26 21:41:03,647 - INFO - Loading model.layers.3.mlp.gate_proj.weight:
2025-04-26 21:41:03,648 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:03,648 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:03,828 - INFO -   - Copied. Param mean: -0.0000, std: 0.0187
2025-04-26 21:41:03,828 - INFO - Loading model.layers.3.mlp.up_proj.weight:
2025-04-26 21:41:03,829 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:03,829 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:03,994 - INFO -   - Copied. Param mean: -0.0000, std: 0.0174
2025-04-26 21:41:03,994 - INFO - Loading model.layers.3.mlp.down_proj.weight:
2025-04-26 21:41:03,994 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:03,995 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:04,165 - INFO -   - Copied. Param mean: 0.0000, std: 0.0173
2025-04-26 21:41:04,166 - INFO - Loading model.layers.4.input_layernorm.weight:
2025-04-26 21:41:04,166 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:04,166 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:04,167 - INFO -   - Copied. Param mean: 0.3199, std: 0.0582
2025-04-26 21:41:04,167 - INFO - Loading model.layers.4.post_attention_layernorm.weight:
2025-04-26 21:41:04,167 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:04,167 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:04,168 - INFO -   - Copied. Param mean: 0.1978, std: 0.0238
2025-04-26 21:41:04,168 - INFO - Loading model.layers.4.self_attn.q_proj.weight:
2025-04-26 21:41:04,168 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:04,168 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:04,234 - INFO -   - Copied. Param mean: 0.0000, std: 0.0260
2025-04-26 21:41:04,234 - INFO - Loading model.layers.4.self_attn.k_proj.weight:
2025-04-26 21:41:04,234 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:04,235 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:04,253 - INFO -   - Copied. Param mean: 0.0000, std: 0.0490
2025-04-26 21:41:04,253 - INFO - Loading model.layers.4.self_attn.v_proj.weight:
2025-04-26 21:41:04,253 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:04,254 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:04,272 - INFO -   - Copied. Param mean: 0.0000, std: 0.0101
2025-04-26 21:41:04,273 - INFO - Loading model.layers.4.self_attn.o_proj.weight:
2025-04-26 21:41:04,273 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:04,273 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:04,334 - INFO -   - Copied. Param mean: -0.0000, std: 0.0140
2025-04-26 21:41:04,335 - INFO - Loading model.layers.4.mlp.gate_proj.weight:
2025-04-26 21:41:04,335 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:04,335 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:04,520 - INFO -   - Copied. Param mean: -0.0000, std: 0.0190
2025-04-26 21:41:04,520 - INFO - Loading model.layers.4.mlp.up_proj.weight:
2025-04-26 21:41:04,520 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:04,521 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:04,695 - INFO -   - Copied. Param mean: -0.0000, std: 0.0173
2025-04-26 21:41:04,695 - INFO - Loading model.layers.4.mlp.down_proj.weight:
2025-04-26 21:41:04,695 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:04,695 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:04,861 - INFO -   - Copied. Param mean: -0.0000, std: 0.0173
2025-04-26 21:41:04,861 - INFO - Loading model.layers.5.input_layernorm.weight:
2025-04-26 21:41:04,862 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:04,862 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:04,862 - INFO -   - Copied. Param mean: 0.2800, std: 0.0488
2025-04-26 21:41:04,862 - INFO - Loading model.layers.5.post_attention_layernorm.weight:
2025-04-26 21:41:04,863 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:04,863 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:04,863 - INFO -   - Copied. Param mean: 0.2160, std: 0.0225
2025-04-26 21:41:04,863 - INFO - Loading model.layers.5.self_attn.q_proj.weight:
2025-04-26 21:41:04,863 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:04,863 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:04,929 - INFO -   - Copied. Param mean: -0.0000, std: 0.0271
2025-04-26 21:41:04,930 - INFO - Loading model.layers.5.self_attn.k_proj.weight:
2025-04-26 21:41:04,930 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:04,930 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:04,946 - INFO -   - Copied. Param mean: 0.0000, std: 0.0502
2025-04-26 21:41:04,946 - INFO - Loading model.layers.5.self_attn.v_proj.weight:
2025-04-26 21:41:04,946 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:04,947 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:04,956 - INFO -   - Copied. Param mean: 0.0000, std: 0.0115
2025-04-26 21:41:04,956 - INFO - Loading model.layers.5.self_attn.o_proj.weight:
2025-04-26 21:41:04,956 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:04,956 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:05,014 - INFO -   - Copied. Param mean: -0.0000, std: 0.0145
2025-04-26 21:41:05,014 - INFO - Loading model.layers.5.mlp.gate_proj.weight:
2025-04-26 21:41:05,014 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:05,015 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:05,191 - INFO -   - Copied. Param mean: -0.0000, std: 0.0192
2025-04-26 21:41:05,192 - INFO - Loading model.layers.5.mlp.up_proj.weight:
2025-04-26 21:41:05,192 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:05,192 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:05,363 - INFO -   - Copied. Param mean: 0.0000, std: 0.0174
2025-04-26 21:41:05,363 - INFO - Loading model.layers.5.mlp.down_proj.weight:
2025-04-26 21:41:05,363 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:05,363 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:05,560 - INFO -   - Copied. Param mean: 0.0000, std: 0.0173
2025-04-26 21:41:05,561 - INFO - Loading model.layers.6.input_layernorm.weight:
2025-04-26 21:41:05,561 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:05,561 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:05,562 - INFO -   - Copied. Param mean: 0.2680, std: 0.0792
2025-04-26 21:41:05,562 - INFO - Loading model.layers.6.post_attention_layernorm.weight:
2025-04-26 21:41:05,562 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:05,562 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:05,563 - INFO -   - Copied. Param mean: 0.2251, std: 0.0219
2025-04-26 21:41:05,563 - INFO - Loading model.layers.6.self_attn.q_proj.weight:
2025-04-26 21:41:05,563 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:05,563 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:05,630 - INFO -   - Copied. Param mean: 0.0000, std: 0.0263
2025-04-26 21:41:05,630 - INFO - Loading model.layers.6.self_attn.k_proj.weight:
2025-04-26 21:41:05,631 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:05,631 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:05,649 - INFO -   - Copied. Param mean: 0.0001, std: 0.0471
2025-04-26 21:41:05,650 - INFO - Loading model.layers.6.self_attn.v_proj.weight:
2025-04-26 21:41:05,650 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:05,650 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:05,666 - INFO -   - Copied. Param mean: -0.0000, std: 0.0113
2025-04-26 21:41:05,666 - INFO - Loading model.layers.6.self_attn.o_proj.weight:
2025-04-26 21:41:05,667 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:05,667 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:05,728 - INFO -   - Copied. Param mean: 0.0000, std: 0.0140
2025-04-26 21:41:05,728 - INFO - Loading model.layers.6.mlp.gate_proj.weight:
2025-04-26 21:41:05,728 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:05,728 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:05,896 - INFO -   - Copied. Param mean: 0.0000, std: 0.0196
2025-04-26 21:41:05,896 - INFO - Loading model.layers.6.mlp.up_proj.weight:
2025-04-26 21:41:05,897 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:05,897 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:06,083 - INFO -   - Copied. Param mean: -0.0000, std: 0.0172
2025-04-26 21:41:06,083 - INFO - Loading model.layers.6.mlp.down_proj.weight:
2025-04-26 21:41:06,084 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:06,084 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:06,251 - INFO -   - Copied. Param mean: -0.0000, std: 0.0171
2025-04-26 21:41:06,252 - INFO - Loading model.layers.7.input_layernorm.weight:
2025-04-26 21:41:06,252 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:06,252 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:06,252 - INFO -   - Copied. Param mean: 0.3070, std: 0.0577
2025-04-26 21:41:06,253 - INFO - Loading model.layers.7.post_attention_layernorm.weight:
2025-04-26 21:41:06,253 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:06,253 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:06,253 - INFO -   - Copied. Param mean: 0.2384, std: 0.0224
2025-04-26 21:41:06,253 - INFO - Loading model.layers.7.self_attn.q_proj.weight:
2025-04-26 21:41:06,253 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:06,253 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:06,315 - INFO -   - Copied. Param mean: -0.0000, std: 0.0266
2025-04-26 21:41:06,315 - INFO - Loading model.layers.7.self_attn.k_proj.weight:
2025-04-26 21:41:06,315 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:06,315 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:06,328 - INFO -   - Copied. Param mean: 0.0000, std: 0.0450
2025-04-26 21:41:06,329 - INFO - Loading model.layers.7.self_attn.v_proj.weight:
2025-04-26 21:41:06,329 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:06,329 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:06,346 - INFO -   - Copied. Param mean: -0.0000, std: 0.0130
2025-04-26 21:41:06,347 - INFO - Loading model.layers.7.self_attn.o_proj.weight:
2025-04-26 21:41:06,347 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:06,347 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:06,421 - INFO -   - Copied. Param mean: 0.0000, std: 0.0149
2025-04-26 21:41:06,421 - INFO - Loading model.layers.7.mlp.gate_proj.weight:
2025-04-26 21:41:06,422 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:06,422 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:06,598 - INFO -   - Copied. Param mean: 0.0001, std: 0.0209
2025-04-26 21:41:06,598 - INFO - Loading model.layers.7.mlp.up_proj.weight:
2025-04-26 21:41:06,599 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:06,599 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:06,782 - INFO -   - Copied. Param mean: 0.0000, std: 0.0168
2025-04-26 21:41:06,782 - INFO - Loading model.layers.7.mlp.down_proj.weight:
2025-04-26 21:41:06,782 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:06,783 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:06,966 - INFO -   - Copied. Param mean: 0.0000, std: 0.0167
2025-04-26 21:41:06,966 - INFO - Loading model.layers.8.input_layernorm.weight:
2025-04-26 21:41:06,966 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:06,966 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:06,967 - INFO -   - Copied. Param mean: 0.3188, std: 0.1083
2025-04-26 21:41:06,967 - INFO - Loading model.layers.8.post_attention_layernorm.weight:
2025-04-26 21:41:06,967 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:06,967 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:06,968 - INFO -   - Copied. Param mean: 0.2645, std: 0.0304
2025-04-26 21:41:06,968 - INFO - Loading model.layers.8.self_attn.q_proj.weight:
2025-04-26 21:41:06,968 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:06,968 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:07,037 - INFO -   - Copied. Param mean: -0.0000, std: 0.0273
2025-04-26 21:41:07,038 - INFO - Loading model.layers.8.self_attn.k_proj.weight:
2025-04-26 21:41:07,038 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:07,038 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:07,053 - INFO -   - Copied. Param mean: -0.0000, std: 0.0466
2025-04-26 21:41:07,054 - INFO - Loading model.layers.8.self_attn.v_proj.weight:
2025-04-26 21:41:07,054 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:07,054 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:07,065 - INFO -   - Copied. Param mean: -0.0000, std: 0.0121
2025-04-26 21:41:07,065 - INFO - Loading model.layers.8.self_attn.o_proj.weight:
2025-04-26 21:41:07,065 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:07,066 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:07,127 - INFO -   - Copied. Param mean: -0.0000, std: 0.0145
2025-04-26 21:41:07,128 - INFO - Loading model.layers.8.mlp.gate_proj.weight:
2025-04-26 21:41:07,128 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:07,128 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:07,319 - INFO -   - Copied. Param mean: 0.0001, std: 0.0202
2025-04-26 21:41:07,319 - INFO - Loading model.layers.8.mlp.up_proj.weight:
2025-04-26 21:41:07,320 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:07,320 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:07,499 - INFO -   - Copied. Param mean: 0.0000, std: 0.0173
2025-04-26 21:41:07,499 - INFO - Loading model.layers.8.mlp.down_proj.weight:
2025-04-26 21:41:07,499 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:07,499 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:07,690 - INFO -   - Copied. Param mean: 0.0000, std: 0.0172
2025-04-26 21:41:07,690 - INFO - Loading model.layers.9.input_layernorm.weight:
2025-04-26 21:41:07,691 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:07,691 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:07,691 - INFO -   - Copied. Param mean: 0.3141, std: 0.0609
2025-04-26 21:41:07,691 - INFO - Loading model.layers.9.post_attention_layernorm.weight:
2025-04-26 21:41:07,691 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:07,692 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:07,692 - INFO -   - Copied. Param mean: 0.2707, std: 0.0268
2025-04-26 21:41:07,692 - INFO - Loading model.layers.9.self_attn.q_proj.weight:
2025-04-26 21:41:07,692 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:07,692 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:07,764 - INFO -   - Copied. Param mean: 0.0000, std: 0.0266
2025-04-26 21:41:07,764 - INFO - Loading model.layers.9.self_attn.k_proj.weight:
2025-04-26 21:41:07,764 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:07,764 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:07,782 - INFO -   - Copied. Param mean: 0.0001, std: 0.0479
2025-04-26 21:41:07,782 - INFO - Loading model.layers.9.self_attn.v_proj.weight:
2025-04-26 21:41:07,783 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:07,783 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:07,793 - INFO -   - Copied. Param mean: 0.0000, std: 0.0122
2025-04-26 21:41:07,794 - INFO - Loading model.layers.9.self_attn.o_proj.weight:
2025-04-26 21:41:07,794 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:07,794 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:07,857 - INFO -   - Copied. Param mean: 0.0000, std: 0.0150
2025-04-26 21:41:07,857 - INFO - Loading model.layers.9.mlp.gate_proj.weight:
2025-04-26 21:41:07,857 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:07,857 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:08,028 - INFO -   - Copied. Param mean: 0.0000, std: 0.0207
2025-04-26 21:41:08,028 - INFO - Loading model.layers.9.mlp.up_proj.weight:
2025-04-26 21:41:08,028 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:08,029 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:08,200 - INFO -   - Copied. Param mean: 0.0000, std: 0.0171
2025-04-26 21:41:08,200 - INFO - Loading model.layers.9.mlp.down_proj.weight:
2025-04-26 21:41:08,201 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:08,201 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:08,401 - INFO -   - Copied. Param mean: -0.0000, std: 0.0169
2025-04-26 21:41:08,402 - INFO - Loading model.layers.10.input_layernorm.weight:
2025-04-26 21:41:08,402 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:08,402 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:08,402 - INFO -   - Copied. Param mean: 0.3328, std: 0.0563
2025-04-26 21:41:08,402 - INFO - Loading model.layers.10.post_attention_layernorm.weight:
2025-04-26 21:41:08,403 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:08,403 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:08,403 - INFO -   - Copied. Param mean: 0.2796, std: 0.0302
2025-04-26 21:41:08,403 - INFO - Loading model.layers.10.self_attn.q_proj.weight:
2025-04-26 21:41:08,403 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:08,403 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:08,460 - INFO -   - Copied. Param mean: 0.0000, std: 0.0268
2025-04-26 21:41:08,461 - INFO - Loading model.layers.10.self_attn.k_proj.weight:
2025-04-26 21:41:08,461 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:08,461 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:08,472 - INFO -   - Copied. Param mean: -0.0000, std: 0.0493
2025-04-26 21:41:08,472 - INFO - Loading model.layers.10.self_attn.v_proj.weight:
2025-04-26 21:41:08,472 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:08,472 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:08,488 - INFO -   - Copied. Param mean: -0.0000, std: 0.0123
2025-04-26 21:41:08,488 - INFO - Loading model.layers.10.self_attn.o_proj.weight:
2025-04-26 21:41:08,488 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:08,489 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:08,558 - INFO -   - Copied. Param mean: 0.0000, std: 0.0149
2025-04-26 21:41:08,559 - INFO - Loading model.layers.10.mlp.gate_proj.weight:
2025-04-26 21:41:08,559 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:08,559 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:08,754 - INFO -   - Copied. Param mean: 0.0000, std: 0.0204
2025-04-26 21:41:08,755 - INFO - Loading model.layers.10.mlp.up_proj.weight:
2025-04-26 21:41:08,755 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:08,755 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:08,949 - INFO -   - Copied. Param mean: 0.0000, std: 0.0174
2025-04-26 21:41:08,949 - INFO - Loading model.layers.10.mlp.down_proj.weight:
2025-04-26 21:41:08,950 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:08,950 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:09,142 - INFO -   - Copied. Param mean: 0.0000, std: 0.0173
2025-04-26 21:41:09,143 - INFO - Loading model.layers.11.input_layernorm.weight:
2025-04-26 21:41:09,143 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:09,143 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:09,143 - INFO -   - Copied. Param mean: 0.3955, std: 0.0736
2025-04-26 21:41:09,144 - INFO - Loading model.layers.11.post_attention_layernorm.weight:
2025-04-26 21:41:09,144 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:09,144 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:09,144 - INFO -   - Copied. Param mean: 0.2884, std: 0.0302
2025-04-26 21:41:09,144 - INFO - Loading model.layers.11.self_attn.q_proj.weight:
2025-04-26 21:41:09,144 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:09,145 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:09,216 - INFO -   - Copied. Param mean: 0.0000, std: 0.0256
2025-04-26 21:41:09,216 - INFO - Loading model.layers.11.self_attn.k_proj.weight:
2025-04-26 21:41:09,217 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:09,217 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:09,233 - INFO -   - Copied. Param mean: -0.0001, std: 0.0445
2025-04-26 21:41:09,233 - INFO - Loading model.layers.11.self_attn.v_proj.weight:
2025-04-26 21:41:09,233 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:09,233 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:09,247 - INFO -   - Copied. Param mean: 0.0000, std: 0.0115
2025-04-26 21:41:09,247 - INFO - Loading model.layers.11.self_attn.o_proj.weight:
2025-04-26 21:41:09,247 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:09,247 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:09,313 - INFO -   - Copied. Param mean: 0.0000, std: 0.0146
2025-04-26 21:41:09,313 - INFO - Loading model.layers.11.mlp.gate_proj.weight:
2025-04-26 21:41:09,314 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:09,314 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:09,508 - INFO -   - Copied. Param mean: -0.0000, std: 0.0205
2025-04-26 21:41:09,509 - INFO - Loading model.layers.11.mlp.up_proj.weight:
2025-04-26 21:41:09,509 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:09,509 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:09,708 - INFO -   - Copied. Param mean: -0.0000, std: 0.0174
2025-04-26 21:41:09,709 - INFO - Loading model.layers.11.mlp.down_proj.weight:
2025-04-26 21:41:09,709 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:09,709 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:09,871 - INFO -   - Copied. Param mean: -0.0000, std: 0.0172
2025-04-26 21:41:09,872 - INFO - Loading model.layers.12.input_layernorm.weight:
2025-04-26 21:41:09,872 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:09,872 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:09,872 - INFO -   - Copied. Param mean: 0.3438, std: 0.0674
2025-04-26 21:41:09,873 - INFO - Loading model.layers.12.post_attention_layernorm.weight:
2025-04-26 21:41:09,873 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:09,873 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:09,873 - INFO -   - Copied. Param mean: 0.2991, std: 0.0256
2025-04-26 21:41:09,873 - INFO - Loading model.layers.12.self_attn.q_proj.weight:
2025-04-26 21:41:09,873 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:09,874 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:09,932 - INFO -   - Copied. Param mean: -0.0000, std: 0.0259
2025-04-26 21:41:09,932 - INFO - Loading model.layers.12.self_attn.k_proj.weight:
2025-04-26 21:41:09,932 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:09,933 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:09,941 - INFO -   - Copied. Param mean: -0.0001, std: 0.0462
2025-04-26 21:41:09,941 - INFO - Loading model.layers.12.self_attn.v_proj.weight:
2025-04-26 21:41:09,941 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:09,941 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:09,951 - INFO -   - Copied. Param mean: -0.0001, std: 0.0144
2025-04-26 21:41:09,952 - INFO - Loading model.layers.12.self_attn.o_proj.weight:
2025-04-26 21:41:09,952 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:09,952 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:10,007 - INFO -   - Copied. Param mean: -0.0000, std: 0.0157
2025-04-26 21:41:10,008 - INFO - Loading model.layers.12.mlp.gate_proj.weight:
2025-04-26 21:41:10,008 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:10,008 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:10,179 - INFO -   - Copied. Param mean: -0.0000, std: 0.0212
2025-04-26 21:41:10,180 - INFO - Loading model.layers.12.mlp.up_proj.weight:
2025-04-26 21:41:10,180 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:10,180 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:10,352 - INFO -   - Copied. Param mean: -0.0000, std: 0.0172
2025-04-26 21:41:10,353 - INFO - Loading model.layers.12.mlp.down_proj.weight:
2025-04-26 21:41:10,353 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:10,353 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:10,539 - INFO -   - Copied. Param mean: -0.0000, std: 0.0170
2025-04-26 21:41:10,539 - INFO - Loading model.layers.13.input_layernorm.weight:
2025-04-26 21:41:10,540 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:10,540 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:10,540 - INFO -   - Copied. Param mean: 0.3773, std: 0.0761
2025-04-26 21:41:10,541 - INFO - Loading model.layers.13.post_attention_layernorm.weight:
2025-04-26 21:41:10,541 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:10,541 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:10,541 - INFO -   - Copied. Param mean: 0.3128, std: 0.0242
2025-04-26 21:41:10,541 - INFO - Loading model.layers.13.self_attn.q_proj.weight:
2025-04-26 21:41:10,542 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:10,542 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:10,602 - INFO -   - Copied. Param mean: 0.0000, std: 0.0254
2025-04-26 21:41:10,603 - INFO - Loading model.layers.13.self_attn.k_proj.weight:
2025-04-26 21:41:10,603 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:10,603 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:10,624 - INFO -   - Copied. Param mean: -0.0000, std: 0.0469
2025-04-26 21:41:10,625 - INFO - Loading model.layers.13.self_attn.v_proj.weight:
2025-04-26 21:41:10,625 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:10,625 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:10,637 - INFO -   - Copied. Param mean: -0.0000, std: 0.0123
2025-04-26 21:41:10,637 - INFO - Loading model.layers.13.self_attn.o_proj.weight:
2025-04-26 21:41:10,637 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:10,638 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:10,696 - INFO -   - Copied. Param mean: -0.0000, std: 0.0151
2025-04-26 21:41:10,696 - INFO - Loading model.layers.13.mlp.gate_proj.weight:
2025-04-26 21:41:10,697 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:10,697 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:10,867 - INFO -   - Copied. Param mean: -0.0000, std: 0.0211
2025-04-26 21:41:10,867 - INFO - Loading model.layers.13.mlp.up_proj.weight:
2025-04-26 21:41:10,868 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:10,868 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:11,055 - INFO -   - Copied. Param mean: -0.0000, std: 0.0174
2025-04-26 21:41:11,055 - INFO - Loading model.layers.13.mlp.down_proj.weight:
2025-04-26 21:41:11,056 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:11,056 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:11,233 - INFO -   - Copied. Param mean: 0.0000, std: 0.0172
2025-04-26 21:41:11,233 - INFO - Loading model.layers.14.input_layernorm.weight:
2025-04-26 21:41:11,233 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:11,234 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:11,234 - INFO -   - Copied. Param mean: 0.3614, std: 0.0651
2025-04-26 21:41:11,234 - INFO - Loading model.layers.14.post_attention_layernorm.weight:
2025-04-26 21:41:11,234 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:11,234 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:11,235 - INFO -   - Copied. Param mean: 0.3289, std: 0.0258
2025-04-26 21:41:11,235 - INFO - Loading model.layers.14.self_attn.q_proj.weight:
2025-04-26 21:41:11,235 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:11,235 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:11,303 - INFO -   - Copied. Param mean: -0.0000, std: 0.0256
2025-04-26 21:41:11,303 - INFO - Loading model.layers.14.self_attn.k_proj.weight:
2025-04-26 21:41:11,303 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:11,304 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:11,324 - INFO -   - Copied. Param mean: -0.0000, std: 0.0482
2025-04-26 21:41:11,325 - INFO - Loading model.layers.14.self_attn.v_proj.weight:
2025-04-26 21:41:11,325 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:11,325 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:11,347 - INFO -   - Copied. Param mean: 0.0000, std: 0.0134
2025-04-26 21:41:11,348 - INFO - Loading model.layers.14.self_attn.o_proj.weight:
2025-04-26 21:41:11,348 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:11,348 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:11,425 - INFO -   - Copied. Param mean: -0.0000, std: 0.0155
2025-04-26 21:41:11,426 - INFO - Loading model.layers.14.mlp.gate_proj.weight:
2025-04-26 21:41:11,426 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:11,426 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:11,627 - INFO -   - Copied. Param mean: -0.0001, std: 0.0211
2025-04-26 21:41:11,627 - INFO - Loading model.layers.14.mlp.up_proj.weight:
2025-04-26 21:41:11,627 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:11,628 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:11,808 - INFO -   - Copied. Param mean: 0.0000, std: 0.0179
2025-04-26 21:41:11,809 - INFO - Loading model.layers.14.mlp.down_proj.weight:
2025-04-26 21:41:11,809 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:11,809 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:11,994 - INFO -   - Copied. Param mean: 0.0000, std: 0.0176
2025-04-26 21:41:11,994 - INFO - Loading model.layers.15.input_layernorm.weight:
2025-04-26 21:41:11,995 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:11,995 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:11,995 - INFO -   - Copied. Param mean: 0.4188, std: 0.0651
2025-04-26 21:41:11,995 - INFO - Loading model.layers.15.post_attention_layernorm.weight:
2025-04-26 21:41:11,995 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:11,996 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:11,996 - INFO -   - Copied. Param mean: 0.3524, std: 0.0248
2025-04-26 21:41:11,996 - INFO - Loading model.layers.15.self_attn.q_proj.weight:
2025-04-26 21:41:11,996 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:11,997 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:12,064 - INFO -   - Copied. Param mean: -0.0000, std: 0.0260
2025-04-26 21:41:12,065 - INFO - Loading model.layers.15.self_attn.k_proj.weight:
2025-04-26 21:41:12,065 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:12,065 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:12,085 - INFO -   - Copied. Param mean: 0.0001, std: 0.0428
2025-04-26 21:41:12,085 - INFO - Loading model.layers.15.self_attn.v_proj.weight:
2025-04-26 21:41:12,086 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:12,086 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:12,105 - INFO -   - Copied. Param mean: -0.0000, std: 0.0137
2025-04-26 21:41:12,105 - INFO - Loading model.layers.15.self_attn.o_proj.weight:
2025-04-26 21:41:12,106 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:12,106 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:12,170 - INFO -   - Copied. Param mean: 0.0000, std: 0.0159
2025-04-26 21:41:12,170 - INFO - Loading model.layers.15.mlp.gate_proj.weight:
2025-04-26 21:41:12,170 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:12,171 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:12,357 - INFO -   - Copied. Param mean: -0.0001, std: 0.0211
2025-04-26 21:41:12,357 - INFO - Loading model.layers.15.mlp.up_proj.weight:
2025-04-26 21:41:12,357 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:12,358 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:12,609 - INFO -   - Copied. Param mean: -0.0000, std: 0.0180
2025-04-26 21:41:12,609 - INFO - Loading model.layers.15.mlp.down_proj.weight:
2025-04-26 21:41:12,610 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:12,610 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:12,817 - INFO -   - Copied. Param mean: 0.0000, std: 0.0177
2025-04-26 21:41:12,817 - INFO - Loading model.layers.16.input_layernorm.weight:
2025-04-26 21:41:12,817 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:12,817 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:12,818 - INFO -   - Copied. Param mean: 0.4019, std: 0.0667
2025-04-26 21:41:12,818 - INFO - Loading model.layers.16.post_attention_layernorm.weight:
2025-04-26 21:41:12,818 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:12,818 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:12,819 - INFO -   - Copied. Param mean: 0.3908, std: 0.0267
2025-04-26 21:41:12,819 - INFO - Loading model.layers.16.self_attn.q_proj.weight:
2025-04-26 21:41:12,819 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:12,819 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:12,893 - INFO -   - Copied. Param mean: -0.0000, std: 0.0265
2025-04-26 21:41:12,893 - INFO - Loading model.layers.16.self_attn.k_proj.weight:
2025-04-26 21:41:12,894 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:12,894 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:12,903 - INFO -   - Copied. Param mean: 0.0000, std: 0.0440
2025-04-26 21:41:12,904 - INFO - Loading model.layers.16.self_attn.v_proj.weight:
2025-04-26 21:41:12,904 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:12,904 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:12,918 - INFO -   - Copied. Param mean: -0.0000, std: 0.0151
2025-04-26 21:41:12,919 - INFO - Loading model.layers.16.self_attn.o_proj.weight:
2025-04-26 21:41:12,919 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:12,919 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:12,994 - INFO -   - Copied. Param mean: -0.0000, std: 0.0158
2025-04-26 21:41:12,995 - INFO - Loading model.layers.16.mlp.gate_proj.weight:
2025-04-26 21:41:12,995 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:12,996 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:13,196 - INFO -   - Copied. Param mean: -0.0001, std: 0.0216
2025-04-26 21:41:13,197 - INFO - Loading model.layers.16.mlp.up_proj.weight:
2025-04-26 21:41:13,197 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:13,198 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:13,406 - INFO -   - Copied. Param mean: 0.0000, std: 0.0180
2025-04-26 21:41:13,406 - INFO - Loading model.layers.16.mlp.down_proj.weight:
2025-04-26 21:41:13,406 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:13,406 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:13,590 - INFO -   - Copied. Param mean: -0.0000, std: 0.0176
2025-04-26 21:41:13,590 - INFO - Loading model.layers.17.input_layernorm.weight:
2025-04-26 21:41:13,591 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:13,591 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:13,591 - INFO -   - Copied. Param mean: 0.4315, std: 0.0760
2025-04-26 21:41:13,592 - INFO - Loading model.layers.17.post_attention_layernorm.weight:
2025-04-26 21:41:13,592 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:13,592 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:13,592 - INFO -   - Copied. Param mean: 0.4231, std: 0.0308
2025-04-26 21:41:13,593 - INFO - Loading model.layers.17.self_attn.q_proj.weight:
2025-04-26 21:41:13,593 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:13,593 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:13,670 - INFO -   - Copied. Param mean: 0.0000, std: 0.0243
2025-04-26 21:41:13,670 - INFO - Loading model.layers.17.self_attn.k_proj.weight:
2025-04-26 21:41:13,671 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:13,671 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:13,690 - INFO -   - Copied. Param mean: 0.0000, std: 0.0404
2025-04-26 21:41:13,690 - INFO - Loading model.layers.17.self_attn.v_proj.weight:
2025-04-26 21:41:13,691 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:13,691 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:13,711 - INFO -   - Copied. Param mean: -0.0000, std: 0.0188
2025-04-26 21:41:13,711 - INFO - Loading model.layers.17.self_attn.o_proj.weight:
2025-04-26 21:41:13,711 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:13,712 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:13,782 - INFO -   - Copied. Param mean: -0.0000, std: 0.0171
2025-04-26 21:41:13,782 - INFO - Loading model.layers.17.mlp.gate_proj.weight:
2025-04-26 21:41:13,782 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:13,783 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:13,966 - INFO -   - Copied. Param mean: -0.0001, std: 0.0217
2025-04-26 21:41:13,966 - INFO - Loading model.layers.17.mlp.up_proj.weight:
2025-04-26 21:41:13,966 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:13,967 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:14,161 - INFO -   - Copied. Param mean: -0.0000, std: 0.0183
2025-04-26 21:41:14,161 - INFO - Loading model.layers.17.mlp.down_proj.weight:
2025-04-26 21:41:14,162 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:14,162 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:14,365 - INFO -   - Copied. Param mean: -0.0000, std: 0.0180
2025-04-26 21:41:14,366 - INFO - Loading model.layers.18.input_layernorm.weight:
2025-04-26 21:41:14,366 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:14,366 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:14,367 - INFO -   - Copied. Param mean: 0.4387, std: 0.0815
2025-04-26 21:41:14,367 - INFO - Loading model.layers.18.post_attention_layernorm.weight:
2025-04-26 21:41:14,367 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:14,367 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:14,368 - INFO -   - Copied. Param mean: 0.4623, std: 0.0351
2025-04-26 21:41:14,368 - INFO - Loading model.layers.18.self_attn.q_proj.weight:
2025-04-26 21:41:14,368 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:14,369 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:14,443 - INFO -   - Copied. Param mean: 0.0000, std: 0.0251
2025-04-26 21:41:14,444 - INFO - Loading model.layers.18.self_attn.k_proj.weight:
2025-04-26 21:41:14,444 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:14,444 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:14,460 - INFO -   - Copied. Param mean: -0.0000, std: 0.0403
2025-04-26 21:41:14,460 - INFO - Loading model.layers.18.self_attn.v_proj.weight:
2025-04-26 21:41:14,461 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:14,461 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:14,480 - INFO -   - Copied. Param mean: -0.0000, std: 0.0196
2025-04-26 21:41:14,480 - INFO - Loading model.layers.18.self_attn.o_proj.weight:
2025-04-26 21:41:14,480 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:14,480 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:14,539 - INFO -   - Copied. Param mean: 0.0000, std: 0.0175
2025-04-26 21:41:14,540 - INFO - Loading model.layers.18.mlp.gate_proj.weight:
2025-04-26 21:41:14,540 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:14,540 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:14,782 - INFO -   - Copied. Param mean: -0.0001, std: 0.0218
2025-04-26 21:41:14,783 - INFO - Loading model.layers.18.mlp.up_proj.weight:
2025-04-26 21:41:14,783 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:14,783 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:14,989 - INFO -   - Copied. Param mean: 0.0000, std: 0.0186
2025-04-26 21:41:14,989 - INFO - Loading model.layers.18.mlp.down_proj.weight:
2025-04-26 21:41:14,990 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:14,990 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:15,187 - INFO -   - Copied. Param mean: -0.0000, std: 0.0182
2025-04-26 21:41:15,187 - INFO - Loading model.layers.19.input_layernorm.weight:
2025-04-26 21:41:15,187 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:15,187 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:15,188 - INFO -   - Copied. Param mean: 0.4350, std: 0.0937
2025-04-26 21:41:15,188 - INFO - Loading model.layers.19.post_attention_layernorm.weight:
2025-04-26 21:41:15,188 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:15,188 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:15,188 - INFO -   - Copied. Param mean: 0.4969, std: 0.0362
2025-04-26 21:41:15,189 - INFO - Loading model.layers.19.self_attn.q_proj.weight:
2025-04-26 21:41:15,189 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:15,189 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:15,256 - INFO -   - Copied. Param mean: -0.0000, std: 0.0242
2025-04-26 21:41:15,257 - INFO - Loading model.layers.19.self_attn.k_proj.weight:
2025-04-26 21:41:15,257 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:15,258 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:15,278 - INFO -   - Copied. Param mean: 0.0000, std: 0.0386
2025-04-26 21:41:15,278 - INFO - Loading model.layers.19.self_attn.v_proj.weight:
2025-04-26 21:41:15,278 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:15,279 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:15,292 - INFO -   - Copied. Param mean: 0.0000, std: 0.0231
2025-04-26 21:41:15,292 - INFO - Loading model.layers.19.self_attn.o_proj.weight:
2025-04-26 21:41:15,293 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:15,293 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:15,363 - INFO -   - Copied. Param mean: -0.0000, std: 0.0184
2025-04-26 21:41:15,364 - INFO - Loading model.layers.19.mlp.gate_proj.weight:
2025-04-26 21:41:15,364 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:15,364 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:15,555 - INFO -   - Copied. Param mean: -0.0001, std: 0.0218
2025-04-26 21:41:15,555 - INFO - Loading model.layers.19.mlp.up_proj.weight:
2025-04-26 21:41:15,556 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:15,556 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:15,738 - INFO -   - Copied. Param mean: -0.0000, std: 0.0190
2025-04-26 21:41:15,738 - INFO - Loading model.layers.19.mlp.down_proj.weight:
2025-04-26 21:41:15,738 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:15,738 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:15,947 - INFO -   - Copied. Param mean: -0.0000, std: 0.0186
2025-04-26 21:41:15,947 - INFO - Loading model.layers.20.input_layernorm.weight:
2025-04-26 21:41:15,948 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:15,948 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:15,948 - INFO -   - Copied. Param mean: 0.4330, std: 0.0860
2025-04-26 21:41:15,949 - INFO - Loading model.layers.20.post_attention_layernorm.weight:
2025-04-26 21:41:15,949 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:15,949 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:15,949 - INFO -   - Copied. Param mean: 0.5282, std: 0.0366
2025-04-26 21:41:15,950 - INFO - Loading model.layers.20.self_attn.q_proj.weight:
2025-04-26 21:41:15,950 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:15,950 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:16,029 - INFO -   - Copied. Param mean: 0.0000, std: 0.0247
2025-04-26 21:41:16,029 - INFO - Loading model.layers.20.self_attn.k_proj.weight:
2025-04-26 21:41:16,029 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:16,030 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:16,048 - INFO -   - Copied. Param mean: 0.0001, std: 0.0398
2025-04-26 21:41:16,048 - INFO - Loading model.layers.20.self_attn.v_proj.weight:
2025-04-26 21:41:16,049 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:16,049 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:16,069 - INFO -   - Copied. Param mean: 0.0000, std: 0.0235
2025-04-26 21:41:16,070 - INFO - Loading model.layers.20.self_attn.o_proj.weight:
2025-04-26 21:41:16,070 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:16,070 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:16,143 - INFO -   - Copied. Param mean: 0.0000, std: 0.0184
2025-04-26 21:41:16,143 - INFO - Loading model.layers.20.mlp.gate_proj.weight:
2025-04-26 21:41:16,144 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:16,144 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:16,356 - INFO -   - Copied. Param mean: -0.0000, std: 0.0219
2025-04-26 21:41:16,357 - INFO - Loading model.layers.20.mlp.up_proj.weight:
2025-04-26 21:41:16,357 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:16,357 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:16,564 - INFO -   - Copied. Param mean: -0.0000, std: 0.0194
2025-04-26 21:41:16,565 - INFO - Loading model.layers.20.mlp.down_proj.weight:
2025-04-26 21:41:16,565 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:16,566 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:16,774 - INFO -   - Copied. Param mean: 0.0000, std: 0.0190
2025-04-26 21:41:16,774 - INFO - Loading model.layers.21.input_layernorm.weight:
2025-04-26 21:41:16,775 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:16,775 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:16,775 - INFO -   - Copied. Param mean: 0.4637, std: 0.0852
2025-04-26 21:41:16,775 - INFO - Loading model.layers.21.post_attention_layernorm.weight:
2025-04-26 21:41:16,776 - INFO -   - Tensor shape: torch.Size([2048]), dtype: torch.bfloat16
2025-04-26 21:41:16,776 - INFO -   - Param shape: torch.Size([2048]), dtype: torch.float32
2025-04-26 21:41:16,776 - INFO -   - Copied. Param mean: 0.5546, std: 0.0391
2025-04-26 21:41:16,776 - INFO - Loading model.layers.21.self_attn.q_proj.weight:
2025-04-26 21:41:16,776 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:16,777 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:16,847 - INFO -   - Copied. Param mean: -0.0000, std: 0.0238
2025-04-26 21:41:16,847 - INFO - Loading model.layers.21.self_attn.k_proj.weight:
2025-04-26 21:41:16,847 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:16,848 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:16,858 - INFO -   - Copied. Param mean: 0.0000, std: 0.0395
2025-04-26 21:41:16,859 - INFO - Loading model.layers.21.self_attn.v_proj.weight:
2025-04-26 21:41:16,859 - INFO -   - Tensor shape: torch.Size([256, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:16,859 - INFO -   - Param shape: torch.Size([256, 2048]), dtype: torch.float32
2025-04-26 21:41:16,876 - INFO -   - Copied. Param mean: -0.0000, std: 0.0259
2025-04-26 21:41:16,876 - INFO - Loading model.layers.21.self_attn.o_proj.weight:
2025-04-26 21:41:16,877 - INFO -   - Tensor shape: torch.Size([2048, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:16,877 - INFO -   - Param shape: torch.Size([2048, 2048]), dtype: torch.float32
2025-04-26 21:41:16,955 - INFO -   - Copied. Param mean: -0.0000, std: 0.0191
2025-04-26 21:41:16,956 - INFO - Loading model.layers.21.mlp.gate_proj.weight:
2025-04-26 21:41:16,956 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:16,956 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:17,173 - INFO -   - Copied. Param mean: -0.0000, std: 0.0243
2025-04-26 21:41:17,174 - INFO - Loading model.layers.21.mlp.up_proj.weight:
2025-04-26 21:41:17,174 - INFO -   - Tensor shape: torch.Size([5632, 2048]), dtype: torch.bfloat16
2025-04-26 21:41:17,174 - INFO -   - Param shape: torch.Size([5632, 2048]), dtype: torch.float32
2025-04-26 21:41:17,401 - INFO -   - Copied. Param mean: -0.0000, std: 0.0195
2025-04-26 21:41:17,402 - INFO - Loading model.layers.21.mlp.down_proj.weight:
2025-04-26 21:41:17,402 - INFO -   - Tensor shape: torch.Size([2048, 5632]), dtype: torch.bfloat16
2025-04-26 21:41:17,402 - INFO -   - Param shape: torch.Size([2048, 5632]), dtype: torch.float32
2025-04-26 21:41:17,626 - INFO -   - Copied. Param mean: 0.0000, std: 0.0180
2025-04-26 21:41:17,627 - INFO - Model initialized with default dtype: torch.float32
2025-04-26 21:41:17,652 - INFO - lm_head first 10 values: [0.01251220703125, -0.022705078125, -0.0245361328125, -0.000713348388671875, -0.013671875, -0.002471923828125, 0.0101318359375, -0.001953125, 0.00811767578125, 0.0086669921875]
2025-04-26 21:41:17,652 - INFO - 
===== Q: A: Style =====
Prompt: Q: What is the capital of France?
A:
2025-04-26 21:41:17,653 - INFO - Prompt token IDs: [529, 29879, 29958, 29984, 29901, 1724, 338, 278, 7483, 310, 3444, 29973, 13, 29909, 29901]
2025-04-26 21:41:17,653 - INFO - Initial Input IDs (with BOS): [529, 29879, 29958, 29984, 29901, 1724, 338, 278, 7483, 310, 3444, 29973, 13, 29909, 29901]
2025-04-26 21:41:17,660 - INFO - Embedding stats for first token: min=-0.033935546875, max=0.0654296875, mean=0.00032116103102453053
2025-04-26 21:41:17,660 - INFO - First RMSNorm output stats: min=-1.4920575618743896, max=3.147249221801758, mean=0.0024326592683792114
2025-04-26 21:41:17,663 - INFO - Saved reference RMSNorm output (Layer 0, Token 0) to rmsnorm_out_ref.bin
2025-04-26 21:41:17,685 - INFO - First Q projection output stats: min=-4.503389835357666, max=4.144922733306885, mean=-0.0003170715644955635
2025-04-26 21:41:17,688 - INFO - Q before RoPE shape: [2048] num_heads=32 head_dim=64 pos=0 first 5: [-0.10789877 -0.04578701 -0.13515998 -0.01534481  0.11997594]
2025-04-26 21:41:17,707 - INFO - Q after RoPE shape: [256] first 5: [-0.10789877 -0.04578701 -0.13515998 -0.01534481  0.11997594]
2025-04-26 21:41:17,715 - INFO - First K projection output stats: min=-11.205004692077637, max=3.983145236968994, mean=-0.19396047294139862
2025-04-26 21:41:17,746 - INFO - First V projection output stats: min=-0.07605941593647003, max=0.06179859861731529, mean=0.00015027551853563637
2025-04-26 21:41:17,748 - INFO - K after RoPE shape: [256] first 5: [-0.07320985 -0.06033581  0.05773955  0.01598645  0.12556581]
2025-04-26 21:41:17,750 - INFO - First attention score (dot Q_rope, K_rope): 2.862595319747925
2025-04-26 21:41:17,781 - INFO - First attention probability (after softmax): 1.0
2025-04-26 21:41:17,797 - INFO - First attention output stats: min=-0.07605941593647003, max=0.06179859861731529, mean=0.00015027551853563637
2025-04-26 21:41:17,814 - INFO - Starting generation loop (max_new_tokens=50, eos_token_id=2)
2025-04-26 21:41:17,846 - INFO - Using token-by-token generation mode with KVCache.
2025-04-26 21:41:17,854 - INFO - Initializing KVCache for batch_size=1, max_seq_len=2048, layers=22, kv_heads=4, head_dim=64
2025-04-26 21:41:17,881 - INFO - KVCache initialized. k_cache length: 22, v_cache length: 22
2025-04-26 21:41:17,882 - INFO - Example k_cache[0] shape: torch.Size([1, 4, 2048, 64])
2025-04-26 21:41:17,882 - INFO - Priming KVCache with prompt tokens (excluding last): [529, 29879, 29958, 29984, 29901, 1724, 338, 278, 7483, 310, 3444, 29973, 13, 29909]
2025-04-26 21:41:17,883 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=0
2025-04-26 21:41:18,174 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:18,176 - INFO - Primed KVCache for pos=0
2025-04-26 21:41:18,176 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=1
2025-04-26 21:41:18,185 - INFO - [PyTorch] PyT L0 Output (pos=1): shape=[1, 1, 2048], min=-0.282582, max=0.266320, mean=-0.000196, all_finite=True
2025-04-26 21:41:18,186 - INFO - [PyTorch] PyT L0 Output (pos=1) first 5: -0.008421 -0.010634 -0.010333 -0.005166 0.003777
2025-04-26 21:41:18,186 - INFO - --- PyTorch Layer 0 End for pos=1 ---
2025-04-26 21:41:18,331 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:18,332 - INFO - Primed KVCache for pos=1
2025-04-26 21:41:18,332 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=2
2025-04-26 21:41:18,478 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:18,479 - INFO - Primed KVCache for pos=2
2025-04-26 21:41:18,479 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=3
2025-04-26 21:41:18,613 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:18,613 - INFO - Primed KVCache for pos=3
2025-04-26 21:41:18,614 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=4
2025-04-26 21:41:18,747 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:18,747 - INFO - Primed KVCache for pos=4
2025-04-26 21:41:18,748 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=5
2025-04-26 21:41:18,877 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:18,878 - INFO - Primed KVCache for pos=5
2025-04-26 21:41:18,878 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=6
2025-04-26 21:41:19,017 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:19,017 - INFO - Primed KVCache for pos=6
2025-04-26 21:41:19,018 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=7
2025-04-26 21:41:19,146 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:19,146 - INFO - Primed KVCache for pos=7
2025-04-26 21:41:19,147 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=8
2025-04-26 21:41:19,276 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:19,277 - INFO - Primed KVCache for pos=8
2025-04-26 21:41:19,277 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=9
2025-04-26 21:41:19,403 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:19,403 - INFO - Primed KVCache for pos=9
2025-04-26 21:41:19,403 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=10
2025-04-26 21:41:19,691 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:19,692 - INFO - Primed KVCache for pos=10
2025-04-26 21:41:19,693 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=11
2025-04-26 21:41:19,855 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:19,856 - INFO - Primed KVCache for pos=11
2025-04-26 21:41:19,857 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=12
2025-04-26 21:41:19,992 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:19,992 - INFO - Primed KVCache for pos=12
2025-04-26 21:41:19,993 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=13
2025-04-26 21:41:20,128 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:20,129 - INFO - Primed KVCache for pos=13
2025-04-26 21:41:20,129 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=14
2025-04-26 21:41:20,271 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:20,271 - INFO - First generated token logits (token-by-token, first 10): [-8.300727844238281, -7.779010772705078, 5.79564905166626, -4.407018661499023, -4.3599534034729, -6.102900981903076, -3.7960963249206543, -7.309700012207031, -6.306676387786865, -6.896210193634033]
2025-04-26 21:41:20,272 - INFO - Step 1 (Token-by-Token, pos=14): Predicted token ID: 3681
2025-04-26 21:41:20,272 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=15
2025-04-26 21:41:20,422 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:20,422 - INFO - Step 2 (Token-by-Token, pos=15): Predicted token ID: 29889
2025-04-26 21:41:20,423 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=16
2025-04-26 21:41:20,559 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:20,560 - INFO - Step 3 (Token-by-Token, pos=16): Predicted token ID: 2
2025-04-26 21:41:20,560 - INFO - EOS token (2) generated. Stopping generation.
2025-04-26 21:41:20,560 - INFO - Full Generated Sequence IDs: [529, 29879, 29958, 29984, 29901, 1724, 338, 278, 7483, 310, 3444, 29973, 13, 29909, 29901, 3681, 29889, 2]
2025-04-26 21:41:20,561 - INFO - Full Decoded Text:
-------
<s>Q: What is the capital of France?
A: Paris.
-------
2025-04-26 21:41:20,561 - INFO - Generated Part IDs: [3681, 29889, 2]
2025-04-26 21:41:20,561 - INFO - Generated Decoded Text (raw):
-------
Paris.
-------
2025-04-26 21:41:20,561 - INFO - Generated Decoded Text (cleaned):
-------
Paris.
-------
2025-04-26 21:41:20,562 - INFO - 
===== Q: A: Style =====
Prompt: Q: Who wrote Hamlet?
A:
2025-04-26 21:41:20,562 - INFO - Prompt token IDs: [529, 29879, 29958, 29984, 29901, 11644, 5456, 7904, 1026, 29973, 13, 29909, 29901]
2025-04-26 21:41:20,562 - INFO - Initial Input IDs (with BOS): [529, 29879, 29958, 29984, 29901, 11644, 5456, 7904, 1026, 29973, 13, 29909, 29901]
2025-04-26 21:41:20,563 - INFO - Embedding stats for first token: min=-0.033935546875, max=0.0654296875, mean=0.00032116103102453053
2025-04-26 21:41:20,563 - INFO - First RMSNorm output stats: min=-1.4920575618743896, max=3.147249221801758, mean=0.0024326592683792114
2025-04-26 21:41:20,566 - INFO - Saved reference RMSNorm output (Layer 0, Token 0) to rmsnorm_out_ref.bin
2025-04-26 21:41:20,589 - INFO - First Q projection output stats: min=-4.503389835357666, max=4.144922733306885, mean=-0.0003170715644955635
2025-04-26 21:41:20,592 - INFO - Q before RoPE shape: [2048] num_heads=32 head_dim=64 pos=0 first 5: [-0.10789877 -0.04578701 -0.13515998 -0.01534481  0.11997594]
2025-04-26 21:41:20,594 - INFO - Q after RoPE shape: [256] first 5: [-0.10789877 -0.04578701 -0.13515998 -0.01534481  0.11997594]
2025-04-26 21:41:20,597 - INFO - First K projection output stats: min=-11.205004692077637, max=3.983145236968994, mean=-0.19396047294139862
2025-04-26 21:41:20,599 - INFO - First V projection output stats: min=-0.07605941593647003, max=0.06179859861731529, mean=0.00015027551853563637
2025-04-26 21:41:20,600 - INFO - K after RoPE shape: [256] first 5: [-0.07320985 -0.06033581  0.05773955  0.01598645  0.12556581]
2025-04-26 21:41:20,600 - INFO - First attention score (dot Q_rope, K_rope): 2.862595319747925
2025-04-26 21:41:20,601 - INFO - First attention probability (after softmax): 1.0
2025-04-26 21:41:20,601 - INFO - First attention output stats: min=-0.07605941593647003, max=0.06179859861731529, mean=0.00015027551853563637
2025-04-26 21:41:20,603 - INFO - Starting generation loop (max_new_tokens=50, eos_token_id=2)
2025-04-26 21:41:20,608 - INFO - Using token-by-token generation mode with KVCache.
2025-04-26 21:41:20,611 - INFO - Initializing KVCache for batch_size=1, max_seq_len=2048, layers=22, kv_heads=4, head_dim=64
2025-04-26 21:41:20,720 - INFO - KVCache initialized. k_cache length: 22, v_cache length: 22
2025-04-26 21:41:20,721 - INFO - Example k_cache[0] shape: torch.Size([1, 4, 2048, 64])
2025-04-26 21:41:20,722 - INFO - Priming KVCache with prompt tokens (excluding last): [529, 29879, 29958, 29984, 29901, 11644, 5456, 7904, 1026, 29973, 13, 29909]
2025-04-26 21:41:20,722 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=0
2025-04-26 21:41:20,871 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:20,872 - INFO - Primed KVCache for pos=0
2025-04-26 21:41:20,872 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=1
2025-04-26 21:41:20,878 - INFO - [PyTorch] PyT L0 Output (pos=1): shape=[1, 1, 2048], min=-0.282582, max=0.266320, mean=-0.000196, all_finite=True
2025-04-26 21:41:20,879 - INFO - [PyTorch] PyT L0 Output (pos=1) first 5: -0.008421 -0.010634 -0.010333 -0.005166 0.003777
2025-04-26 21:41:20,879 - INFO - --- PyTorch Layer 0 End for pos=1 ---
2025-04-26 21:41:21,008 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:21,009 - INFO - Primed KVCache for pos=1
2025-04-26 21:41:21,009 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=2
2025-04-26 21:41:21,143 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:21,144 - INFO - Primed KVCache for pos=2
2025-04-26 21:41:21,144 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=3
2025-04-26 21:41:21,277 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:21,277 - INFO - Primed KVCache for pos=3
2025-04-26 21:41:21,278 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=4
2025-04-26 21:41:21,417 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:21,418 - INFO - Primed KVCache for pos=4
2025-04-26 21:41:21,418 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=5
2025-04-26 21:41:21,552 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:21,553 - INFO - Primed KVCache for pos=5
2025-04-26 21:41:21,553 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=6
2025-04-26 21:41:21,701 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:21,702 - INFO - Primed KVCache for pos=6
2025-04-26 21:41:21,702 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=7
2025-04-26 21:41:21,842 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:21,843 - INFO - Primed KVCache for pos=7
2025-04-26 21:41:21,843 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=8
2025-04-26 21:41:21,977 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:21,978 - INFO - Primed KVCache for pos=8
2025-04-26 21:41:21,978 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=9
2025-04-26 21:41:22,110 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:22,111 - INFO - Primed KVCache for pos=9
2025-04-26 21:41:22,111 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=10
2025-04-26 21:41:22,240 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:22,241 - INFO - Primed KVCache for pos=10
2025-04-26 21:41:22,242 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=11
2025-04-26 21:41:22,376 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:22,376 - INFO - Primed KVCache for pos=11
2025-04-26 21:41:22,377 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=12
2025-04-26 21:41:22,620 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:22,621 - INFO - First generated token logits (token-by-token, first 10): [-9.270827293395996, -9.22700309753418, 4.590665817260742, -6.461870193481445, -5.624599456787109, -4.809528827667236, -5.793954849243164, -9.606549263000488, -6.8218159675598145, -7.289485454559326]
2025-04-26 21:41:22,621 - INFO - Step 1 (Token-by-Token, pos=12): Predicted token ID: 7904
2025-04-26 21:41:22,621 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=13
2025-04-26 21:41:22,779 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:22,780 - INFO - Step 2 (Token-by-Token, pos=13): Predicted token ID: 1026
2025-04-26 21:41:22,780 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=14
2025-04-26 21:41:22,921 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:22,922 - INFO - Step 3 (Token-by-Token, pos=14): Predicted token ID: 471
2025-04-26 21:41:22,922 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=15
2025-04-26 21:41:23,071 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:23,071 - INFO - Step 4 (Token-by-Token, pos=15): Predicted token ID: 3971
2025-04-26 21:41:23,072 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=16
2025-04-26 21:41:23,204 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:23,205 - INFO - Step 5 (Token-by-Token, pos=16): Predicted token ID: 491
2025-04-26 21:41:23,205 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=17
2025-04-26 21:41:23,340 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:23,341 - INFO - Step 6 (Token-by-Token, pos=17): Predicted token ID: 4667
2025-04-26 21:41:23,341 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=18
2025-04-26 21:41:23,481 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:23,482 - INFO - Step 7 (Token-by-Token, pos=18): Predicted token ID: 23688
2025-04-26 21:41:23,482 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=19
2025-04-26 21:41:23,700 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:23,701 - INFO - Step 8 (Token-by-Token, pos=19): Predicted token ID: 29889
2025-04-26 21:41:23,701 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=20
2025-04-26 21:41:23,844 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:23,845 - INFO - Step 9 (Token-by-Token, pos=20): Predicted token ID: 13
2025-04-26 21:41:23,845 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=21
2025-04-26 21:41:23,981 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:23,982 - INFO - Step 10 (Token-by-Token, pos=21): Predicted token ID: 13
2025-04-26 21:41:23,982 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=22
2025-04-26 21:41:24,112 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:24,113 - INFO - Step 11 (Token-by-Token, pos=22): Predicted token ID: 29933
2025-04-26 21:41:24,113 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=23
2025-04-26 21:41:24,245 - INFO - [PyTorch] TinyLlama.forward returning logits shape: torch.Size([1, 1, 32000])
2025-04-26 21:41:24,246 - INFO - Step 12 (Token-by-Token, pos=23): Predicted token ID: 1463
2025-04-26 21:41:24,246 - INFO - [PyTorch] TinyLlama.forward called with input_ids shape: torch.Size([1, 1]), pos=24
