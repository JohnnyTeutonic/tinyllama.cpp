[INFO] Using data directory: data
[INFO] Loading config: data/config.json
[INFO] Using BOS token string from config: <s>
[INFO] Loaded 201 tensors from model.safetensors:
[INFO]   lm_head.weight | dtype: BF16, shape: [32000, 2048]
[INFO]   model.embed_tokens.weight | dtype: BF16, shape: [32000, 2048]
[INFO]   model.layers.0.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.0.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.0.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.0.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.0.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.0.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.0.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.0.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.0.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.1.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.1.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.1.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.1.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.1.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.1.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.1.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.1.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.1.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.10.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.10.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.10.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.10.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.10.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.10.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.10.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.10.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.10.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.11.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.11.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.11.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.11.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.11.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.11.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.11.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.11.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.11.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.12.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.12.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.12.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.12.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.12.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.12.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.12.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.12.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.12.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.13.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.13.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.13.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.13.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.13.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.13.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.13.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.13.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.13.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.14.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.14.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.14.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.14.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.14.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.14.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.14.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.14.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.14.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.15.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.15.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.15.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.15.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.15.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.15.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.15.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.15.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.15.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.16.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.16.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.16.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.16.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.16.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.16.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.16.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.16.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.16.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.17.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.17.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.17.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.17.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.17.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.17.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.17.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.17.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.17.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.18.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.18.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.18.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.18.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.18.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.18.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.18.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.18.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.18.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.19.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.19.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.19.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.19.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.19.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.19.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.19.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.19.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.19.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.2.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.2.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.2.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.2.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.2.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.2.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.2.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.2.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.2.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.20.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.20.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.20.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.20.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.20.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.20.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.20.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.20.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.20.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.21.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.21.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.21.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.21.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.21.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.21.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.21.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.21.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.21.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.3.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.3.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.3.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.3.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.3.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.3.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.3.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.3.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.3.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.4.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.4.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.4.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.4.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.4.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.4.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.4.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.4.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.4.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.5.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.5.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.5.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.5.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.5.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.5.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.5.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.5.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.5.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.6.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.6.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.6.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.6.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.6.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.6.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.6.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.6.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.6.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.7.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.7.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.7.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.7.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.7.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.7.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.7.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.7.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.7.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.8.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.8.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.8.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.8.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.8.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.8.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.8.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.8.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.8.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.9.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.9.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.9.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.9.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.9.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.9.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.9.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.9.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.9.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.norm.weight | dtype: BF16, shape: [2048]
[INFO] K/V projection kv_dim: 256
[INFO] C++ layer 0 q_proj first 5 (uint16_t): 47811 47904 48115 48230 47934 
[INFO] C++ layer 1 q_proj first 5 (uint16_t): 15059 48280 48138 48155 48000 
[INFO] C++ layer 2 q_proj first 5 (uint16_t): 14922 48459 48472 48576 15772 
[INFO] C++ layer 3 q_proj first 5 (uint16_t): 48131 15496 15499 48386 48178 
[INFO] C++ layer 4 q_proj first 5 (uint16_t): 15525 48162 15624 48259 15506 
[INFO] C++ layer 5 q_proj first 5 (uint16_t): 48089 48208 14953 48173 15201 
[INFO] C++ layer 6 q_proj first 5 (uint16_t): 15593 15366 48306 48351 48160 
[INFO] C++ layer 7 q_proj first 5 (uint16_t): 47413 15232 48000 48252 48345 
[INFO] C++ layer 8 q_proj first 5 (uint16_t): 48278 15308 48296 15593 15196 
[INFO] C++ layer 9 q_proj first 5 (uint16_t): 15407 15468 15434 47968 48014 
[INFO] C++ layer 10 q_proj first 5 (uint16_t): 48433 48201 48280 48138 48417 
[INFO] C++ layer 11 q_proj first 5 (uint16_t): 47943 47993 15390 47863 15361 
[INFO] C++ layer 12 q_proj first 5 (uint16_t): 48414 48324 48399 15744 47940 
[INFO] C++ layer 13 q_proj first 5 (uint16_t): 48379 48305 47985 48179 48052 
[INFO] C++ layer 14 q_proj first 5 (uint16_t): 15160 15068 48140 47905 48257 
[INFO] C++ layer 15 q_proj first 5 (uint16_t): 15645 15162 48326 15115 48174 
[INFO] C++ layer 16 q_proj first 5 (uint16_t): 15215 15364 15619 15409 15640 
[INFO] C++ layer 17 q_proj first 5 (uint16_t): 48160 48061 47960 48228 48161 
[INFO] C++ layer 18 q_proj first 5 (uint16_t): 48383 15502 48071 15142 15359 
[INFO] C++ layer 19 q_proj first 5 (uint16_t): 15446 15375 48405 48452 14486 
[INFO] C++ layer 20 q_proj first 5 (uint16_t): 47982 15236 15057 15388 15571 
[INFO] C++ layer 21 q_proj first 5 (uint16_t): 48291 48322 48404 15675 48290 
[INFO] All model weights loaded (as bfloat16/uint16_t).
[INFO] TinyLlamaModel weights loaded successfully.
[INFO] lm_head first 10 values: 0.0125122 -0.0227051 -0.0245361 -0.000713348 -0.0136719 -0.00247192 0.0101318 -0.00195312 0.00811768 0.00866699 
[INFO] KVCache initialized. Layers: 22, Size per layer: 524288
[INFO] Full Prompt (with BOS string): <s>Q: What is the capital of France?
A:
[INFO] Tokenized IDs: Count=15
[INFO] Prompt token IDs: [ 529 29879 29958 29984 29901 1724 338 278 7483 310 3444 29973 13 29909 29901 ]
[INFO] Embedding stats for first token: min=-0.0339355, max=0.0654297, mean=0.000321161
[INFO] First RMSNorm output stats: min=-1.49206, max=3.14725, mean=0.00243266
[INFO] First Q projection output stats: min=-4.50339, max=4.14492, mean=-0.00031706
[INFO] Q before RoPE shape: [2048] num_heads=32 head_dim=64 pos=0  first 5: -0.107899 -0.0457869 -0.13516 -0.0153448 0.119976 
[INFO] Q after RoPE shape: [2048] first 5: -0.107899 -0.0457869 -0.13516 -0.0153448 0.119976 
[INFO] First K projection output stats: min=-11.205, max=3.98314, mean=-0.19396
[INFO] First V projection output stats: min=-0.0760594, max=0.0617986, mean=0.000150275
[INFO] First attention score (dot Q_rope, K_rope): 2.862595
[INFO] First attention probability (after softmax): 1.000000
[INFO] First attention output stats: min=-0.0760594, max=0.0617986, mean=0.000150275
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.033936, max=0.065430, mean=0.000321, all_finite=yes
[INFO] attn_out (layer 0): min=-0.076059, max=0.061799, mean=0.000019, all_finite=yes
