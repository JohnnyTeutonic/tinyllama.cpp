[INFO] Using data directory: data
[INFO] Loading config: data/config.json
[INFO] Loaded 201 tensors from model.safetensors:
[INFO]   lm_head.weight | dtype: BF16, shape: [32000, 2048]
[INFO]   model.embed_tokens.weight | dtype: BF16, shape: [32000, 2048]
[INFO]   model.layers.0.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.0.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.0.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.0.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.0.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.0.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.0.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.0.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.0.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.1.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.1.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.1.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.1.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.1.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.1.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.1.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.1.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.1.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.10.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.10.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.10.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.10.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.10.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.10.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.10.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.10.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.10.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.11.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.11.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.11.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.11.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.11.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.11.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.11.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.11.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.11.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.12.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.12.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.12.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.12.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.12.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.12.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.12.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.12.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.12.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.13.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.13.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.13.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.13.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.13.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.13.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.13.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.13.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.13.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.14.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.14.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.14.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.14.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.14.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.14.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.14.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.14.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.14.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.15.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.15.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.15.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.15.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.15.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.15.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.15.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.15.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.15.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.16.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.16.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.16.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.16.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.16.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.16.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.16.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.16.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.16.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.17.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.17.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.17.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.17.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.17.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.17.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.17.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.17.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.17.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.18.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.18.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.18.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.18.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.18.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.18.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.18.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.18.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.18.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.19.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.19.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.19.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.19.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.19.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.19.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.19.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.19.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.19.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.2.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.2.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.2.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.2.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.2.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.2.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.2.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.2.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.2.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.20.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.20.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.20.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.20.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.20.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.20.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.20.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.20.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.20.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.21.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.21.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.21.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.21.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.21.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.21.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.21.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.21.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.21.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.3.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.3.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.3.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.3.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.3.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.3.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.3.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.3.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.3.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.4.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.4.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.4.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.4.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.4.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.4.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.4.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.4.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.4.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.5.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.5.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.5.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.5.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.5.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.5.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.5.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.5.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.5.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.6.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.6.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.6.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.6.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.6.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.6.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.6.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.6.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.6.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.7.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.7.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.7.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.7.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.7.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.7.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.7.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.7.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.7.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.8.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.8.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.8.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.8.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.8.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.8.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.8.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.8.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.8.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.9.input_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.9.mlp.down_proj.weight | dtype: BF16, shape: [2048, 5632]
[INFO]   model.layers.9.mlp.gate_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.9.mlp.up_proj.weight | dtype: BF16, shape: [5632, 2048]
[INFO]   model.layers.9.post_attention_layernorm.weight | dtype: BF16, shape: [2048]
[INFO]   model.layers.9.self_attn.k_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.layers.9.self_attn.o_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.9.self_attn.q_proj.weight | dtype: BF16, shape: [2048, 2048]
[INFO]   model.layers.9.self_attn.v_proj.weight | dtype: BF16, shape: [256, 2048]
[INFO]   model.norm.weight | dtype: BF16, shape: [2048]
[INFO] K/V projection kv_dim: 256
[INFO] C++ layer 0 q_proj first 5 (uint16_t): 47811 47904 48115 48230 47934 
[INFO] C++ layer 1 q_proj first 5 (uint16_t): 15059 48280 48138 48155 48000 
[INFO] C++ layer 2 q_proj first 5 (uint16_t): 14922 48459 48472 48576 15772 
[INFO] C++ layer 3 q_proj first 5 (uint16_t): 48131 15496 15499 48386 48178 
[INFO] C++ layer 4 q_proj first 5 (uint16_t): 15525 48162 15624 48259 15506 
[INFO] C++ layer 5 q_proj first 5 (uint16_t): 48089 48208 14953 48173 15201 
[INFO] C++ layer 6 q_proj first 5 (uint16_t): 15593 15366 48306 48351 48160 
[INFO] C++ layer 7 q_proj first 5 (uint16_t): 47413 15232 48000 48252 48345 
[INFO] C++ layer 8 q_proj first 5 (uint16_t): 48278 15308 48296 15593 15196 
[INFO] C++ layer 9 q_proj first 5 (uint16_t): 15407 15468 15434 47968 48014 
[INFO] C++ layer 10 q_proj first 5 (uint16_t): 48433 48201 48280 48138 48417 
[INFO] C++ layer 11 q_proj first 5 (uint16_t): 47943 47993 15390 47863 15361 
[INFO] C++ layer 12 q_proj first 5 (uint16_t): 48414 48324 48399 15744 47940 
[INFO] C++ layer 13 q_proj first 5 (uint16_t): 48379 48305 47985 48179 48052 
[INFO] C++ layer 14 q_proj first 5 (uint16_t): 15160 15068 48140 47905 48257 
[INFO] C++ layer 15 q_proj first 5 (uint16_t): 15645 15162 48326 15115 48174 
[INFO] C++ layer 16 q_proj first 5 (uint16_t): 15215 15364 15619 15409 15640 
[INFO] C++ layer 17 q_proj first 5 (uint16_t): 48160 48061 47960 48228 48161 
[INFO] C++ layer 18 q_proj first 5 (uint16_t): 48383 15502 48071 15142 15359 
[INFO] C++ layer 19 q_proj first 5 (uint16_t): 15446 15375 48405 48452 14486 
[INFO] C++ layer 20 q_proj first 5 (uint16_t): 47982 15236 15057 15388 15571 
[INFO] C++ layer 21 q_proj first 5 (uint16_t): 48291 48322 48404 15675 48290 
[INFO] All model weights loaded (as bfloat16/uint16_t).
[INFO] TinyLlamaModel weights loaded successfully.
[INFO] --- Deleting previous layer_0_cpp_outputs.log (if exists) ---
[INFO] --- Running dedicated forward pass for token ID 1 for logging ---
[INFO] Opened layer_0_cpp_outputs.log for Layer 0 logging (truncating).
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.007629, max=0.006317, mean=0.000026, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-9.025855, max=21.532234, mean=0.072847, all_finite=yes
[INFO] logits: min=-9.590301, max=7.293433, mean=-0.684309, all_finite=yes
[INFO] Forward pass complete.
[INFO] --- Dedicated forward pass for token ID 1 complete ---
[INFO] Prompt: Q: What is the capital of France?
A:
[INFO] Opened layer_0_cpp_outputs.log for Layer 0 logging (truncating).
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.007629, max=0.006317, mean=0.000026, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-9.025855, max=21.532234, mean=0.072847, all_finite=yes
[INFO] logits: min=-9.590301, max=7.293433, mean=-0.684309, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.040771, max=0.065430, mean=-0.000029, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-12.962433, max=8.653706, mean=-0.050697, all_finite=yes
[INFO] logits: min=-13.668696, max=12.169840, mean=-2.472200, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.043457, max=0.069336, mean=-0.000270, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-26.976061, max=16.180563, mean=-0.066638, all_finite=yes
[INFO] logits: min=-11.809195, max=8.516650, mean=-2.020373, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.038330, max=0.047852, mean=-0.000116, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-21.861652, max=18.451651, mean=0.001481, all_finite=yes
[INFO] logits: min=-16.117912, max=8.641842, mean=-2.973732, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.051025, max=0.062500, mean=0.000167, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-20.918083, max=9.347968, mean=0.027274, all_finite=yes
[INFO] logits: min=-14.184191, max=9.360126, mean=-2.567196, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.063965, max=0.075195, mean=0.000024, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-16.484877, max=13.738866, mean=0.023746, all_finite=yes
[INFO] logits: min=-11.251940, max=7.330640, mean=-2.244394, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.047363, max=0.044189, mean=0.000409, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-20.974991, max=17.494303, mean=-0.043259, all_finite=yes
[INFO] logits: min=-14.854976, max=9.006026, mean=-3.047163, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.041260, max=0.092285, mean=0.000121, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-22.725048, max=16.288898, mean=-0.033260, all_finite=yes
[INFO] logits: min=-14.190132, max=8.725683, mean=-2.939806, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.060547, max=0.049805, mean=0.000282, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-14.874833, max=14.431788, mean=-0.047454, all_finite=yes
[INFO] logits: min=-11.623332, max=6.504155, mean=-2.681132, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.037109, max=0.073730, mean=0.000024, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-28.782576, max=10.725630, mean=-0.024978, all_finite=yes
[INFO] logits: min=-11.247627, max=7.756202, mean=-1.090377, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.062256, max=0.068848, mean=0.000024, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-8.890272, max=41.812748, mean=0.033557, all_finite=yes
[INFO] logits: min=-3.614169, max=4.900682, mean=0.211579, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.057129, max=0.059082, mean=-0.000343, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-26.952883, max=10.816219, mean=-0.118220, all_finite=yes
[INFO] logits: min=-10.925396, max=8.493412, mean=-2.178253, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.043457, max=0.069336, mean=-0.000270, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-26.976061, max=16.180563, mean=-0.066638, all_finite=yes
[INFO] logits: min=-11.809195, max=8.516650, mean=-2.020373, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.043457, max=0.069336, mean=-0.000270, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-26.976061, max=16.180563, mean=-0.066638, all_finite=yes
[INFO] logits: min=-11.809195, max=8.516650, mean=-2.020373, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.049805, max=0.068359, mean=0.000005, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-18.837311, max=21.167831, mean=0.011314, all_finite=yes
[INFO] logits: min=-12.418781, max=9.678916, mean=-2.686197, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.066406, max=0.073242, mean=0.000010, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-30.180891, max=19.028980, mean=-0.000575, all_finite=yes
[INFO] logits: min=-14.488927, max=7.726672, mean=-2.413382, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.041504, max=0.064941, mean=0.000110, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-24.095043, max=28.106806, mean=0.036470, all_finite=yes
[INFO] logits: min=-14.406349, max=7.360050, mean=-2.864198, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.049316, max=0.047119, mean=-0.000161, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-20.880863, max=13.556460, mean=-0.048881, all_finite=yes
[INFO] logits: min=-10.267921, max=7.952105, mean=-1.784684, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.041260, max=0.092285, mean=0.000121, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-22.725048, max=16.288898, mean=-0.033260, all_finite=yes
[INFO] logits: min=-14.190132, max=8.725683, mean=-2.939806, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.063965, max=0.075195, mean=0.000024, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-16.484877, max=13.738866, mean=0.023746, all_finite=yes
[INFO] logits: min=-11.251940, max=7.330640, mean=-2.244394, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.046631, max=0.041748, mean=0.000214, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-22.979549, max=13.153867, mean=-0.025924, all_finite=yes
[INFO] logits: min=-12.772730, max=7.024331, mean=-2.359135, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.037109, max=0.094238, mean=0.000092, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-20.807467, max=21.305454, mean=-0.029532, all_finite=yes
[INFO] logits: min=-9.928858, max=7.671634, mean=-1.733555, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.066406, max=0.073242, mean=0.000010, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-30.180891, max=19.028980, mean=-0.000575, all_finite=yes
[INFO] logits: min=-14.488927, max=7.726672, mean=-2.413382, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.053711, max=0.055176, mean=-0.000011, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-14.733565, max=8.834928, mean=0.015447, all_finite=yes
[INFO] logits: min=-10.432450, max=11.637203, mean=-1.554834, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.034424, max=0.040283, mean=0.000211, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-25.120325, max=12.729542, mean=-0.060070, all_finite=yes
[INFO] logits: min=-10.407182, max=8.394503, mean=-1.863512, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.036865, max=0.109863, mean=0.000065, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-33.804386, max=17.463371, mean=-0.014377, all_finite=yes
[INFO] logits: min=-15.003785, max=10.813818, mean=-2.864660, all_finite=yes
[INFO] Forward pass complete.
[INFO] Generated answer: " and other questions of theguard, anderased.
[INFO] Prompt: Q: Who wrote Hamlet?
A:
[INFO] Opened layer_0_cpp_outputs.log for Layer 0 logging (truncating).
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.007629, max=0.006317, mean=0.000026, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-9.025855, max=21.532234, mean=0.072847, all_finite=yes
[INFO] logits: min=-9.590301, max=7.293433, mean=-0.684309, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.040771, max=0.065430, mean=-0.000029, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-12.962433, max=8.653706, mean=-0.050697, all_finite=yes
[INFO] logits: min=-13.668696, max=12.169840, mean=-2.472200, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.043457, max=0.069336, mean=-0.000270, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-26.976061, max=16.180563, mean=-0.066638, all_finite=yes
[INFO] logits: min=-11.809195, max=8.516650, mean=-2.020373, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.041504, max=0.044922, mean=-0.000227, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-11.312392, max=9.994235, mean=-0.039097, all_finite=yes
[INFO] logits: min=-13.116803, max=8.513630, mean=-3.311059, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.043213, max=0.041992, mean=0.000466, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-20.815817, max=21.877731, mean=0.055407, all_finite=yes
[INFO] logits: min=-11.974512, max=9.025077, mean=-3.107548, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.047119, max=0.051514, mean=0.000195, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-16.844975, max=6.278860, mean=0.012337, all_finite=yes
[INFO] logits: min=-11.915450, max=12.307039, mean=-1.828620, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.038330, max=0.044922, mean=0.000191, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-28.650560, max=11.797604, mean=-0.077557, all_finite=yes
[INFO] logits: min=-10.889550, max=9.607992, mean=-1.812803, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.037109, max=0.073730, mean=0.000024, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-28.782576, max=10.725630, mean=-0.024978, all_finite=yes
[INFO] logits: min=-11.247627, max=7.756202, mean=-1.090377, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.062256, max=0.068848, mean=0.000024, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-8.890272, max=41.812748, mean=0.033557, all_finite=yes
[INFO] logits: min=-3.614169, max=4.900682, mean=0.211579, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.057129, max=0.059082, mean=-0.000343, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-26.952883, max=10.816219, mean=-0.118220, all_finite=yes
[INFO] logits: min=-10.925396, max=8.493412, mean=-2.178253, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.043457, max=0.069336, mean=-0.000270, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-26.976061, max=16.180563, mean=-0.066638, all_finite=yes
[INFO] logits: min=-11.809195, max=8.516650, mean=-2.020373, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.043457, max=0.069336, mean=-0.000270, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-26.976061, max=16.180563, mean=-0.066638, all_finite=yes
[INFO] logits: min=-11.809195, max=8.516650, mean=-2.020373, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.049805, max=0.068359, mean=0.000005, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-18.837311, max=21.167831, mean=0.011314, all_finite=yes
[INFO] logits: min=-12.418781, max=9.678916, mean=-2.686197, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.066406, max=0.073242, mean=0.000010, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-30.180891, max=19.028980, mean=-0.000575, all_finite=yes
[INFO] logits: min=-14.488927, max=7.726672, mean=-2.413382, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.041504, max=0.064941, mean=0.000110, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-24.095043, max=28.106806, mean=0.036470, all_finite=yes
[INFO] logits: min=-14.406349, max=7.360050, mean=-2.864198, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.049316, max=0.047119, mean=-0.000161, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-20.880863, max=13.556460, mean=-0.048881, all_finite=yes
[INFO] logits: min=-10.267921, max=7.952105, mean=-1.784684, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.041260, max=0.092285, mean=0.000121, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-22.725048, max=16.288898, mean=-0.033260, all_finite=yes
[INFO] logits: min=-14.190132, max=8.725683, mean=-2.939806, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.063965, max=0.075195, mean=0.000024, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-16.484877, max=13.738866, mean=0.023746, all_finite=yes
[INFO] logits: min=-11.251940, max=7.330640, mean=-2.244394, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.046631, max=0.041748, mean=0.000214, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-22.979549, max=13.153867, mean=-0.025924, all_finite=yes
[INFO] logits: min=-12.772730, max=7.024331, mean=-2.359135, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.037109, max=0.094238, mean=0.000092, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-20.807467, max=21.305454, mean=-0.029532, all_finite=yes
[INFO] logits: min=-9.928858, max=7.671634, mean=-1.733555, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.066406, max=0.073242, mean=0.000010, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-30.180891, max=19.028980, mean=-0.000575, all_finite=yes
[INFO] logits: min=-14.488927, max=7.726672, mean=-2.413382, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.053711, max=0.055176, mean=-0.000011, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-14.733565, max=8.834928, mean=0.015447, all_finite=yes
[INFO] logits: min=-10.432450, max=11.637203, mean=-1.554834, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.034424, max=0.040283, mean=0.000211, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-25.120325, max=12.729542, mean=-0.060070, all_finite=yes
[INFO] logits: min=-10.407182, max=8.394503, mean=-1.863512, all_finite=yes
[INFO] Forward pass complete.
[INFO] Embedding lookup complete (converted to float32).
[INFO] embedding (float32): min=-0.036865, max=0.109863, mean=0.000065, all_finite=yes
[INFO] Transformer blocks complete.
[INFO] final RMSNorm: min=-33.804386, max=17.463371, mean=-0.014377, all_finite=yes
[INFO] logits: min=-15.003785, max=10.813818, mean=-2.864660, all_finite=yes
[INFO] Forward pass complete.
[INFO] Generated answer: " and other questions of theguard, anderased.
[INFO] Pipeline finalised.

